[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The lidR package",
    "section": "",
    "text": "1 Introduction\nlidR is an R package for manipulating and visualizing airborne laser scanning (ALS) data with an emphasis on research & development for forestry and ecology applications. The package is entirely open source and is integrated within the geospatial R ecosystem (i.e., raster/terra/stars and sp/sf). This guide has been written to help both the ALS novice and seasoned point cloud processing veterans. Key functionality of lidR includes functions to:\n\nRead and write .las and .laz files and render customized point-cloud displays (chapter 2)\nProcess point clouds, including point classification (chapter 3), digital terrain models (chapter 4), normalization (chapter 5), and digital surface models (chapter 6)\nPerform individual tree segmentation (chapter 7)\nCompute standard metrics at different levels of regularization (chapters 8, 9, 10, 11, 12, 13)\nManage processing for sets of point-cloud files - referred to as a LAScatalog (chapters 14, 15)\nImplement guidelines for area-based approaches to forest modeling using ALS data (chapter 16)\nFacilitate user-defined processing streams for research and development (chapter 17)\nUnderstand spatial indexing (chapter 18)\nDiscover the plugin system (chapter 19)\n\n\n\n2 Development\nThe current release version of lidR can be found on CRAN, and the source code is hosted on GitHub. Development of the lidR package was made possible thanks to the financial support of:\n\n2015-2018: the AWARE project NSERC CRDPJ 462973-14; grantee Prof. Nicholas C. Coops and Laval University.\n2018-2021: the financial support of the Ministère des Forêts, de la Faune et des Parcs of Québec and Laval University.\n2021-2024: Laval University.\n\n\nSince 2024, the lidR package (as well as the lasR package) is no longer supported by Laval University. While the software will remain free and open-source, r-lidar has transitioned into a company to ensure sustainability. We now offer independent services for training courses, consulting, and development. For more information, please visit our website: r-lidar.com.\n\n\n\n3 Other Lidar Packages\nFor a package focused on production rather than R&D, the reader can look at the lasR package, which is not covered in this book. The lasR package is much more powerful than lidR and is designed to process terabytes of data. However, the counterpart is that it does not allow easy and handy manipulation of the point cloud in R like lidR. Both tools tagert different usages.\n\n\n4 Installation\nIn R simply type:\ninstall.packages(\"lidR\")\nFor linux user you need some external libraries\n# Ubuntu\nsudo apt-get install libgdal-dev libgeos++-dev libudunits2-dev libproj-dev libx11-dev libgl1-mesa-dev libglu1-mesa-dev libfreetype6-dev libxt-dev libfftw3-dev\n\n# Fedora\nsudo dnf install gdal-devel geos-devel udunits2-devel proj-devel mesa-libGL-devel mesa-libGLU-devel freetype-devel libjpeg-turbo-devel\nThe book is shared under CC-BY-NC-SA 2.0\n\n\nThis book was created to provide hands-on descriptions and tutorials for using lidR and is not the formal package documentation. The comprehensive package documentation is shipped with the package.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "io.html",
    "href": "io.html",
    "title": "2  Reading, Plotting, Querying & Validating",
    "section": "",
    "text": "2.1 Reading LiDAR data using readLAS\nDiscrete return ALS sensors record various types of data. Primarily, they capture positional data in three dimensions (X, Y, Z), followed by additional information like the intensity for each point, the position of each point in the return sequence, and the beam incidence angle of each point. Reading, writing, and efficient storage of these ALS data are critical steps prior to any subsequent analysis.\nALS data are most commonly distributed in LAS format, which is specifically designed to store ALS data in a standardized way. These data are officially documented and maintained by the ASPRS. However, LAS files require a large amount of memory because they are not compressed. The LAZ format has become the standard compression scheme because it is free and open-source.\nThe widespread use, standardization, and open-source nature of the LAS and LAZ formats promoted the development of the lidR package. This package is designed to process LAS and LAZ files both as input and output, leveraging the LASlib and LASzip C++ libraries via the rlas package.\nThe function readLAS() reads a LAS or LAZ file and returns an object of class LAS. The LAS formal class is documented in detail in a dedicated vignette. To briefly summarize, a LAS file consists of two parts:\nThe function readLAS() reads and creates an object that contains both the header and the payload.\nWhen printed it displays a summary of its content.\nprint(las)\n#&gt; class        : LAS (v1.2 format 1)\n#&gt; memory       : 4.4 Mb \n#&gt; extent       : 684766.4, 684993.3, 5017773, 5018007 (xmin, xmax, ymin, ymax)\n#&gt; coord. ref.  : NAD83 / UTM zone 17N \n#&gt; area         : 51572 m²\n#&gt; points       : 81.6 thousand points\n#&gt; density      : 1.58 points/m²\n#&gt; density      : 1.08 pulses/m²\nFor a more in-depth print out of the data use the function summary() instead of print().",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Reading, Plotting, Querying & Validating</span>"
    ]
  },
  {
    "objectID": "io.html#sec-read",
    "href": "io.html#sec-read",
    "title": "2  Reading, Plotting, Querying & Validating",
    "section": "",
    "text": "The header, which stores summary information about its content, including the bounding box of the file, coordinate reference system, and point format.\nThe payload, i.e., the point cloud itself.\n\n\nlas &lt;- readLAS(\"files.las\")\n\n\n\n\nParameter select\nA LAS file stores the X Y Z coordinates of each point as well as many other data such as intensity, incidence angle, and return sequence position. These data are called attributes. In practice, many attributes are not actually useful but are loaded by default. This can consume a lot of processing memory because R does not allow for choosing data storage modes (see this vignette for more details).\nTo save memory, readLAS() can take an optional parameter select, which enables the user to selectively load the attributes of interest. For example, one can choose to load only the X Y Z attributes.\nlas &lt;- readLAS(\"file.las\", select = \"xyz\")  # load XYZ only\nlas &lt;- readLAS(\"file.las\", select = \"xyzi\") # load XYZ and intensity only\nExamples of other attribute abbreviations are: t - gpstime, a - scan angle, n - number of returns, r - return number, c - classification, s - synthetic flag, k - keypoint flag, w - withheld flag, o - overlap flag (format 6+), u - user data, p - point source ID, e - edge of flight line flag, d - direction of scan flag\n\n\nParameter filter\nWhile select enables the user to choose “columns” (or attributes) while reading files, filter allows selection of “rows” (or points) during the reading process. Removing superfluous data at read time saves memory and increases computation speed. For example, it’s common practice in forestry to process only the first returns.\nlas &lt;- readLAS(\"file.las\", filter = \"-keep_first\") # Read only first returns\nIt is important to understand that the filter option in readLAS() keeps or discards points at read time, i.e., while reading at the C++ level, without involving any R code. For example, the R function filter_poi() may return the same output as the filter option in readLAS():\nlas1 &lt;- readLAS(\"file.las\", filter = \"-keep_first\")\n\nlas2 &lt;- readLAS(\"file.las\")\nlas2 &lt;- filter_poi(las2, ReturnNumber == 1L)\nIn the example above, we are (1) reading only the first returns or (2) reading all the points and then filtering the first returns in R. Both outputs are strictly identical, but the first method is faster and more memory-efficient because it doesn’t load the entire file into R and avoids using extra processing memory. It should always be preferred when possible. Multiple filter commands can be used simultaneously to, for example, read only the first returns between 5 and 50 meters.\nlas &lt;-  readLAS(\"file.las\", filter = \"-keep_first -drop_z_below 5 -drop_z_above 50\")\nThe full list of available commands can be obtained by using readLAS(filter = \"-help\"). Users of LAStools may recognize these commands, as both LAStools and lidR use the same libraries (LASlib and LASzip) to read and write LAS and LAZ files.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Reading, Plotting, Querying & Validating</span>"
    ]
  },
  {
    "objectID": "io.html#sec-asprs-compliance",
    "href": "io.html#sec-asprs-compliance",
    "title": "2  Reading, Plotting, Querying & Validating",
    "section": "2.2 Validating LiDAR Data",
    "text": "2.2 Validating LiDAR Data\nAn important first step in ALS data processing is ensuring that your data is complete and valid according to the ASPRS LAS specifications. Users commonly report bugs arising from invalid data. This is why we introduced the las_check() function to perform a thorough inspection of LAS objects. This function checks whether a LAS object meets the ASPRS LAS specifications and whether it is valid for processing, providing warnings if it does not.\nA common issue is that a LAS file contains duplicate points. This can lead to problems such as trees being detected twice, invalid metrics, or errors in DTM generation. We may also encounter invalid return numbers, incoherent return numbers and number of returns attributes, and invalid coordinate reference systems, among other issues. Always make sure to run the las_check() function before delving deeply into your data.\nlas_check(las)\n#&gt;  Checking the data\n#&gt;   - Checking coordinates... ✓\n#&gt;   - Checking coordinates type... ✓\n#&gt;   - Checking coordinates range... ✓\n#&gt;   - Checking coordinates quantization... ✓\n#&gt;   - Checking attributes type... ✓\n#&gt;   - Checking ReturnNumber validity...\n#&gt;     ⚠ Invalid data: 1 points with a return number equal to 0 found.\n#&gt;  [...]\nA check is performed at read time regardless, but the read time check is not as thorough as las_check() for computation time reasons. For example duplicated points are not checked at read time.\n\nlas &lt;- readLAS(\"data/chap1/corrupted.laz\")\n#&gt; Warning: Invalid data: 174638 points with a 'return number' greater than the\n#&gt; 'number of returns'.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Reading, Plotting, Querying & Validating</span>"
    ]
  },
  {
    "objectID": "io.html#sec-plot",
    "href": "io.html#sec-plot",
    "title": "2  Reading, Plotting, Querying & Validating",
    "section": "2.3 Plotting",
    "text": "2.3 Plotting\nThe lidR package takes advantage of the rgl package to provide a versatile and interactive 3D viewer with points colored by Z coordinates on a black background as default.\n\nBasic 3D rendering\nThe very basic way to render a point cloud is the function plot().\nplot(las)\n\n\n\n\n\nSimple 3D interactive plot of a point cloud\n\n\n\n\nUsers can change the attributes used for coloring by providing the name of the attribute used to colorize the points. The background color of the viewer can also be changed by assigning a color using the bg argument. Axes can also be added and point sizes can be changed.\n\n# Plot las object by scan angle, \n# make the background white, \n# display XYZ axis and  scale colors\nplot(las, color = \"ScanAngleRank\", bg = \"white\", axis = TRUE, legend = TRUE)\n\n\n\n\nSimple 3D interactive plot of a point cloud colored by scan angle, with a white background and a color scale\n\n\n\n\nNote that if your file contains RGB data the string \"RGB\" is supported:\nplot(las, color = \"RGB\")\nThe argument breaks enables to defined more adequate breaks in the color palette for example when intensity contains large outliers. Otherwise the palette range would be too large and most of the values would be considered as “very low”, so everything would appear in the same color.\n\nplot(las, color = \"Intensity\", breaks = \"quantile\", bg = \"white\")\n\n\n\n\nUsing quantiles to color the intensity provides a clear display despite outliers.\n\n\n\n\n\n\nOverlays\nThe package also provides some easy to use functions for common overlay. For example add_dtm3d() to add a digital terrain model (section Chapter 4)) and add_treetops3d() to visualize the output of an individual tree detection (section Section 7.1))\n\nx &lt;- plot(las, bg = \"white\", size = 3)\nadd_dtm3d(x, dtm)\n\n\n\n\n3D interactive rendering of a point cloud with a digital terrain model overlaid.\n\n\n\n\n\nx &lt;- plot(las, bg = \"white\", size = 3)\nadd_treetops3d(x, ttops)\n\n\n\n\n3D interactive rendering of a point cloud with segmented tree tops overlaid.\n\n\n\n\nIt is also possible to combine two point clouds with different color palettes. In the following example we are using a previously classified point cloud. We first separate the vegetation and non vegetation points using filter_poi() and then plot both on top of each other with different colour schemes using add options in plot()\n\nnonveg &lt;- filter_poi(las, Classification != LASHIGHVEGETATION)\nveg &lt;- filter_poi(las, Classification == LASHIGHVEGETATION)\n\nx &lt;- plot(nonveg, color = \"Classification\", bg = \"white\", size = 3)\nplot(veg, add = x)\n\n\n\n\n3D interactive rendering of two point clouds overlaid with different color palettes.\n\n\n\n\n\n\nAdvanced 3D Rendering\nSince lidR is based on rgl, it is easy to add objects to the main rendering using rgl functions such as rgl::point3d(), rgl::text(), rgl::surface3d(), and so on to produce publication-ready renderings. However, lidR introduces an additional challenge: it does not display the points with their actual coordinates. Instead, the points are shifted to be rendered close to (0, 0) due to accuracy issues, as rgl uses float (32-bit decimal numbers) rather than double (64-bit decimal numbers). When plot() is used, it invisibly returns the shift values, which can later be used to realign other objects.\n\noffsets &lt;- plot(las)\nprint(offsets)\n#&gt; [1]  391867.8 3901019.3\n\nThe coordinates of the objects must be corrected to align with the point cloud. In the following we will add lines to render the trunks. We read a file, we locate the trees (see Section 7.1)), we extract the coordinates and sizes of the trees and plot lines with rgl::segment3d().\n\n\nShow the code\nLASfile &lt;- system.file(\"extdata\", \"MixedConifer.laz\", package=\"lidR\")\nlas &lt;- readLAS(LASfile, select = \"xyzc\")\n\n# get the location of the trees\nttops &lt;- locate_trees(las, lmf(ws = 5)) \n\n# plot the point cloud\noffsets &lt;- plot(las, bg = \"white\", size = 3)\nadd_treetops3d(offsets, ttops)\n\n# extract the coordinates of the trees and\n# apply the shift to display the lines\n# in the rendering coordinate system\nx &lt;- sf::st_coordinates(ttops)[,1] - offsets[1] \ny &lt;- sf::st_coordinates(ttops)[,2] - offsets[2] \nz &lt;- ttops$Z\n\n# Build a GL_LINES matrix for fast rendering\nx &lt;- rep(x, each = 2)\ny &lt;- rep(y, each = 2)\ntmp &lt;- numeric(2*length(z)) \ntmp[2*1:length(z)] &lt;- z\nz &lt;- tmp\nM &lt;- cbind(x,y,z)\n\n# Display lines\nrgl::segments3d(M, col = \"black\", lwd = 2)\n\n\n\n\n\n3D interactive rendering of a point cloud with segmented tree tops and trunks overlaid.\n\n\n\n../../../../../../tmp/RtmpNMWMOm/file1737024096ea6.png\n\n\n\nVoxel rendering\nIt is possible to render voxels. This is useful to render the output of the function voxelise_points() or voxel_metrics() for examples.\n\nvox &lt;- voxelize_points(las, 6)\nplot(vox, voxel = TRUE, bg = \"white\")\n\n\n\n\n\n\n\n\n\n\nCross Sections 2D Rendering\nTo better visualize the vertical structure of a point cloud, investigate classification results, or compare the results of different interpolation routines, a cross section can be plotted. To do this, we first need to decide where the cross section will be located (i.e., define the beginning and end) and specify its width. The point cloud can then be clipped, and the X and Z coordinates used to create the plot.\nFor example, to create a 200 m long cross section, we might define the beginning and end, and then use the clip_transect() function to subset the point cloud.\n\np1 &lt;- c(273457, 5274357)\np2 &lt;- c(273542, 5274542)\nlas_tr &lt;- clip_transect(las, p1, p2, width = 5, xz = TRUE)\n\nRendering can be achieved with base plot or ggplot2. Notice the use of payload() to extract the data.frame from the LAS object.\n\nlibrary(ggplot2)\n\nggplot(payload(las_tr), aes(X,Z, color = Z)) + \n  geom_point(size = 0.5) + \n  coord_equal() + \n  theme_minimal() +\n  scale_color_gradientn(colours = height.colors(50))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Reading, Plotting, Querying & Validating</span>"
    ]
  },
  {
    "objectID": "gnd.html",
    "href": "gnd.html",
    "title": "3  Ground Classification",
    "section": "",
    "text": "3.1 Progressive Morphological Filter\nThe implementation of PMF algorithm in lidR is based on the method described in Zhang et al. (2003) with some technical modifications. The original method is raster-based, while lidR performs point-based morphological operations because lidR is a point cloud oriented software. The main step of the methods are summarised in the figure below:\nThe pmf() function requires defining the following input parameters: ws (window size or sequence of window sizes), and th (threshold size or sequence of threshold heights). More experienced users may experiment with these parameters to achieve best classification accuracy, however lidR contains util_makeZhangParam() function that includes the default parameter values described in Zhang et al. (2003).\nLASfile &lt;- system.file(\"extdata\", \"Topography.laz\", package=\"lidR\")\nlas &lt;- readLAS(LASfile, select = \"xyzrn\")\nlas &lt;- classify_ground(las, algorithm = pmf(ws = 5, th = 3))\nWe can now visualize the result:\nplot(las, color = \"Classification\", size = 3, bg = \"white\")\nTo better illustrate the classification results we can generate and plot a cross section of the point cloud (see Section 2.3.5)).\nWe can see that although the classification worked, there are multiple points above terrain that are classified 2 (i.e. “ground” according to ASPRS specifications). This clearly indicates that additional filtering steps are needed and that both ws and th parameters should be adjusted. Below we use multiple values for the two parameters instead of a single value in the example above.\nws &lt;- seq(3, 12, 3)\nth &lt;- seq(0.1, 1.5, length.out = length(ws))\nlas &lt;- classify_ground(las, algorithm = pmf(ws = ws, th = th))\nAfter this adjustment the classification result changed, and points in the canopy are no longer classified as “ground”.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ground Classification</span>"
    ]
  },
  {
    "objectID": "gnd.html#sec-csf",
    "href": "gnd.html#sec-csf",
    "title": "3  Ground Classification",
    "section": "3.2 Cloth Simulation Function",
    "text": "3.2 Cloth Simulation Function\nCloth simulation filtering (CSF) uses the Zhang et al 2016 algorithm and consists of simulating a piece of cloth draped over a reversed point cloud. In this method the point cloud is turned upside down and then a cloth is dropped on the inverted surface. Ground points are determined by analyzing the interactions between the nodes of the cloth and the inverted surface. The cloth simulation itself is based on a grid that consists of particles with mass and interconnections that together determine the three-dimensional position and shape of the cloth.\n\n\n\nThe csf() functions use the default values proposed by Zhang et al 2016 and can be used without providing any arguments.\n\nlas &lt;- classify_ground(las, algorithm = csf())\n\nSimilar to the previous examples, classification results can be assessed using a cross section:\n\n\n\n\n\n\n\n\n\nWhile the default parameters of csf() are designed to be universal and provide accurate classification results, according to the original paper, it’s apparent that the algorithm did not work properly in our example because a significant portion of points located in the ground were not classified. In such cases the algorithm parameters need to be tuned to improve the result. For this particular data set a set of parameters that resulted in an improved classification result were formulated as follows:\n\nmycsf &lt;- csf(sloop_smooth = TRUE, class_threshold = 1, cloth_resolution = 1, time_step = 1)\nlas &lt;- classify_ground(las, mycsf)\n\n\n\n\n\n\n\n\n\n\nWe can also subset only the ground points to display the results in 3D\n\ngnd &lt;- filter_ground(las)\nplot(gnd, size = 3, bg = \"white\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ground Classification</span>"
    ]
  },
  {
    "objectID": "gnd.html#sec-mcc",
    "href": "gnd.html#sec-mcc",
    "title": "3  Ground Classification",
    "section": "3.3 Multiscale Curvature Classification (MCC)",
    "text": "3.3 Multiscale Curvature Classification (MCC)\nMultiscale Curvature Classification (MCC) uses the Evans and Hudak 2016 algorithm originally implemented in the mcc-lidar software.\n\nlas &lt;- classify_ground(las, mcc(1.5,0.3))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ground Classification</span>"
    ]
  },
  {
    "objectID": "gnd.html#sec-edge-artifact",
    "href": "gnd.html#sec-edge-artifact",
    "title": "3  Ground Classification",
    "section": "3.4 Edge Artifacts",
    "text": "3.4 Edge Artifacts\nNo matter which algorithm is used in lidR or other software, ground classification will be weaker at the edges of point clouds as the algorithm must analyze the local neighbourhood (which is missing on edges). To find ground points, an algorithm need to analyze the local neighborhood or local context that is missing at edge areas. When processing point clouds it’s important to always consider a buffer around the region of interest to avoid edge artifacts. lidR has tools to manage buffered tiles and this advanced use of the package will be covered in Chapter 14).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ground Classification</span>"
    ]
  },
  {
    "objectID": "gnd.html#sec-method-selection",
    "href": "gnd.html#sec-method-selection",
    "title": "3  Ground Classification",
    "section": "3.5 How to Choose a Method and Its Parameters?",
    "text": "3.5 How to Choose a Method and Its Parameters?\nIdentifying the optimal algorithm parameters is not a trivial task and often requires several trial runs. lidR proposes several algorithms and may introduce even more in future versions; however, the main goal is to provide a means to compare outputs. We don’t know which one is better or which parameters best suit a given terrain. It’s likely that parameters need to be dynamically adjusted to the local context, as parameters that work well in one file may yield inadequate results in another.\n\n\n\n\n\n\nNote\n\n\n\nlidR is an research and development package, not a production package. The goal of lidR is to provide numerous and handy ways to manipulate, test, compare point cloud processing methods. If available, we recommend using classifications provided by the data provider. The classify_ground() function is useful for small to medium-sized unclassified regions of interest because it is feasible to visually assess classification results. For large acquisitions where visual assessment is no longer feasible, we do not recommend performing ground classification without first studying its accuracy.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ground Classification</span>"
    ]
  },
  {
    "objectID": "gnd.html#sec-other",
    "href": "gnd.html#sec-other",
    "title": "3  Ground Classification",
    "section": "3.6 Other Methods",
    "text": "3.6 Other Methods\nGround segmentation is not limited to the 3 methods described above. Many more have been presented and described in the literature. In Chapter 19 we will learn how to create a plugin algorithm and test it seamlessly in R with lidR using a syntax like:\nlas &lt;- classify_ground(las, algorithm = my_new_method(param1 = 0.05))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ground Classification</span>"
    ]
  },
  {
    "objectID": "dtm.html",
    "href": "dtm.html",
    "title": "4  Digital terrain model",
    "section": "",
    "text": "4.1 Triangular irregular network\nThis method is based on triangular irregular network (TIN) of ground point data to derive a bivariate function for each triangle, which is then used to estimate the values at unsampled locations (between known ground points).\nPlanar facets of each generated triangle are used to interpolate. Used with a Delaunay triangulation, this is the most simple solution because it involves no parameters. The Delaunay triangulation is unique and the linear interpolation is parameter-free. The drawbacks of the method are that it creates a non-smooth DTM and that it cannot extrapolate the terrain outside the convex hull delimited by the ground points since there are no triangle facets outside the convex hull. Moreover, the interpolation is weak at the edges because large irrelevant triangles are often created. It’s therefore important to compute the triangulation with a buffer to be able to crop the DTM and clear the edge artifacts (see Chapter 14).\nTo generate a DTM model with the TIN algorithm we use rasterize_terrain() where algorithm = tin().\ndtm_tin &lt;- rasterize_terrain(las, res = 1, algorithm = tin())\nplot_dtm3d(dtm_tin, bg = \"white\")\nNotice the ugly edge interpolations. This occurs because we didn’t process with a buffer.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Digital terrain model</span>"
    ]
  },
  {
    "objectID": "dtm.html#sec-idw",
    "href": "dtm.html#sec-idw",
    "title": "4  Digital terrain model",
    "section": "4.2 Invert distance weighting",
    "text": "4.2 Invert distance weighting\nInvert distance weighting (IDW) is one of the simplest and most readily available methods that can be applied to create DTMs. It is based on an assumption that the value at an unsampled point can be approximated as a weighted average of values at points within a certain cut-off distance d, or from a given number k of closest neighbours. Weights are usually inversely proportional to a power p of the distance between the location and the neighbour, which leads to the computing of an estimator.\nCompared to tin() this method is more robust to edge artifacts because it uses a more relevant neighbourhood but generates terrains that are “bumpy” and probably not as realistic as those generated using TINs. There are always trade-offs to different methods!\nTo generate a DTM model with the IDW algorithm we use rasterize_terrain() where algorithm = knnidw().\n\ndtm_idw &lt;- rasterize_terrain(las, algorithm = knnidw(k = 10L, p = 2))\nplot_dtm3d(dtm_idw, bg = \"white\") \n\n\n\n\n\n\n\n\nNotice the bumpy nature of the DTM compared to the previous one generated with tin(). In 1D and IDW interpolation looks like:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Digital terrain model</span>"
    ]
  },
  {
    "objectID": "dtm.html#sec-kriging",
    "href": "dtm.html#sec-kriging",
    "title": "4  Digital terrain model",
    "section": "4.3 Kriging",
    "text": "4.3 Kriging\nKriging is the most advanced approach and utilizes advanced geostatistical interpolation methods that take into account the relationships between the returns and their respective distances from each other. lidR uses the package gstat to perform the kriging. This method is very advanced, difficult to manipulate, and extremely slow to compute, but probably provides the best results with minimal edge artifacts.\nTo generate a DTM model with the kriging algorithm we use rasterize_terrain() where algorithm = kriging().\n\ndtm_kriging &lt;- rasterize_terrain(las, algorithm = kriging(k = 40))\nplot_dtm3d(dtm_kriging, bg = \"white\") \n\n\n\n\n\n\n\n\nNotice that the algorithm has issues interpolating regions with missing point such as lakes.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Digital terrain model</span>"
    ]
  },
  {
    "objectID": "dtm.html#sec-dtm-pros-cons",
    "href": "dtm.html#sec-dtm-pros-cons",
    "title": "4  Digital terrain model",
    "section": "4.4 Pros and cons",
    "text": "4.4 Pros and cons\n\nTriangulation is a very fast and efficient method that generates very good DTMs and is robust to empty regions inside the point cloud. It is however weak at edges. Although lidR uses the nearest neighbour to complete the missing pixel out of the convex hull of the ground points the interpolation remains poor. This algorithm must therefore always be used with a buffer of extra points to ensure that the region of interest is not on an edge. The TIN method is recommended for broad DTM computation but should be avoided for small regions of interest loaded without buffers.\nInvert distance weighting is fast, but approximately twice as slower than TIN. The terrain is not very realistic, but edges are likely to be free of strong edge artifacts. IDW is a compromise between TIN and KRIGING. It is recommended if you want a simple method, if you cannot load a buffer, and if edge regions are important.\nKriging is very slow because it is computationally demanding. It is not recommended for use on medium to large areas. It can be used for small plots without buffers to get a nice DTM without strong edges artifact.\n\nWhatever the method used, edges are critical. Results will always be weak if the method needs to guess the local topography with only partial information on the neighborhood. Though different methods provide better and worse estimates in these regions, best practice is to always use a buffer to obtain some information about the neighborhood and remove the buffer once the terrain is computed.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Digital terrain model</span>"
    ]
  },
  {
    "objectID": "dtm.html#sec-dtm-other",
    "href": "dtm.html#sec-dtm-other",
    "title": "4  Digital terrain model",
    "section": "4.5 Other methods",
    "text": "4.5 Other methods\nSpatial interpolation is not limited to the 3 methods described above. Many more have been presented and described in the literature. In Chapter 19 we will learn how to create a plugin algorithm compatible with rasterize_terrain() based on a multilevel B-spline approximation (MBA) using the MBA package.\n\ndtm_mba &lt;- rasterize_terrain(las, algorithm = mba())\nplot_dtm3d(dtm_mba, bg = \"white\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Digital terrain model</span>"
    ]
  },
  {
    "objectID": "dtm.html#sec-hillshade",
    "href": "dtm.html#sec-hillshade",
    "title": "4  Digital terrain model",
    "section": "4.6 Render shaded DTM",
    "text": "4.6 Render shaded DTM\nGenerating a hillshade layer in R is relatively straight forward and is done using functions from the terra package. The terrain() and hillShade() functions can be combined to take the DTM raster layers as input and return a hillshade raster:\n\nlibrary(terra)\ndtm &lt;- rasterize_terrain(las, algorithm = tin())\ndtm_prod &lt;- terrain(dtm, v = c(\"slope\", \"aspect\"), unit = \"radians\")\ndtm_hillshade &lt;- shade(slope = dtm_prod$slope, aspect = dtm_prod$aspect)\nplot(dtm_hillshade, col =gray(0:30/30), legend = FALSE)\n\n\n\n\n\n\n\n\nThe rayshader package also provides interesting tools to generate shaded DTM. The dtm must be a RasterLayer\n\nlibrary(rayshader)\ndtm &lt;- raster::raster(dtm)\nelmat &lt;- raster_to_matrix(dtm)\nmap &lt;- elmat %&gt;%\n  sphere_shade(texture = \"imhof1\", progbar = FALSE) %&gt;%\n  add_water(detect_water(elmat), color = \"imhof1\") %&gt;%\n  add_shadow(ray_shade(elmat, progbar = FALSE), 0.5) %&gt;%\n  add_shadow(ambient_shade(elmat, progbar = FALSE), 0)\n\n2D plot\n\nplot_map(map)\n\n\n\n\n\n\n\n\n3D plot\n\nplot_3d(map, elmat, zscale = 1, windowsize = c(800, 800))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Digital terrain model</span>"
    ]
  },
  {
    "objectID": "normalization.html",
    "href": "normalization.html",
    "title": "5  Height normalization",
    "section": "",
    "text": "5.1 DTM normalization\nTo normalize points using a DTM we first need to create the DTM itself. For this we use the rasterize_terrain() function (see Chapter 4). For this example we chose to use a grid resolution of 1 m and to use the knnidw() algorithm with default parameters.\ndtm &lt;- rasterize_terrain(las, 1, knnidw())\nplot(dtm, col = gray(1:50/50))\nNow that we have our surface and are satisfied with it we can use it to normalize our point cloud through subtraction.\nnlas &lt;- las - dtm\nplot(nlas, size = 4, bg = \"white\")\nWe can see that the point cloud has been normalized, making the point cloud flat. All the elevations are now relative to the ground surface. The ground surface being the reference 0 all the ground points are expected to be at Z = 0 by definition. But are they? Lets look at the distribution of ground points.\nhist(filter_ground(nlas)$Z, breaks = seq(-0.6, 0.6, 0.01), main = \"\", xlab = \"Elevation\")\nWe can see that the ground points are not all at Z=0 and the histogram shows some points at +/- 25 cm. This occurs because the DTM is a discretized raster. The location of the pixels do not match the locations of the ground points. Lets assume we have two ground points with elevations of 257.5 and 258 meters respectively in a given pixel at 257.9 m. After normalization, their respective elevation will be -0.4 m and 0.1 m because each pixel has a single value meaning that all the points within a given pixel get normalized with the exact same elevation value. In a raster, the elevations are a succession of flat areas with discontinuities at each pixel. Thus a simple subtraction of the raster gives good results visually, but in practice can lead to many inaccuracies because of the discretized nature of the storage format.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Height normalization</span>"
    ]
  },
  {
    "objectID": "normalization.html#sec-norm-point-cloud",
    "href": "normalization.html#sec-norm-point-cloud",
    "title": "5  Height normalization",
    "section": "5.2 Point cloud normalization",
    "text": "5.2 Point cloud normalization\nPoint cloud normalization without a DTM interpolates the elevation of every single point locations using ground points. It no longer uses elevations at discrete predefined locations. Thus the methods is exact, computationally speaking. It means that it is equivalent to using a continuous DTM but it is important to recall that all interpolation methods are interpolation and by definition make guesses with different strategies. Thus by “exact” we mean “continuous”. To compute the continuous normalization, we can feed normalize_height() with an algorithm for spatial interpolation instead of a raster.\n\nnlas &lt;- normalize_height(las, knnidw())\n\nAll the ground points should be exactly 0. Let check it:\n\nhist(filter_ground(nlas)$Z, breaks = seq(-0.6, 0.6, 0.01), main = \"\", xlab = \"Elevation\")\n\n\n\n\n\n\n\n\nOne can reproduce this with other algorithm such as tin(). It’s also important to recall buffer and edge artifacts also apply here",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Height normalization</span>"
    ]
  },
  {
    "objectID": "normalization.html#hybrid-method",
    "href": "normalization.html#hybrid-method",
    "title": "5  Height normalization",
    "section": "5.3 Hybrid method",
    "text": "5.3 Hybrid method\nnlas &lt;- las - dtm makes very simple subtractions without any interpolation. normalize_height(las, knnidw()) computes on-the-fly the interpolation of the ground points and estimates the exact elevation of every points. An hybrid method consists in the interpolation of the pixel of an already computed DTM.\n\nnlas &lt;- normalize_height(las, tin(), dtm = dtm)\n\nIn this case, the ground points in las are not considered for interpolation. The DTM is used as regularly spaced ground points that are triangulated.\n\nhist(filter_ground(nlas)$Z, breaks = seq(-0.6, 0.6, 0.01), main = \"\", xlab = \"Elevation\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Height normalization</span>"
    ]
  },
  {
    "objectID": "normalization.html#sec-norm-pros-cons",
    "href": "normalization.html#sec-norm-pros-cons",
    "title": "5  Height normalization",
    "section": "5.4 Pros and cons",
    "text": "5.4 Pros and cons\nPoint cloud based normalization is superior in terms of computational accuracy by normalizing with a continuous terrain instead of a discretized terrain. It is however computationally intensive compared to a raster-based method. In addition raster DTMs are storable on disk and can be loaded quicky to be used to normalize different data sets, while point cloud based methods need to be recomputed for each point cloud. It’s up to the user to choose which method best suits their needs. lidR provides the options. The hybrid method is also computationally demanding because it interpolates even more data (a DTM is usually 1 point per square meter while ground points are usually less than that.)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Height normalization</span>"
    ]
  },
  {
    "objectID": "normalization.html#sec-norm-reverse",
    "href": "normalization.html#sec-norm-reverse",
    "title": "5  Height normalization",
    "section": "5.5 Reversing normalization",
    "text": "5.5 Reversing normalization\nlidR also has the capacity to reverse normalization using the unnormalize_height function. This reverts the normalized point cloud to its pre-normalized state.\nlas &lt;- unnormalize_height(nlas)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Height normalization</span>"
    ]
  },
  {
    "objectID": "dsm.html",
    "href": "dsm.html",
    "title": "6  Digital Surface Model and Canopy Height model",
    "section": "",
    "text": "6.1 Point-to-raster\nPoint-to-raster algorithms are conceptually simple, consisting of establishing a grid at a user defined resolution and attributing the elevation of the highest point to each pixel. Algorithmic implementations are computationally simple and extremely fast. In the first example we will set the pixel size to 1 and set algorithm = p2r().\nchm &lt;- rasterize_canopy(las, res = 1, algorithm = p2r())\ncol &lt;- height.colors(25)\nplot(chm, col = col)\nOne drawback of the point-to-raster method is that some pixels can be empty if the grid resolution is too fine for the available point density. Some pixels may then fall within a location that does not contain any points, and as a result the value is not defined.\nIn the following example we will use the exact same method, but increase the spatial resolution of the raster by changing the pixel size to 0.5 m.\nchm &lt;- rasterize_canopy(las, res = 0.5, algorithm = p2r())\nplot(chm, col = col)\nWe can clearly see that there are a lot of empty pixels in the derived surface that correspond to NA pixels in the point cloud. The spatial resolution was increased, however the CHM contains to many voids.\nOne option to reduce the number of voids in the surface model is to replace every point in the point cloud with a disk of a known radius (e.g. 15 cm). This operation is meant to simulate the fact that the laser footprint is not a point, but rather a circular area. It is equivalent to computing the CHM from a densified point cloud in a way that tries to have a physical meaning.\nchm &lt;- rasterize_canopy(las, res = 0.5, algorithm = p2r(subcircle = 0.15))\nplot(chm, col = col)\nThis CHM is the CHM computed “as if” the point cloud was the following\nWe can zoom in to see the small disks that replace each point.\nThe p2r() function contains one additional argument that allows to interpolate the remaining empty pixels. Empty pixels are interpolated using methods described in section Chapter 4).\nchm &lt;- rasterize_canopy(las, res = 0.5, p2r(0.2, na.fill = tin()))\nplot(chm, col = col)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Digital Surface Model and Canopy Height model</span>"
    ]
  },
  {
    "objectID": "dsm.html#sec-chm-tin",
    "href": "dsm.html#sec-chm-tin",
    "title": "6  Digital Surface Model and Canopy Height model",
    "section": "6.2 Triangulation",
    "text": "6.2 Triangulation\nThe triangulation algorithm works by first creating a triangular irregular network (TIN) using first returns only, followed by interpolation within each triangle to compute an elevation value for each pixel of a raster. In its simplest form, this method consists of a strict 2-D triangulation of first returns. Despite being more complex compared to the point-to-raster algorithm, an advantage of the triangulation approach is that it is parameter free and it does not output empty pixels, regardless of the resolution of the output raster (i.e. the entire area is interpolated).\nLike point-to-raster however, the TIN method can lead to gaps and other noise in the surface - referred to as “pits” - that are attributable to first returns that penetrated deep into the canopy. Pits can make individual tree segmentation more difficult and change the texture of the canopy in a unrealistic way. To avoid this issue the CHM is often smoothed in post processing in an attempt to produce a more realistic surface with fewer pits and less noise. To create a surface model using triangulation we use algorithm = dsmtin().\n\nchm &lt;- rasterize_canopy(las, res = 0.5, algorithm = dsmtin())\nplot(chm, col = col)\n\n\n\n\n\n\n\n\nThe triangulation method may also be weak when a lot of points are missing. We can generate an example using the Topography.laz data set that contains empty lakes.\n\nLASfile &lt;- system.file(\"extdata\", \"Topography.laz\", package = \"lidR\")\nlas2 &lt;- readLAS(LASfile)\nlas2 &lt;- normalize_height(las2, algorithm = tin())\nplot(las2, size = 3, bg = \"white\")\n\n\n\n\n\n\n\n\nIn this case the CHM is incorrectly computed in the empty lakes\n\nchm &lt;- rasterize_canopy(las2, res = 0.5, algorithm = dsmtin())\nplot(chm, col = col)\n\n\n\n\n\n\n\n\nTo fix this, one option is to use the max_edge argument, which defines the maximum edge of a triangle allowed in the Delaunay triangulation. By default this argument is set to 0 meaning that no triangle are removed. If set to e.g. 8 it means that every triangle with an edge longer than 8 will be discarded from the triangulation.\n\nchm &lt;- rasterize_canopy(las2, res = 0.5, algorithm = dsmtin(max_edge = 8))\nplot(chm, col = col)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Digital Surface Model and Canopy Height model</span>"
    ]
  },
  {
    "objectID": "dsm.html#sec-pitfree",
    "href": "dsm.html#sec-pitfree",
    "title": "6  Digital Surface Model and Canopy Height model",
    "section": "6.3 Pit-free algorithm",
    "text": "6.3 Pit-free algorithm\nMore advanced algorithms have also been designed that avoid pits during the computation step instead of requiring post-processing. Khosravipour et al. (2014) proposed a ‘pit-free’ algorithm, which consists of a series of sequential height thresholds where Delaunay triangulations are applied to first returns. For each threshold, the triangulation is cleaned of triangles that are too large, similar to the example given in the previous section. In a final step, the partial rasters are stacked and only the highest pixels of each raster are retained (figure below). The output is a DSM that is expected to be natively free of pits without using any post-processing or correction methods.\n\n\n\nTo understand this method better, we can reproduce the figure above with algorithm = dsmtin():\n\n# The first layer is a regular triangulation\nlayer0 &lt;- rasterize_canopy(las, res = 0.5, algorithm = dsmtin())\n\n# Triangulation of first return above 10 m\nabove10 &lt;- filter_poi(las, Z &gt;= 10)\nlayer10 &lt;- rasterize_canopy(above10, res = 0.5, algorithm = dsmtin(max_edge = 1.5))\n\n# Triangulation of first return above 20 m\nabove20 &lt;- filter_poi(above10, Z &gt;= 20)\nlayer20 &lt;- rasterize_canopy(above20, res = 0.5, algorithm = dsmtin(max_edge = 1.5))\n\n# The final surface is a stack of the partial rasters\ndsm &lt;- layer0\ndsm[] &lt;- pmax(as.numeric(layer0[]), as.numeric(layer10[]), as.numeric(layer20[]), na.rm = T)\n\nlayers &lt;- c(layer0, layer10, layer20, dsm)\nnames(layers) &lt;- c(\"Base\", \"Layer10m\", \"Layer20m\", \"pitfree\")\nplot(layers, col = col)\n\n\n\n\n\n\n\n\nIn practice, the internal implementation of pitfree() works much like the example above but is easier to use.\n\nchm &lt;- rasterize_canopy(las, res = 0.5, pitfree(thresholds = c(0, 10, 20), max_edge = c(0, 1.5)))\nplot(chm, col = col)\n\n\n\n\n\n\n\n\nBy increasing the max_edge argument for the pit-free triangulation part from 1.5 to 2 the CHM becomes smoother but also less realistic:\n\nchm &lt;- rasterize_canopy(las, res = 0.5, pitfree(max_edge = c(0, 2.5)))\nplot(chm, col = col)\n\n\n\n\n\n\n\n\nSimilarly to point-to-raster, the pit-free algorithm in lidR also includes a subcircle option that replaces each first return by a disk made of 8 points. Because it would be too computationally demanding to triangulate 8 times more points, the choice was made to select only the highest point of each pixel after subcircling to perform the triangulation.\n\nchm &lt;- rasterize_canopy(las, res = 0.5, pitfree(subcircle = 0.15))\nplot(chm, col = col)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Digital Surface Model and Canopy Height model</span>"
    ]
  },
  {
    "objectID": "dsm.html#sec-chm-post-process",
    "href": "dsm.html#sec-chm-post-process",
    "title": "6  Digital Surface Model and Canopy Height model",
    "section": "6.4 Post-processing a CHM",
    "text": "6.4 Post-processing a CHM\nCHMs are usually post-processed to smooth or to fill empty pixels. This is mainly because most publications use a point-to-raster approach without any tweaks. One may want to apply a post-processing step to the CHM. Sadly lidR does not provide any tools for that. lidR is a point cloud oriented software. Once a function has returned a raster, the job of the lidR package has ended. Further work requires other dedicated tools to process rasters. The terra package is one of those. The code below presents a simple option to fill NAs and smooth a CHM with the terra package. For more details see the documentation of the package terra.\n\nfill.na &lt;- function(x, i=5) { if (is.na(x)[i]) { return(mean(x, na.rm = TRUE)) } else { return(x[i]) }}\nw &lt;- matrix(1, 3, 3)\n\nchm &lt;- rasterize_canopy(las, res = 0.5, algorithm = p2r(subcircle = 0.15), pkg = \"terra\")\nfilled &lt;- terra::focal(chm, w, fun = fill.na)\nsmoothed &lt;- terra::focal(chm, w, fun = mean, na.rm = TRUE)\n\nchms &lt;- c(chm, filled, smoothed)\nnames(chms) &lt;- c(\"Base\", \"Filled\", \"Smoothed\")\nplot(chms, col = col)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Digital Surface Model and Canopy Height model</span>"
    ]
  },
  {
    "objectID": "itd.html",
    "href": "itd.html",
    "title": "7  Indivitual tree dectection and segmentation",
    "section": "",
    "text": "7.1 Individual Tree Detection (ITD)\nTree tops can be detected by applying a Local Maximum Filter (LMF) on the loaded data set. The LMF in lidR is point cloud-based, meaning that it finds the tree tops from the point cloud without using any raster. The processing, however is actually very similar. For a given point, the algorithm analyzes neighbourhood points, checking if the processed point is the highest. This algorithm can be applied with the lmf() function.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Indivitual tree dectection and segmentation</span>"
    ]
  },
  {
    "objectID": "itd.html#sec-itd",
    "href": "itd.html#sec-itd",
    "title": "7  Indivitual tree dectection and segmentation",
    "section": "",
    "text": "Local Maximum Filter with fixed windows size\nThe LMF can be applied with a constant size windows. Here with a windows size of ws = 5 meters meaning that for a given point the algorithm looks to the neigbourhood points within a 2.5 radius circle to figure out if the point is the local highest. While the algorithm does not need any CHM to work we chose to display the results on top of a CHM for better visualization.\n\nttops &lt;- locate_trees(las, lmf(ws = 5))\n\nplot(chm, col = height.colors(50))\nplot(sf::st_geometry(ttops), add = TRUE, pch = 3)\n\n\n\n\n\n\n\n\nTree detection results can also be visualized in 3D!\n\nx &lt;- plot(las, bg = \"white\", size = 4)\nadd_treetops3d(x, ttops)\n\n\n\n\n\n\n\n\nThe number of detected trees is correlated to the ws argument. Small windows sizes usually gives more trees, while large windows size generally miss smaller trees that are “hidden” by big trees that contain the highest points in the neighbourhood. This can be seen in the figure below where too many trees are found with a small window size and only the dominant trees are found with a large windows size.\n\nttops_3m &lt;- locate_trees(las, lmf(ws = 3))\nttops_11m &lt;- locate_trees(las, lmf(ws = 11))\n\npar(mfrow=c(1,2))\nplot(chm, col = height.colors(50))\nplot(sf::st_geometry(ttops_3m), add = TRUE, pch = 3)\nplot(chm, col = height.colors(50))\nplot(sf::st_geometry(ttops_11m), add = TRUE, pch = 3)\n\n\n\n\n\n\n\n\n\n\nLocal Maximum Filter with variable windows size\nThe examples above demonstrate the lmf() function with a constant window size. A large windows is suitable for large scattered trees while a small windows size if preferable for small and close trees. In reality trees of variable sizes may be present in a single scene leading to sub-optimal outputs. To overcome this issue, a windows size that adapts to the height of pixels or height of the points in our case becomes necessary.\nFor example, a point at 30 m (a big tree) will be tested with a large window, while a point at 10 m (a smaller tree) will be tested with a smaller window. The window size can therefore be defined as a function of height. Tall trees have comparatively larger crowns, needing larger window sizes to accurately detect their treetops. Variable window sizes are especially suitable for stands of more complex structures, or when tree detection is performed on larger areas, covering heterogeneous stands.\nIn lidR a user can design a function that computes a windows size as a function of point (or pixel) height. When designing a function to define the window size based on point heights we need to determine what the minimum and maximum window size should be related to the minimum and maximum tree heights. In general, the minimum window size should not be smaller than 3 meters. Below we show an example where the windows size is related to the point height with an affine relationship. When a point is at 0 the windows size is 3 meters. At 10 m it is 4 m and so on.\n\nf &lt;- function(x) {x * 0.1 + 3}\nheights &lt;- seq(0,30,5)\nws &lt;- f(heights)\nplot(heights, ws, type = \"l\", ylim = c(0,6))\n\n\n\n\n\n\n\n\nWhen applied within lmf(), the function yields the following result:\n\nttops &lt;- locate_trees(las, lmf(f))\n\nplot(chm, col = height.colors(50))\nplot(sf::st_geometry(ttops), add = TRUE, pch = 3)\n\n\n\n\n\n\n\n\nThere is no intrinsic limitation to the user-defined function. One does however need to pay attention to special cases. For example, if a point is below 0 m the function above may compute a negative windows size and the detection will fail. Similarly, if an outlier is encountered a massive window may be computed and lead to suspicious results. We therefore recommend using a more robust function with some built in thresholds.\nIn the next example any points below 2 m will equate to a window size of 3 m, while points above 20 meters equate to a window size of 5 m. Anything between 2 and 20 meter will have a non-linear relationship (for the need of the demo).\n\nf &lt;- function(x) {\n  y &lt;- 2.6 * (-(exp(-0.08*(x-2)) - 1)) + 3\n  y[x &lt; 2] &lt;- 3\n  y[x &gt; 20] &lt;- 5\n  return(y)\n}\n\nheights &lt;- seq(-5,30,0.5)\nws &lt;- f(heights)\nplot(heights, ws, type = \"l\",  ylim = c(0,5))\n\n\n\n\n\n\n\n\n\n\nLocal Maximum Filter on a CHM\nSo far tree detection was performed on a point cloud. A CHM can however also be used to find trees instead of the point cloud. This is also a perfectly valid way to process ALS data. Performing the detection on a CHM is faster because there are less data to process, but it is also more complex because the output depends on how the CHM has been built. The spatial resolution (i.e. pixel size), the algorithm used (see section Chapter 6), and any additional post-processing steps will influence tree detection results.\nIn the examples below we run tree detection on CHMs generated with different algorithms (p2r, pitfree), different resolutions (0.5 and 1 m), and different post-processing smoothing steps with a median filter applied.\nFirst the different CHMs are generated:\n\n# Point-to-raster 2 resolutions\nchm_p2r_05 &lt;- rasterize_canopy(las, 0.5, p2r(subcircle = 0.2), pkg = \"terra\")\nchm_p2r_1 &lt;- rasterize_canopy(las, 1, p2r(subcircle = 0.2), pkg = \"terra\")\n\n# Pitfree with and without subcircle tweak\nchm_pitfree_05_1 &lt;- rasterize_canopy(las, 0.5, pitfree(), pkg = \"terra\")\nchm_pitfree_05_2 &lt;- rasterize_canopy(las, 0.5, pitfree(subcircle = 0.2), pkg = \"terra\")\n\n# Post-processing median filter\nkernel &lt;- matrix(1,3,3)\nchm_p2r_05_smoothed &lt;- terra::focal(chm_p2r_05, w = kernel, fun = median, na.rm = TRUE)\nchm_p2r_1_smoothed &lt;- terra::focal(chm_p2r_1, w = kernel, fun = median, na.rm = TRUE)\n\nThen the same tree detection routine with a constant windows size of 5 m is applied to each CHM:\n\nttops_chm_p2r_05 &lt;- locate_trees(chm_p2r_05, lmf(5))\nttops_chm_p2r_1 &lt;- locate_trees(chm_p2r_1, lmf(5))\nttops_chm_pitfree_05_1 &lt;- locate_trees(chm_pitfree_05_1, lmf(5))\nttops_chm_pitfree_05_2 &lt;- locate_trees(chm_pitfree_05_2, lmf(5))\nttops_chm_p2r_05_smoothed &lt;- locate_trees(chm_p2r_05_smoothed, lmf(5))\nttops_chm_p2r_1_smoothed &lt;- locate_trees(chm_p2r_1_smoothed, lmf(5))\n\nFinally the detection results are visualized to see the various output found as a function of the CHM building choices:\n\npar(mfrow=c(3,2))\ncol &lt;- height.colors(50)\nplot(chm_p2r_05, main = \"CHM P2R 0.5\", col = col); plot(sf::st_geometry(ttops_chm_p2r_05), add = T, pch =3)\nplot(chm_p2r_1, main = \"CHM P2R 1\", col = col); plot(sf::st_geometry(ttops_chm_p2r_1), add = T, pch = 3)\nplot(chm_p2r_05_smoothed, main = \"CHM P2R 0.5 smoothed\", col = col); plot(sf::st_geometry(ttops_chm_p2r_05_smoothed), add = T, pch =3)\nplot(chm_p2r_1_smoothed, main = \"CHM P2R 1 smoothed\", col = col); plot(sf::st_geometry(ttops_chm_p2r_1_smoothed), add = T, pch =3)\nplot(chm_pitfree_05_1, main = \"CHM PITFREE 1\", col = col); plot(sf::st_geometry(ttops_chm_pitfree_05_1), add = T, pch =3)\nplot(chm_pitfree_05_2, main = \"CHM PITFREE 2\", col = col); plot(sf::st_geometry(ttops_chm_pitfree_05_2), add = T, pch =3)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Indivitual tree dectection and segmentation</span>"
    ]
  },
  {
    "objectID": "itd.html#sec-its",
    "href": "itd.html#sec-its",
    "title": "7  Indivitual tree dectection and segmentation",
    "section": "7.2 Individual Tree Segmentation (ITS)",
    "text": "7.2 Individual Tree Segmentation (ITS)\nWhile individual tree detection provides very useful information about tree density and size, one may want to go further to segment and extract each tree individually. Several algorithms are available in lidR and can be divided in two families.\n\nPoint cloud-based that perform without a CHM.\nRaster-based that perform with a CHM.\n\nEach family can be divided into two sub families\n\nAlgorithms that work in two steps - Individual tree detection followed by segmentation.\nAlgorithms that are all-in-one.\n\nIn this section we won’t go through all of these possibilities, but its important to recognize that all algorithms are not well suited to every context. In the examples below we used the dalponte2016() algorithm, because we found it to perform best using the example data.\n\nSegmentation of the point-cloud\nEven when the algorithm is raster-based (which is the case of dalponte2016()), lidR segments the point cloud and assigns an ID to each point by inserting a new attribute named treeID in the LAS object. This means that every point is associated with a particular tree. This is because lidR is point cloud oriented and we want to provide an immediate way to be able to access a segmented point cloud not the segmented raster.\n\nalgo &lt;- dalponte2016(chm_p2r_05_smoothed, ttops_chm_p2r_05_smoothed)\nlas &lt;- segment_trees(las, algo) # segment point cloud\nplot(las, bg = \"white\", size = 4, color = \"treeID\") # visualize trees\n\n\n\n\n\n\n\n\nWe assign an ID to each point because it enables interesting analyses at the point-cloud level. This could involve extracting every tree to derive measurements. In the following we extracted and displayed the tree number 110.\n\ntree110 &lt;- filter_poi(las, treeID == 110)\nplot(tree110, size = 8, bg = \"white\")\n\n\n\n\n\n\n\n\nlidR provides functions that can be used following segment_trees(), like delineation of crown shapes using crown_metrics(). This function computes the hulls (either convex or concave) of each tree as well as user-defined metrics (see also sections Chapter 8, Chapter 9, Chapter 10, Chapter 11, Chapter 12 and Chapter 13):\n\ncrowns &lt;- crown_metrics(las, func = .stdtreemetrics, geom = \"convex\")\nplot(crowns[\"convhull_area\"], main = \"Crown area (convex hull)\")\n\n\n\n\n\n\n\n\n\n\nSegmentation of the CHM\nWhile point cloud segmentation is standard in lidR, users may only have access to a CHM. There are many reasons for only using a CHM, and this is why raster-based methods can be run standalone outside segment_trees(). Whether using the point cloud or a raster, segmentation results will be exactly the same. The difference will be the data format of the segmentation result. In lidR, a LAS object will gain a treeID attribute, while for rasters, delineated crowns are returned in a raster format. To work outside segment_trees() it suffices to call the function standalone like this:\n\nalgo &lt;- dalponte2016(chm_p2r_05_smoothed, ttops_chm_p2r_05_smoothed)\ncrowns &lt;- algo()\n\nplot(crowns, col = pastel.colors(200))\n\n\n\n\n\n\n\n\nThe output is a raster, for which lidR does not provide any processing tools. At this stage it is up to the user to find the required tools to perform more analysis. The terra package is a good place to start!\n\n\nComparaison of tree segmentations\nAt the point cloud level it’s pretty easy to compare tree segmentations by choosing a different attribute names for each method. For example we can compare dalponte2016 (which is a raster based methods in two steps) and li2012 (which is an “all-in-one” point cloud based method) side by side.\n\nalgo1 &lt;- dalponte2016(chm_p2r_05_smoothed, ttops_chm_p2r_05_smoothed)\nalgo2 &lt;- li2012()\nlas &lt;- segment_trees(las, algo1, attribute = \"IDdalponte\")\nlas &lt;- segment_trees(las, algo2, attribute = \"IDli\")\n\nx &lt;- plot(las, bg = \"white\", size = 4, color = \"IDdalponte\", colorPalette = pastel.colors(200))\n#&gt; The argument 'coloPalette' is deprecated. Use 'pal' instead\nplot(las, add = x + c(100,0), bg = \"white\", size = 4, color = \"IDli\", colorPalette = pastel.colors(200))\n#&gt; The argument 'coloPalette' is deprecated. Use 'pal' instead\n\n\n\n\n\n\n\n\nBy comparing their crowns with crown_metrics() we can more easily see that the li2012 algorithm doesn’t perform well in this example with default parameters - maybe some parameter tuning can lead to better results!\n\ncrowns_dalponte &lt;- crown_metrics(las, func = NULL, attribute = \"IDdalponte\", geom = \"concave\")\ncrowns_li &lt;- crown_metrics(las, func = NULL, attribute = \"IDli\", geom = \"concave\")\n\npar(mfrow=c(1,2),mar=rep(0,4))\nplot(sf::st_geometry(crowns_dalponte), reset = FALSE)\nplot(sf::st_geometry(crowns_li), reset = FALSE)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Indivitual tree dectection and segmentation</span>"
    ]
  },
  {
    "objectID": "metrics.html",
    "href": "metrics.html",
    "title": "8  Derived metrics",
    "section": "",
    "text": "8.1 The basics\nWhile there are some differences between functions dedicated to metric computations the basic idea is the same for all of them. In addition to the input point cloud, the user needs to provide a formula to calculate the metric(s) of interest. For example, the average height of points for the point cloud, for each pixel, for each tree crown, for each voxel or for each hexagonal cell can be be calculated using mean(Z).\nAll the functions work the same way but the output format depends on the regularization level. In each case, outputs will be a different class (i.e. a list, a spatial raster, a spatial vector or a data.frame) to take advantage of the best storage method, but the same two metrics will be calculated for every unit of analysis. In the following example we are computing average intensity at different levels of regularization\nLASfile &lt;- system.file(\"extdata\", \"MixedConifer.laz\", package =\"lidR\")\nlas &lt;- readLAS(LASfile)\n\nm &lt;- ~list(avgI = mean(Intensity))\n\na &lt;- pixel_metrics(las, m, res = 5)\nb &lt;- crown_metrics(las, m, geom = \"point\")\nc &lt;- crown_metrics(las, m, geom = \"convex\")\nd &lt;- hexagon_metrics(las, m, area = 25)\n\npar(mfrow=c(2,2))\nplot(a, col = heat.colors(15), legend = FALSE)\nplot(b[\"avgI\"], pal = heat.colors, pch = 19, cex = 1, axes = TRUE, key.pos = NULL, reset = FALSE)\nplot(c[\"avgI\"], pal = heat.colors, axes = TRUE, key.pos = NULL, reset = FALSE)\nplot(d[\"avgI\"], pal = heat.colors, axes = TRUE, key.pos = NULL, reset = FALSE)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Derived metrics</span>"
    ]
  },
  {
    "objectID": "metrics.html#sec-metrics-basics",
    "href": "metrics.html#sec-metrics-basics",
    "title": "8  Derived metrics",
    "section": "",
    "text": "cloud_metrics(las, func = ~mean(Z))\npixel_metrics(las, func = ~mean(Z))\ntree_metrics(las, func = ~mean(Z))\nhexagon_metrics(las, func = ~mean(Z))\nvoxel_metrics(las, func = ~mean(Z))",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Derived metrics</span>"
    ]
  },
  {
    "objectID": "metrics.html#sec-metrics-user-defined",
    "href": "metrics.html#sec-metrics-user-defined",
    "title": "8  Derived metrics",
    "section": "8.2 User-defined metrics",
    "text": "8.2 User-defined metrics\nIn the example above only a single metric is calculated - the mean intensity of the points. Calculations can however be easily extended to any number of user-defined metrics. To do this, users can design custom functions. The function can contain any number of metrics, but needs to return a labeled list. For example, to calculate the mean of elevation, and the standard deviation and mean of intensity, the following function can be used:\n\nf &lt;- function(z, i) {\n  list(\n    mean = mean(z), \n    sd = sd(i),\n    imean = mean(i))\n}\n\nThe user-defined function f can then be used.\ncloud_metrics(las, func = ~f(Z, Intensity))\npixel_metrics(las, func = ~f(Z, Intensity))\ntree_metrics(las, func = ~f(Z, Intensity))\nhaxagon_metrics(las, func = ~f(Z, Intensity))\nvoxel_metrics(las, func = ~f(Z, Intensity))\nWhile any metric can be computed at any level of regularization, it’s important to recognize that they may not all be meaningful. For example, the average elevation of points has a meaning at the pixel level or at the tree level but not at the voxel level. Indeed at the voxel level it corresponds to the average elevation of the points within the voxel i.e. more or less the Z coordinate of the voxel. This in the end needs to be considered by the user. Perhaps asking “does this metric make sense?” is a good place to start!",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Derived metrics</span>"
    ]
  },
  {
    "objectID": "metrics.html#sec-metrics-pre-defined",
    "href": "metrics.html#sec-metrics-pre-defined",
    "title": "8  Derived metrics",
    "section": "8.3 Pre-defined metrics",
    "text": "8.3 Pre-defined metrics\nThe most commonly used metrics are already predefined in lidR - the stdmetrics*() group of functions contain metrics that summarize the vertical distribution of points, their intensities, and return structure.\nThe complete list of all metrics can be found in the lidR wiki page and can be use that way:\ncloud_metrics(las, func = .stdmetrics)\npixel_metrics(las, func = .stdmetrics)\ntree_metrics(las, func = .stdmetrics)\nvoxel_metrics(las, func = .stdmetrics)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Derived metrics</span>"
    ]
  },
  {
    "objectID": "metrics.html#sec-metrics-3rd-party",
    "href": "metrics.html#sec-metrics-3rd-party",
    "title": "8  Derived metrics",
    "section": "8.4 Metrics using 3rd party packages",
    "text": "8.4 Metrics using 3rd party packages\nIn some cases, users may want to calculate metrics from lidar data that are complex or less intuitive to code. In these examples it makes sense to use functions that are already available from other packages. We present 2 pertinent examples below that were brought to our attention by lidR users.\n\nL-moments\nL-moments are linear combinations of ordered data values. In the context of lidar data this relates to elevation and intensity. The ratios of L-moments are comparable to variance, skewness and kurtosis, and have been shown valuable for a number of lidar-derived attribute modeling studies (eg. Valbuena et al. 2017, Adnan et al. 2021) as well as being standard metrics produced in FUSION lidar processing software.\nIn order to calculate these metrics we can leverage the lmom package which provides functions to calculate L-moments and associated ratios. In the example we use the samlmu() function.\ncloud_metrics(las, func = ~as.list(lmom::samlmu(Z)))\npixel_metrics(las, func = ~as.list(lmom::samlmu(Z)), 10) \nWe see that we can supply the func argument in pixel_metrics() with a function from an external package. This allows us to be fairly flexible with the metrics we are able to calculate without needing to introduce dependencies within the lidR package.\n\n\nFractal dimensions\nFractal dimensions are another statistical description that is showing promise within lidar and forestry related research (eg. Saarinen et al., 2021). The use of fractal dimensions seeks to help understand the relationship between structural complexity and stem/crown size and shape.\nThe calculation of fractal dimensions — similarly to L-moments — can be tricky to implement efficiently, and would add an additional dependency to lidR’s growing list. To avoid lidR needing to depend on external packages, we can leverage them (in this case Rdimtools) to calculate fractal dimensions within a pixel_metrics() example.\nFirst, we can create a user-defined function that leverages the est.boxcount() function within Rdimtools. This allows us to define the variables we want the function to utilize within pixel_metrics()\n#create user-defined function\nfd = function(X,Y,Z) {\n  M = cbind(X,Y,Z)\n  est.boxcount(M)$estdim\n}\nNow the we have defined the fd function, which calculates our fractal dimensions using XYZ values, we can apply it on our lidar data and create output rasters.\ncloud_metrics(las, func = ~fd(X,Y,Z), 10)\npixel_metrics(las, func = ~fd(X,Y,Z), 10)\nThere are obviously many potential calculations to apply and statistics to derive from lidar data. The ability to include third party functions like the examples listed above within metric calculations makes lidR valuable and flexible from a research standpoint.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Derived metrics</span>"
    ]
  },
  {
    "objectID": "cloud_metrics.html",
    "href": "cloud_metrics.html",
    "title": "9  Derived metrics at the cloud level",
    "section": "",
    "text": "9.1 Overview\nThe “cloud” level of regularization corresponds to the computation of derived metrics using all available points. As seen in section Chapter 8, calculating derived metrics for the whole point cloud is straightforward and users only need to provide a formula to calculate metric(s) of interest. For example, to calculate the average height (mean(Z)) of all points we can run the following:\nLASfile &lt;- system.file(\"extdata\", \"Megaplot.laz\", package=\"lidR\")\nlas &lt;- readLAS(LASfile)\ncloud_metrics(las, func = ~mean(Z)) # calculate mean height\n#&gt; [1] 13.27202\nTo calculate more than one metric a custom function needs to be used, or one of the pre-defined functions within lidR. To calculate the whole suite of 36 metrics defined in stdmetrics_z() we can use func = .stdmetrics_z. When several metrics are computed they are returned as a list.\nmetrics &lt;- cloud_metrics(las, func = .stdmetrics_z)\nstr(head(metrics)) # output is a list\n#&gt; List of 6\n#&gt;  $ zmax    : num 30\n#&gt;  $ zmean   : num 13.3\n#&gt;  $ zsd     : num 7.45\n#&gt;  $ zskew   : num -0.476\n#&gt;  $ zkurt   : num 2.08\n#&gt;  $ zentropy: num 0.903",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Derived metrics at the cloud level</span>"
    ]
  },
  {
    "objectID": "cloud_metrics.html#sec-cba-applications",
    "href": "cloud_metrics.html#sec-cba-applications",
    "title": "9  Derived metrics at the cloud level",
    "section": "9.2 Applications",
    "text": "9.2 Applications\nPoint cloud metrics become interesting when computed for a set of plot inventories. In this case it can serves to compute a set of metrics for each plot, where known attributes have been measured in the field to construct a predictive model. Users could easily clip and loop through plot inventory files but we defined an all-in-one convenient function plot_metrics(). In the following example we load a .las and compute the metrics for each plot inventory using a shapefile of plot centers\n\nLASfile &lt;- system.file(\"extdata\", \"Megaplot.laz\", package=\"lidR\")\nlas &lt;- readLAS(LASfile, filter = \"-keep_random_fraction 0.5\")\nshpfile &lt;- system.file(\"extdata\", \"efi_plot.shp\", package=\"lidR\")\ninventory &lt;- sf::st_read(shpfile, quiet = TRUE)\nmetrics &lt;- plot_metrics(las, .stdmetrics_z, inventory, radius = 11.28)\n\nLook at the content of inventory and metrics. inventory contains the plot IDs, their coordinates, and VOI a Value Of Interest. metrics contains 36 derived metrics for each plot combined with the inventory data\n\nhead(inventory)\n#&gt; Simple feature collection with 5 features and 2 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 684838.9 ymin: 5017796 xmax: 684976.6 ymax: 5017958\n#&gt; Projected CRS: NAD83 / UTM zone 17N\n#&gt;   IDPEP       VOI                 geometry\n#&gt; 1 PEPQ1 14.157140 POINT (684976.6 5017821)\n#&gt; 2 PEPQ2 12.720584 POINT (684923.9 5017958)\n#&gt; 3 PEPQ3 11.396656 POINT (684838.9 5017942)\n#&gt; 4 PEPQ4 11.597471   POINT (684855 5017891)\n#&gt; 5 PEPQ5  8.263425   POINT (684944 5017796)\nhead(metrics[,1:8])\n#&gt; Simple feature collection with 5 features and 8 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 684838.9 ymin: 5017796 xmax: 684976.6 ymax: 5017958\n#&gt; Projected CRS: NAD83 / UTM zone 17N\n#&gt;   IDPEP       VOI  zmax     zmean      zsd      zskew    zkurt  zentropy\n#&gt; 1 PEPQ1 14.157140 26.51 15.266532 7.501101 -0.9581676 2.687772 0.8737729\n#&gt; 2 PEPQ2 12.720584 25.21 16.672082 6.047432 -1.1462600 3.621553 0.8650151\n#&gt; 3 PEPQ3 11.396656 26.18 16.021130 6.527808 -0.6480083 2.423936 0.9338795\n#&gt; 4 PEPQ4 11.597471 25.56 12.650627 6.382219 -0.2211468 2.577838 0.9059746\n#&gt; 5 PEPQ5  8.263425 21.15  8.158578 5.836870  0.2195201 2.083665 0.9278232\n#&gt;                   geometry\n#&gt; 1 POINT (684976.6 5017821)\n#&gt; 2 POINT (684923.9 5017958)\n#&gt; 3 POINT (684838.9 5017942)\n#&gt; 4   POINT (684855 5017891)\n#&gt; 5   POINT (684944 5017796)\n\nWe have computed many metrics for each plot and we know the value of interest VOI. We can use that to build a linear model with some metrics. Here we have only 5 plots so it is not going to be big science\n\nmodel &lt;- lm(VOI~zsd+zmax, data = metrics)\nsummary(model)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = VOI ~ zsd + zmax, data = metrics)\n#&gt; \n#&gt; Residuals:\n#&gt;       1       2       3       4       5 \n#&gt;  0.4529  1.2803 -1.1660 -0.3993 -0.1680 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)\n#&gt; (Intercept) -11.6898     7.7171  -1.515    0.269\n#&gt; zsd           0.9381     1.4239   0.659    0.578\n#&gt; zmax          0.6925     0.4220   1.641    0.242\n#&gt; \n#&gt; Residual standard error: 1.302 on 2 degrees of freedom\n#&gt; Multiple R-squared:  0.8212, Adjusted R-squared:  0.6424 \n#&gt; F-statistic: 4.592 on 2 and 2 DF,  p-value: 0.1788\n\nThis example can be improved. In Chapter 14 we will study how to extract a ground inventory and in Chapter 16 we will study more in depth modelling presenting a complete workflow from the plot extraction to the mapping of the predictive model.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Derived metrics at the cloud level</span>"
    ]
  },
  {
    "objectID": "grid_metrics.html",
    "href": "grid_metrics.html",
    "title": "10  Derived metrics at the pixel level",
    "section": "",
    "text": "10.1 Overview\nThe “pixel” level of regularization corresponds to the computation of derived metrics for regularly spaced locations in 2D. Derived metrics calculated at pixel level are the basis of the area-based approach (ABA) that we discuss with more details in Chapter 16. In brief, the ABA allows the creation of wall-to-wall predictions of forest inventory attributes (e.g. basal area or total volume per hectare) by linking ALS variables with field measured references. ABA is one application of derived metrics at the pixel level but not the only one.\nAs seen in Chapter 8 and Chapter 9 calculating derived metrics is straightforward. The user only needs to provide a formula to calculate the metric of interest. For example, to calculate the average height (mean(Z)) of all points within 10 x 10 m pixels we can run the following:\nhmean &lt;- pixel_metrics(las, ~mean(Z), 10) # calculate mean at 10 m\nplot(hmean, col = height.colors(50))\nThe returned hmean object is a raster. The default format is terra but an argument pkg allows for RasterLayer or stars outputs:\nhmean\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 50, 50, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 10, 10  (x, y)\n#&gt; extent      : 338000, 338500, 5238500, 5239000  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : WGS 84 / UTM zone 19N (EPSG:32619) \n#&gt; source(s)   : memory\n#&gt; name        :         V1 \n#&gt; min value   :  0.2670795 \n#&gt; max value   : 14.0898504\nAs described in Chapter 8 and Chapter 9, to calculate more than one metric at a time a custom function needs to be created first. The function can contain any number of metrics but needs to return a labeled list. For example, to calculate the mean and standard deviation of point heights, the following function can be created. In this case the return object is a multilayer raster is returned.\nf &lt;- function(x) { # user-defined fucntion\n  list(mean = mean(x), sd = sd(x))\n}\n\nmetrics &lt;- pixel_metrics(las, ~f(Z), 10) # calculate grid metrics\nplot(metrics, col = height.colors(50))\nThe functions that specify which metrics to calculate can of course contain any number of metrics. The most commonly used metrics are already predefined in lidR - the stdmetrics() function contains metrics that summarize the vertical distribution of points, their intensities, and return structure. The complete list of all metrics can be found in the lidR wiki page. To use the predefined list of 56 metrics we can run the pixel_metrics() function as follows:\nmetrics &lt;- pixel_metrics(las, .stdmetrics, 10) # calculate standard metrics\nplot(metrics, col = height.colors(50))\nBecause of the flexibility in defining metrics, it is very easy to extend basic functionality to create new, non-standard metrics. For example, below we demonstrate how the coefficient of variation and inter-quartile range can be calculated:\nmetrics_custom &lt;- function(z) { # user defined function\nlist(\n   coef_var &lt;-   sd(z) / mean(z) * 100, # coefficient of variation\n   iqr &lt;-  IQR(z)) # inter-quartile range\n}\n\nmetrics &lt;- pixel_metrics(las, ~metrics_custom(z=Z), 10) # calculate grid metrics\nplot(metrics, col = height.colors(25))",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Derived metrics at the pixel level</span>"
    ]
  },
  {
    "objectID": "grid_metrics.html#sec-aba-applications",
    "href": "grid_metrics.html#sec-aba-applications",
    "title": "10  Derived metrics at the pixel level",
    "section": "10.2 Applications",
    "text": "10.2 Applications\n\nModeling\nAll *_metrics functions can map any kind of formula as long as it returns a number or a list of numbers, meaning that that it’s possible to input an expression derived from a predictive model to map the resource. In the Chapter 9 we made a model that can be written \\(0.7018 \\times pzabove2 + 0.9268 \\times zmax\\). We can map this predictive model with a resolution of 10 meters:\n\nprediction &lt;- pixel_metrics(las, ~0.7018 * sum(Z &gt; 2)/length(Z) + 0.9268 *max(Z), 20) # predicting model mapping\nplot(prediction, col = height.colors(50)) # some plotting\n\n\n\n\n\n\n\n\n\n\nDensity\nPoint density is the number of points within a pixel divided by the area of the pixel.\n\ndensity &lt;- pixel_metrics(las, ~length(Z)/16, 4) # calculate density\nplot(density, col = gray.colors(50,0,1)) # some plotting\n\n\n\n\n\n\n\n\nWhen using only the first returns, the same formula gives the pulse density instead of the point density\ndensity &lt;- pixel_metrics(las, ~length(Z)/16, 4, filter = ~ReturnNumber == 1L)\n\n\nIntensity\nIt’s possible to generate a map of the average intensity of first return only\n\nimap &lt;- pixel_metrics(las, ~mean(Intensity), 4, filter = ~ReturnNumber == 1L) # mapping average intensity\nplot(imap, col = heat.colors(25))\n\n\n\n\n\n\n\n\n\n\nOther\nMany other raster-based applications can be derived with adequate metrics. In Chapter 17 we will see some out of the box possibilities to demonstrate how the concept of metrics can be leveraged to design new applications. A simple uncommon application could be to map the ratio between multiple returns and single returns.\nTo count single returns we can count the number of points where number of returns equal to 1. To count the number of multiple returns we can count the number of points with a return number equal to 1 AND a return number above 1.\n\nmymetric &lt;- function(return_number, number_of_returns) { #user-defined function\n  nsingle &lt;- sum(number_of_returns == 1L)\n  nmultiple &lt;- sum(return_number == 1L & number_of_returns &gt; 1L)\n  return(list(n_single = nsingle,\n              n_multiple = nmultiple,\n              ratio = nmultiple/nsingle))\n}\n\nrmap &lt;- pixel_metrics(las, ~mymetric(ReturnNumber, NumberOfReturns), 8) # mapping retunrs\nplot(rmap, col = viridis::viridis(50))",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Derived metrics at the pixel level</span>"
    ]
  },
  {
    "objectID": "tree_metrics.html",
    "href": "tree_metrics.html",
    "title": "11  Derived metrics at the tree level",
    "section": "",
    "text": "11.1 Overview\nThe “tree” level of regularization corresponds to the computation of derived metrics centered on each tree using the points that belong to each tree. Derived metrics calculated at tree level are the basis for an inventory at the individual tree level or the basis for individual species identification.\nSimilarly to what we have seen in Chapter 8, Chapter 9, and Chapter 10 calculating derived metrics is straightforward and works exactly the same way as in cloud_metrics() or pixel_metrics(). Derived tree metrics are calculated using the crown_metrics() function. The input data for this function is a point cloud that needs to contain information from tree segmentation (e.g. usually the treeID attribute). In the majority of cases crown_metrics() is run after segmenting tree crowns with segment_trees() (Chapter 7) but the segmentation could also be performed in another way independently of lidR.\nIn the example below we show the basic use of the crown_metrics() function on the files we used in Chapter 7. This file already stores an ID for each point referring to each tree, so we don’t need to perform the segmentation first. We compute two metrics z_max and z_mean using the formula list(z_max = max(Z), z_mean = mean(Z). The output is a sf/sfc_POINT.\nLASfile &lt;- system.file(\"extdata\", \"MixedConifer.laz\", package=\"lidR\") \nlas &lt;- readLAS(LASfile, filter = \"-drop_z_below 0\") # read the file\n\nmetrics &lt;- crown_metrics(las, ~list(z_max = max(Z), z_mean = mean(Z))) # calculate tree metrics\nhead(metrics)\n#&gt; Simple feature collection with 6 features and 3 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XYZ\n#&gt; Bounding box:  xmin: 481263.4 ymin: 3812992 xmax: 481294.7 ymax: 3813011\n#&gt; z_range:       zmin: 9.18 zmax: 26.95\n#&gt; Projected CRS: NAD83 / UTM zone 12N\n#&gt;   treeID z_max    z_mean                       geometry\n#&gt; 1      1 16.00  7.232609  POINT Z (481294.7 3813011 16)\n#&gt; 2      2 26.95 19.401542 POINT Z (481281.9 3813003 2...\n#&gt; 3      3 23.58 18.242222 POINT Z (481278.4 3813002 2...\n#&gt; 4      4 15.83  8.820677 POINT Z (481272.9 3813006 1...\n#&gt; 5      5 20.91 10.642114 POINT Z (481265.7 3812992 2...\n#&gt; 6      6  9.18  2.578750 POINT Z (481263.4 3812992 9...\nThe XYZ coordinates of the points correspond to the XYZ coordinates of the highest point within each tree and each point is associated to 3 attributes treeID + the two user-defined metrics. In the plot below the output is visualized using color to depict the z_max metrics.\nplot(metrics[\"z_max\"], pal = hcl.colors, pch = 19) # plot using z_max\nLike other functions seen in Chapter 9 and Chapter 10, users can create their own custom functions containing all of the metrics of interest. In the example below we show how to calculate metrics that are based both on point heights and intensities.\ncustom_crown_metrics &lt;- function(z, i) { # user-defined function\n  metrics &lt;- list(\n     z_max = max(z),   # max height\n     z_sd = sd(z),     # vertical variability of points\n     i_mean = mean(i), # mean intensity\n     i_max  = max(i)   # max intensity\n   )\n   return(metrics) # output\n}\n\nccm = ~custom_crown_metrics(z = Z, i = Intensity)\ncrown_metrics() can also return sf/sfc_POLYGON by changing the geom argument\nmetrics &lt;- crown_metrics(las, func = ccm, geom = \"convex\")\nplot(metrics[\"z_max\"], pal = hcl.colors)\nmetrics &lt;- crown_metrics(las, func = ccm, geom = \"concave\")\nplot(metrics[\"z_max\"], pal = hcl.colors)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Derived metrics at the tree level</span>"
    ]
  },
  {
    "objectID": "tree_metrics.html#sec-tba-applications",
    "href": "tree_metrics.html#sec-tba-applications",
    "title": "11  Derived metrics at the tree level",
    "section": "11.2 Applications",
    "text": "11.2 Applications\n\nSelection of trees\ncrown_metrics() gives the ID of trees and associated metrics. We can use these data to filter the scene and remove trees with a low intensity.\n\nmetrics &lt;- crown_metrics(las, ~list(imean = mean(Intensity))) # calculate tree intensity metrics\nmetrics &lt;- metrics[metrics$imean &gt; 80,] # filter intensity\n\nsubset &lt;- filter_poi(las, treeID %in% metrics$treeID)\nx &lt;- plot(las, bg = \"white\", size = 4)\nplot(subset, add = x + c(-100, 0), size = 5) # some plotting\n\n\n\n\n\n\n\n\n\n\nTree based inventory\nAssuming we know a relationship between the derived metrics and a value of interest G - such as the biomass of a tree - we can map the resource. Lets assume that \\(G = 0.7 \\times z_{max} + 0.1 \\times i_{mean}\\). In real life a value of interest is more likely to be related to the crown size, but this is a simplified example. First we can compute G for each tree:\n\nmetrics &lt;- crown_metrics(las, func = ~custom_crown_metrics(Z, Intensity)) # calculate intensity metrics\nmetrics$G &lt;- 0.7 * metrics$z_max + 0.1 * metrics$i_mean # set value of interest\nplot(metrics[\"G\"], pal = hcl.colors, pch = 19) # some plotting\n\n\n\n\n\n\n\n\nThen using a raster package, we can rasterize the map. Here we get the sum of G from each tree within each 15 m pixel to get the total value of G for a given pixel. The final output is a predictive model that mixes the area-based approach and the tree-based-approach.\n\n\nr &lt;- terra::rast(ext(las),  resolution = 15)\nv &lt;- terra::vect(metrics[\"G\"])\nmap &lt;- terra::rasterize(v, r, field = \"G\", fun = sum) # extract sum of G at 15m\nplot(map, col = hcl.colors(15)) # some plotting\n\n\n\n\n\n\n\n\nThis is a small example that may not be of interest, but one may imagine or test the result on a bigger data set.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Derived metrics at the tree level</span>"
    ]
  },
  {
    "objectID": "voxel_metrics.html",
    "href": "voxel_metrics.html",
    "title": "12  Derived metrics at the voxel level",
    "section": "",
    "text": "12.1 Overview\nThe “voxel” level of regularization corresponds to the computation of derived metrics for regularly spaced location in 3D. The voxel_metrics() function allows calculation of voxel-based metrics on provided point clouds and works like cloud_metrics(), grid_metrics(), and tree_metrics() seen in Chapter 8, Chapter 9, Chapter 10 and Chapter 11. In the examples below we use the Megaplot.laz data set, but the potential to use voxel_metrics() is particularly interesting for dense point clouds such as those produced by terrestrial lidar, or digital photogrammetry.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Derived metrics at the voxel level</span>"
    ]
  },
  {
    "objectID": "voxel_metrics.html#sec-vba-applications",
    "href": "voxel_metrics.html#sec-vba-applications",
    "title": "12  Derived metrics at the voxel level",
    "section": "12.2 Applications",
    "text": "12.2 Applications\nWe can count the number of points inside each 4 x 4 x 4 m voxel:\n\nLASfile &lt;- system.file(\"extdata\", \"Megaplot.laz\", package=\"lidR\")\nlas &lt;- readLAS(LASfile) # read file\nvox_met &lt;- voxel_metrics(las, ~list(N = length(Z)), 4) # calculate voxel metrics\n\nIn this example the point cloud is first converted into 4 m voxels, then the function length(Z) is applied to all points located inside every voxel. The output is a data.table that contains the X, Y, and Z coordinates of voxels, and the calculated number of points and can be visualized in 3D using the plot() function as follows:\n\nplot(vox_met, color=\"N\", pal = heat.colors, size = 4, bg = \"white\", voxel = TRUE)\n\n\n\n\n\n\n\n\nSimilarly to other *_metrics() functions designed to calculate derived metrics, voxel_metrics() can be used to calculate any number of pre- or user-defined summaries. For example, to calculate minimum, mean, maximum, and standard deviation of intensity in each voxel we can create a following function:\n\ncustom_metrics &lt;- function(x) { # user-defined function\n  m &lt;- list(\n    i_min = min(x),\n    i_mean = mean(x),\n    i_max = max(x),\n    i_sd = sd(x)\n  )\n  return(m) # output\n}\n\nvox_met &lt;- voxel_metrics(las, ~custom_metrics(Intensity), 4) # calculate voxel metrics",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Derived metrics at the voxel level</span>"
    ]
  },
  {
    "objectID": "point_metrics.html",
    "href": "point_metrics.html",
    "title": "13  Derived metrics at point level",
    "section": "",
    "text": "13.1 Overview\nThe “point” level of regularization corresponds to the computation of derived metrics for each point of the point cloud using its neighborhood to define a local subset. The point_metrics() function allows calculation of point-based metrics and works like cloud_metrics(), grid_metrics(), tree_metrics() or voxel_metrics() seen in Chapter 8, Chapter 9, Chapter 10, Chapter 11 and Chapter 12. Refer to these sections for a more comprehensive overview.\nFor each point, the neighbourhood can be either:\nThe syntax for these 3 options is given below:\nLASfile &lt;- system.file(\"extdata\", \"Megaplot.laz\", package=\"lidR\")\nlas &lt;- readLAS(LASfile) # read file\nmetrics &lt;- point_metrics(las, ~list(imean = mean(Intensity)), k = 7)        # 1\nmetrics &lt;- point_metrics(las, ~list(imean = mean(Intensity)), r = 3)        # 2\nmetrics &lt;- point_metrics(las, ~list(imean = mean(Intensity)), k = 7, r = 3) # 3\nThe output is a data.table with the ID of the reference points and the metric(s) for each.\nhead(metrics)\n#&gt;    pointID    imean\n#&gt;      &lt;int&gt;    &lt;num&gt;\n#&gt; 1:       1 41.42857\n#&gt; 2:       2 41.42857\n#&gt; 3:       3 33.71429\n#&gt; 4:       4  3.00000\n#&gt; 5:       5 29.28571\n#&gt; 6:       6 22.00000\nInstead of the ID, one may prefer to get the coordinates with xyz = TRUE:\nIt is also possible to process a selection of points (excluding some points) without creating a copy of the point cloud. In the following, the mean intensity is computed excluding outliers with an intensity above 100. No metric is computed for outliers, and they are not used to get the local neighborhood of the other points.\nmetrics &lt;- point_metrics(las, ~list(imean = mean(Intensity)), k = 7, filter = ~Intensity &lt; 100 ) # calculate mean intensity and exclude outliers\nhead(metrics)\n#&gt;    pointID    imean\n#&gt;      &lt;int&gt;    &lt;num&gt;\n#&gt; 1:       1 41.42857\n#&gt; 2:       2 41.42857\n#&gt; 3:       3 33.71429\n#&gt; 4:       4 16.57143\n#&gt; 5:       5 29.28571\n#&gt; 6:       6 23.14286",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Derived metrics at point level</span>"
    ]
  },
  {
    "objectID": "point_metrics.html#sec-pba-overview",
    "href": "point_metrics.html#sec-pba-overview",
    "title": "13  Derived metrics at point level",
    "section": "",
    "text": "The k nearest neighbours\nThe points within a sphere centred on the processed point, or\nThe k nearest neighbours but with a limited radius search.\n\n\n\n\n\n\nmetrics &lt;- point_metrics(las, ~list(imean = mean(Intensity)), k = 7, xyz = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Derived metrics at point level</span>"
    ]
  },
  {
    "objectID": "point_metrics.html#sec-pba-applications",
    "href": "point_metrics.html#sec-pba-applications",
    "title": "13  Derived metrics at point level",
    "section": "13.2 Applications",
    "text": "13.2 Applications\n\nRoof segmentation\nRoofs are flat areas. Limberger et al. (2015) described a method to find flat areas in a point clouds using an eigen values decomposition. Using a user-defined function we can use point_metrics() to create this algorithm. In this example we will use the following urban scene:\n\nlas &lt;- readLAS(\"data/chap11/building_WilliamsAZ_Urban_normalized.laz\")\nplot(las, size = 3)\n\n\n\n\n\n\n\n\nFirst we need to define a function that computes the eigen value decomposition of a set of points and estimates if the set of points is flat according to the Limberger et al. (2015) criteria.\nis.planar &lt;- function(x, y, z, th1 = 25, th2 = 6) {\n  xyz &lt;- cbind(x,y,z)\n  cov_m &lt;- cov(xyz)\n  eigen_m &lt;- eigen(cov_m)$value\n  is_planar &lt;- eigen_m[2] &gt; (th1*eigen_m[3]) && (th2*eigen_m[2]) &gt; eigen_m[1]\n  return(list(planar = is_planar))\n}\nWe then apply this function using 20 neighbors. We also use filter = ~Classification != LASGROUND to not process points classified as ground. We do this first, because the scene is normalized so by definition each ground point is expected to be part of a flat planar surface, and second, because it will speed-up computation because fewer points will be considered.\nM &lt;- point_metrics(las, ~is.planar(X,Y,Z), k = 20, filter = ~Classification != LASGROUND)\nWe finally merge the output with the point cloud to visualize the result:\nlas &lt;- add_attribute(las, FALSE, \"planar\")\nlas$planar[M$pointID] &lt;- M$planar\nplot(las, color = \"planar\")\n\n\n\n\n\n\n\n\n\nWe can eventually set a valid classification (LASBUILDING) to those points:\nlas$Classification[M$pointID] &lt;- LASBUILDING\nThe function is.planar() is highly inefficient because eigen() is very slow. We can rewrite the eigen decomposition in C++ with Rcpp to make the function 10 times faster!\nRcpp::sourceCpp(code = \"\n#include &lt;RcppArmadillo.h&gt;\n// [[Rcpp::depends(RcppArmadillo)]]\n// [[Rcpp::export]]\narma::vec eigen_values(arma::mat A) {\narma::mat coeff, score;\narma::vec latent;\narma::princomp(coeff, score, latent, A);\nreturn(latent);\n}\")\n\nis.planar &lt;- function(x, y, z, th1 = 25, th2 = 6) {\n  xyz &lt;- cbind(x,y,z)\n  eigen_m &lt;- eigen_values(xyz)\n  is_planar &lt;- eigen_m[2] &gt; (th1*eigen_m[3]) && (th2*eigen_m[2]) &gt; eigen_m[1]\n  return(list(planar = is_planar))\n}\nWe can do this… but we know that not everyone is well versed in C++. lidR therefore has a dedicated and even faster function (segment_shapes()) for this specific task because we believe that it’s of high interest to provide a specialized and faster version of this tool.\nlas &lt;- segment_shapes(las, shp_plane(k = 25), \"planar\")\n\n\nLake and wire segmentation\nEigen value decomposition applied using point_metrics() opens a lot of possibilities. For example, by tweaking the previous example we can design a lake segmentation algorithm. A lake is a planar region with a vertical normal vector, so to create a lake segmentation we simply need to add such constraints in is.planar().\nWe can also design a wire segmentation algorithm. For a wire we can change the Limberger et al. (2015) constrains to enable the detection of elongated linear features instead of flat feature.\n\nis.linear &lt;- function(x, y, z, th = 10) {\n  xyz &lt;- cbind(x,y,z)\n  eigen_m &lt;- eigen_values(xyz)\n  is_linear &lt;-  th*eigen_m[3] &lt; eigen_m[1] && th*eigen_m[2] &lt; eigen_m[1]\n  return(list(linear = is_linear))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgain, lidR has a dedicated function for this specific task because we believe that it is of high interest to provide a specialized and faster version of this tool.\nlas &lt;- segment_shapes(las, shp_line(th = 8, k = 15), \"linear\")\n\n\nMulti-spectral coloring\npoint_metrics() is not limited to eigen value related metrics. It is only limited by a users imagination. In the following example we will demonstrate how it an be used to attribute false colors to a multi-spectral point cloud. Multi-spectral ALS data are sampled with 3 scanners each emitting a different wavelength. The point cloud is usually provided in the form of 3 .las files where each file corresponds to a spectral wavelength. No matter the actual wavelength, we can consider the first band as blue, the second as red and the third as green, and thus consider that each point has a pure color.\n\nf1 &lt;- \"data/chap11/PR1107_c1_ar_c.laz\"\nf2 &lt;- \"data/chap11/PR1107_c2_ar_c.laz\"\nf3 &lt;- \"data/chap11/PR1107_c3_ar_c.laz\"\nlas &lt;- readMSLAS(f1, f2, f3,  filter = \"-keep_z_below 300\")\nplot(las, color = \"ScannerChannel\", size = 5)\n\n\n\n\n\n\n\n\nWe now want to attribute an RGB value to each point. A single point being sampled with a single ‘color’ we need to use its neighborhood to define 3 bands. For each point we look in its neighbourhood, where some points are red, some are blue, and some are green. We average the intensities of each color and we consider these 3 values as the RGB color of the central point. Because some points are likely to have 0 red/blue/green neighbours we can set R, G, and B equal to NA for those points and later discard those points.\n\nset.color &lt;- function(intensity, channel)\n{\n  # Split the intensities of each channel\n  i1 &lt;- intensity[channel == 1]\n  i2 &lt;- intensity[channel == 2]\n  i3 &lt;- intensity[channel == 3]\n  \n  # If one channel is missing return RGB = NA\n  if (length(i1) == 0 | length(i2) == 0 | length(i3) == 0)\n    return(list(R = NA_integer_, G = NA_integer_, B = NA_integer_))\n  \n  # Average and normalise the intensities\n  i1 &lt;- as.integer(mean(i1))\n  i2 &lt;- as.integer(mean(i2))\n  i3 &lt;- as.integer(mean(i3))\n  if (i1 &gt; 255L) i1 &lt;- 255L\n  if (i2 &gt; 255L) i2 &lt;- 255L\n  if (i3 &gt; 255L) i3 &lt;- 255L\n  \n  return(list(R = i1, G = i2, B = i3))\n}\n\nWe can then apply this function with point_metrics() using a spherical neighborhood. The next steps being to assign the output of point_metrics() back into the LAS object for a nice display.\n\nM &lt;- point_metrics(las, ~set.color(Intensity, ScannerChannel), r = 0.5)\nlas &lt;- add_lasrgb(las, M$R, M$G, M$B)\ncolored &lt;- filter_poi(las, !is.na(R)) # remove RGB = NA\nplot(colored, color = \"RGB\", size = 3)\n\n\n\n\n\n\n\n\nThis method is a bit naive ‘as is’. First, the intensities returned by each channel are not comparable and required to be normalized. We could also argue about the choice of discarding RGB = NA. Instead we could have chosen to set a pure color. To finish, enforcing a maximum value to 255 works in this specific example because very few intensity values are actually above 255 but is meaningless in a general case. Either way, this is only a demo to show how to think out of the box with point_metrics().\n\n\nOutlier filtering\nAn outlier is a point that is alone compared to other points. Outliers are usually high points with no close neighbors. The term “no close neighbor” can have several formal definitions such as no neighbors closer than x meters or less than n neighbors within a distance x meter among others. In all cases, it can be estimated with metrics and thus point_metrics() enables the design of an outlier filter method.\nPerhaps a potential exercise for the reader :) .",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Derived metrics at point level</span>"
    ]
  },
  {
    "objectID": "engine.html",
    "href": "engine.html",
    "title": "14  LAScatalog processing engine (1/2)",
    "section": "",
    "text": "14.1 Rationale for LAScatalog functionality\nSo far we have demonstrated how to process a point cloud file using readLAS(). In practice, ALS acquisitions are made up of hundreds or even thousands of files, not being feasible (or possible!) for all to be loaded at once into memory. For example, the image below shows an acquisition split into 320 1 km² tiles:\nThe extraction of regions of interest (ROIs) such as plot inventories becomes difficult in these situations because one must search in which file and sometimes which files the ROI belongs. It also makes the creation of continuous outputs such as a DTM, a raster of metrics, individual tree detection, or anything else far more complicated. One may be tempted to loop though individual files (and we have seen many users doing so) like so:\nThis however is very bad practice because each file is loaded without a buffer, meaning outputs will be invalid or weak at the edges because of the absence of spatial context.\nLet’s compute a DTM (see Chapter 4) as an example, where we use a for loop on 4 contiguous files. We can see the obvious invalidity of the DTM between the files, but also at the periphery, even if less obvious.\nFor comparison, see the accurate DTM below, that was generated with the LAScatalog processing engine. The benefit here is the ability to manage (among a variety of other capabilities) on-the-fly buffering.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>`LAScatalog` processing engine (1/2)</span>"
    ]
  },
  {
    "objectID": "engine.html#sec-engine-rationale",
    "href": "engine.html#sec-engine-rationale",
    "title": "14  LAScatalog processing engine (1/2)",
    "section": "",
    "text": "files &lt;- list.files(\"folder/\")\nfor (file in files) {\n  las &lt;- readLAS(file)\n  output &lt;- do.something(las)\n  write(output, \"path/to/output.ext\")\n}",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>`LAScatalog` processing engine (1/2)</span>"
    ]
  },
  {
    "objectID": "engine.html#sec-engine-engine",
    "href": "engine.html#sec-engine-engine",
    "title": "14  LAScatalog processing engine (1/2)",
    "section": "14.2 The LAScatalog engine",
    "text": "14.2 The LAScatalog engine\nThe lidR package provides a powerful and versatile set of tools to work with collections of files and enables the application of workflow with an automatic management of tile buffering, on disk storage, parallelization of tasks and so on. While the engine is powerful and versatile, it’s also a complex tool, so we have dedicated two section to its description.\nThis section presents the justification for the engine and demonstrates how to use it by applying common lidR functions seen in previous section. The next section goes deeper into the engine to demonstrate how developers can leverage the internal functionality of lidR to create their own applications.\nThe LAScatalog class and the LAScatalog engine are intricately documented in two dedicated vignettes available here and here. The purpose of this book is to propose alternative documentation with more illustrated examples and real use cases.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>`LAScatalog` processing engine (1/2)</span>"
    ]
  },
  {
    "objectID": "engine.html#sec-engine-lascatalog",
    "href": "engine.html#sec-engine-lascatalog",
    "title": "14  LAScatalog processing engine (1/2)",
    "section": "14.3 Read a collection of files",
    "text": "14.3 Read a collection of files\nThe function readLAScatalog() reads the header of all the LAS files of a given folder. The header of a LAS file contains, among others, the bounding box of the point cloud, meaning that it’s possible to know where the file is situated spatially without reading a potentially massive amount of data. readLAScatalog() builds a sf/sfc_POLYGON out of the bounding boxes of the files and presents them in an easily accessible manner.\nctg &lt;- readLAScatalog(\"path/to/folder/\")\n\nctg\n#&gt; class       : LAScatalog (v1.1 format 1)\n#&gt; extent      : 678815.8, 709272.6, 5004393, 5028388 (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : WGS 84 / UTM zone 17N \n#&gt; area        : 295.35 km²\n#&gt; points      : 910.11 million points\n#&gt; density     : 3.1 points/m²\n#&gt; density     : 2.1 pulses/m²\n#&gt; num. files  : 361\nplot(ctg)\n\n\n\n\n\n\n\n\nIn this and the following chapter (Chapter 15) we will use a collection of nine 500 x 500 m files to create examples. The nine files can be downloaded here:\n\ntiles_338000_5238000.laz\ntiles_338000_5238500.laz\ntiles_338000_5239000.laz\ntiles_338500_5238000.laz\ntiles_338500_5238500.laz\ntiles_338500_5239000.laz\ntiles_339000_5238000.laz\ntiles_339000_5238500.laz\ntiles_339000_5239000.laz\n\n\nctg &lt;- readLAScatalog(\"data/ENGINE/catalog/\")\nctg\n#&gt; class       : LAScatalog (v1.0 format 1)\n#&gt; extent      : 338000, 339500, 5238000, 5239500 (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : WGS 84 / UTM zone 19N \n#&gt; area        : 2.25 km²\n#&gt; points      : 9.45 million points\n#&gt; density     : 4.2 points/m²\n#&gt; density     : 3.4 pulses/m²\n#&gt; num. files  : 9\nplot(ctg)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>`LAScatalog` processing engine (1/2)</span>"
    ]
  },
  {
    "objectID": "engine.html#sec-engine-asprs-compliance",
    "href": "engine.html#sec-engine-asprs-compliance",
    "title": "14  LAScatalog processing engine (1/2)",
    "section": "14.4 Validation",
    "text": "14.4 Validation\nSimilar to Chapter 2, an important first step in ALS data processing is ensuring that your data is complete and valid. The las_check() function performs an inspection of LAScatalog objects for file consistency.\nlas_check(ctg)\n#&gt;  Checking headers consistency\n#&gt;   - Checking file version consistency... ✓\n#&gt;   - Checking scale consistency... ✓\n#&gt;   - Checking offset consistency... ✓\n#&gt;   - Checking point type consistency... ✓\n#&gt;   - Checking VLR consistency... ✓\n#&gt;   - Checking CRS consistency... ✓\n#&gt; [...]\nFor a deep (and longer) inspection of each file use deep = TRUE. This will sequentially load each file entirely. It’s thus important to be sure you have enough memory to manage this.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>`LAScatalog` processing engine (1/2)</span>"
    ]
  },
  {
    "objectID": "engine.html#sec-engine-clip",
    "href": "engine.html#sec-engine-clip",
    "title": "14  LAScatalog processing engine (1/2)",
    "section": "14.5 Extract Regions of interest",
    "text": "14.5 Extract Regions of interest\n\nExtract a single ROI\nFunctions using the clip_*() moniker are a good starting point for exploring the capabilities of the LAScataglog engine. clip_*() functions allow for the extraction of a region of interest (ROI) from a point cloud. The following example extracts a circle from a point cloud loaded into memory in a LAS object:\nplot &lt;- clip_circle(las, x = 338700, y = 5238650, radius = 15)\nThis can be extended to a LAScatalog seamlessly. The engine searches in which file(s) the ROI belongs and extracts corresponding regions from all applicable files. The output is a LAS object.\n\nroi &lt;- clip_circle(ctg, x = 338700, y = 5238650, radius = 40)\nplot(roi, bg = \"white\", size = 4)\n\n\n\n\n\n\n\n\n\n\nMultiple extractions\nMultiple extractions is also possible and is performed the same way by searching the corresponding files and then querying in each file no matter if the region of interest is situated in one or several files. The output becomes list of LAS objects.\n\nx &lt;- c(339348.8, 339141.9, 338579.6, 338520.8, 338110.0, 339385)\ny &lt;- c(5239254, 5238717, 5238169, 5239318, 5239247, 5239290)\nr &lt;- 40\n\nplot(ctg)\npoints(x, y)\n\n\n\n\n\n\n\n\nrois &lt;- clip_circle(ctg, x, y, r)\nrois[1:2]\n#&gt; [[1]]\n#&gt; class        : LAS (v1.0 format 1)\n#&gt; memory       : 1.5 Mb \n#&gt; extent       : 339308.9, 339388.7, 5239214, 5239294 (xmin, xmax, ymin, ymax)\n#&gt; coord. ref.  : WGS 84 / UTM zone 19N \n#&gt; area         : 5051 m²\n#&gt; points       : 25.4 thousand points\n#&gt; density      : 5.04 points/m²\n#&gt; density      : 3.9 pulses/m²\n#&gt; \n#&gt; [[2]]\n#&gt; class        : LAS (v1.0 format 1)\n#&gt; memory       : 1.3 Mb \n#&gt; extent       : 339102, 339181.9, 5238677, 5238757 (xmin, xmax, ymin, ymax)\n#&gt; coord. ref.  : WGS 84 / UTM zone 19N \n#&gt; area         : 5013 m²\n#&gt; points       : 21.1 thousand points\n#&gt; density      : 4.2 points/m²\n#&gt; density      : 3.65 pulses/m²",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>`LAScatalog` processing engine (1/2)</span>"
    ]
  },
  {
    "objectID": "engine.html#sec-engine-options",
    "href": "engine.html#sec-engine-options",
    "title": "14  LAScatalog processing engine (1/2)",
    "section": "14.6 Modification of default behavior",
    "text": "14.6 Modification of default behavior\nWhen processing a LAScatalog, no matter with which function, it internally uses what we called the LAScatalog processing engine. This is what happened under the hood when using clip_circle() in above examples. The default behavior of the engine is set in such a way that it returns what is most likely to be expected by the users. However the behavior of the engine can be tuned to optimize processing. Engine options, at the user level, can be modified with opt_*() functions.\nThe goal of this section is to present these options and how they affect the behavior of the engine.\n\nMultiple extractions on disk\nThe engine has the ability to write generated results to disk storage rather than keeping everything in memory. This option can be activated with opt_output_files() &lt;- that is used to designate the path where files will be written to disk. It expects a templated filename so each written file will be attributed a unique name.\nIn the following example, several LAS files will be written to disk with names like 339348.8_5239254_1.las (center coordinates from each file) and the function returns a LAScatalog object that references all the new files instead of a list of LAS object.\n\nopt_output_files(ctg) &lt;- paste0(tempdir(), \"/{XCENTER}_{YCENTER}_{ID}\")\nrois &lt;- clip_circle(ctg, x, y, r)\nrois\n#&gt; class       : LAScatalog (v1.0 format 1)\n#&gt; extent      : 338070, 339424.9, 5238129, 5239358 (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : WGS 84 / UTM zone 19N \n#&gt; area        : 38290.55 m²\n#&gt; points      : 123.8 thousand points\n#&gt; density     : 3.2 points/m²\n#&gt; density     : 2.7 pulses/m²\n#&gt; num. files  : 6\nplot(rois)\n\n\n\n\n\n\n\n\nWe can check the files that were written on disk and see that the names match the template.\n\nrois$filename\n#&gt; [1] \"/tmp/Rtmp2DfSGa/339348.8_5239254_1.las\"\n#&gt; [2] \"/tmp/Rtmp2DfSGa/339141.9_5238717_2.las\"\n#&gt; [3] \"/tmp/Rtmp2DfSGa/338579.6_5238169_3.las\"\n#&gt; [4] \"/tmp/Rtmp2DfSGa/338520.8_5239318_4.las\"\n#&gt; [5] \"/tmp/Rtmp2DfSGa/338110_5239247_5.las\"  \n#&gt; [6] \"/tmp/Rtmp2DfSGa/339385_5239290_6.las\"\n\n\n\nMultiple extraction with point cloud indexation\nPoint cloud indexation is a topic covered by this vignette. In short, LAS file indexation allows for faster queries when extracting ROIs in files. Under the hood lidR relies on LASlib to read files and inherits of the capability to leverage the use of LAX files developed by Martin Isenburg. While extracting hundreds of plots from hundreds of files may take many seconds, the use of index files can reduce processing to few seconds.\n\n\nSummary\nWe have learned several things about the LAScatalog engine in this section:\n\nMany functions of the package work the same either with a point cloud (LAS) or a collection of files (LAScatalog).\nThe behavior of the engine can be modified to write objects to disk instead of loading everything into memory.\nThe engine takes advantage of file indexes.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>`LAScatalog` processing engine (1/2)</span>"
    ]
  },
  {
    "objectID": "engine.html#sec-engine-gnd-classification",
    "href": "engine.html#sec-engine-gnd-classification",
    "title": "14  LAScatalog processing engine (1/2)",
    "section": "14.7 Ground classification",
    "text": "14.7 Ground classification\n\nChunk processing\nclassify_ground() (see Chapter 3) is an important function of lidR, which works seamlessly with the LAScatalog engine. An ALS acquisition is processed in pieces, referred to here as chunks. An acquisition is usually split into chunks where files correspond to a tiling pattern. Using classify_ground() within the LAScatalog engine will process each file sequentially. Given that we emphasize the importance of processing point clouds including a buffer, the engine loads a buffer on-the-fly around each tile before any processing is conducted.\nThe most basic implementation is very similar to examples in Chapter 3. In this case we first specify where to write the outputs using opt_output_files(). The files written on disk will be LAS files, while the output in R will be a LAScatalog. If we don’t write to disk, the result of each chunk will be stored into memory, potentially leading to memory issues.\nopt_output_files(ctg) &lt;- paste0(tempdir(), \"{*}_classified\")\nclassified_ctg &lt;- classify_ground(ctg, csf())\nIn reality, R won’t crash because the function won’t allow users to classify a collection of files without providing a path to save outputs. Most functions in lidR check user inputs to mitigate issues.\n\nopt_output_files(ctg) &lt;- \"\"\nclassified_ctg &lt;- classify_ground(ctg, csf())\n#&gt; Error: This function requires that the LAScatalog provides an output file template.\n\n\n\nModifying buffers\nIt is possible to modify the behavior of the engine by modifying the size of the buffer with opt_chunk_buffer() &lt;-. The option chunk = TRUE of the function plot() allows visualization of how an option affects the processing pattern. The red boundaries show the chunks that will be sequentially processed and the green dotted lines show the extended regions used for buffering each chunk.\n\nopt_chunk_buffer(ctg) &lt;- 20\nplot(ctg, chunk = TRUE)\n\nopt_chunk_buffer(ctg) &lt;- 50\nplot(ctg, chunk = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDepending on the point cloud density and the processes applied, it might be necessary to increase the default buffer. Sometimes no buffer is needed at all. No matter the size, the buffered region is only useful to derive a spatial context that extends the actual size of the chunk. After processing, the buffer is removed and never returned in memory or saved in files. It is only loaded temporarily for computation.\n\n\nModify the chunk size\nIn the example above we have seen that each chunk is actually a file. This is the default behavior because it corresponds to the physical splitting pattern. The engine can however sequentially process a LAScatalog in any arbitrarily sized chunk. This may be useful when each file is too big to be loaded in memory, and reducing the size of each processing region may be suitable. Sometimes increasing chunk size to process more than a single file might be useful as well. This is controlled by opt_chunk_size() &lt;-, and again the function plot() enables the preview of the processing pattern.\n\n# Process sequentially tiny 250 x 250 chunks with 10 m buffer\nopt_chunk_size(ctg) &lt;- 250\nopt_chunk_buffer(ctg) &lt;- 10\nplot(ctg, chunk = TRUE)\n\n# Process sequentially bigger 800 x 800 chunks with 40 m buffer\nopt_chunk_size(ctg) &lt;- 800\nopt_chunk_buffer(ctg) &lt;- 40\nplot(ctg, chunk = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParallel processing\nIn lidR every iterative process can be computed in parallel on a single machine or on multiple machines accessible remotely. Remote parallelization is a very advanced case not covered in this book for which a wiki page has been written.\nIn the following example only local parallelization is covered. Parallelization is driven by the package future. Understanding this package is thus a requirement for being able to parallelize tasks in lidR. In the following example we load future and register a parallelization plan to enable parallelized classification.\nlibrary(future)\nplan(multisession)\nopt_chunk_size(ctg) &lt;- 400\nopt_chunk_buffer(ctg) &lt;- 40\nopt_output_files(ctg) &lt;- paste0(tempdir(), \"/{*}_classified\")\nclassified_ctg &lt;- classify_ground(ctg, csf())\nParallelization is a capability of the engine and every function is capable of benefiting from parallel processing. Registering a parallelization plan would have worked with clip_circle() as well. However doing so does no mean that classification is performed in parallel, but that several chunks will be processed simultaneously. This means that its important to pay attention to memory requirements and parallelization is not necessarily relevant in all cases or in all computers.\n\n\nSummary\nWe have learned several things about the engine in this section\n\nThe engine iteratively processes regions of interest called chunks.\nEach chunk is loaded with a buffer on-the-fly.\nUsers can change chunk sizes to reduce the amount of memory used if files are too big.\nThe buffer is used to remove ridge artifacts, which are removed once the computation is done, and is not transferred to processing outputs.\nIterative processes can be performed in parallel.\nParallelization is performed by processing several chunks simultaneously. Users need processing memory to load several chunks at a time.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>`LAScatalog` processing engine (1/2)</span>"
    ]
  },
  {
    "objectID": "engine.html#sec-engine-dtm",
    "href": "engine.html#sec-engine-dtm",
    "title": "14  LAScatalog processing engine (1/2)",
    "section": "14.8 Digital Terrain Model",
    "text": "14.8 Digital Terrain Model\nSo far we have learned enough about the engine to generate a nice and valid DTM using rasterize_terrain() (see Chapter 4).\n\nIn memory DTM\nGenerating a DTM that encompasses an entire ALS acquisition is as easy as generating a DTM that encompasses a single point cloud file. The following generates a DTM with default parameters (i.e. processing by file with a 30 m buffer). In each chunk rasterize_terrain() computes a DTM. The result is a collection of rasters, however one feature of the engine is to merge everything in single, seamless, manipulable object. A raster is usually less memory intensive that a point cloud, so returning everything in memory is feasible if the raster is not too large.\n\ndtm &lt;- rasterize_terrain(ctg, 2, tin(), pkg = \"terra\")\n\nWe can render a shaded DTM like that from Section 4.6) to better visualize the output:\n\ndtm_prod &lt;- terra::terrain(dtm, v = c(\"slope\", \"aspect\"), unit = \"radians\")\ndtm_hillshade &lt;- terra::shade(slope = dtm_prod$slope, aspect = dtm_prod$aspect)\nplot(dtm_hillshade, col = gray(0:50/50), legend = FALSE)\n\n\n\n\n\n\n\n\n\n\nOn disk DTM\nWhen the DTM is too big for memory storage it can be written on disk. In this case users will have one raster file per chunk. The buffer is used to perform a valid computation but is removed after the computation, leaving no overlap between files. It is possible to build a virtual raster from multiple files to return a lightweight raster that references on disk files.\n\nopt_output_files(ctg) &lt;- opt_output_files(ctg) &lt;- paste0(tempdir(), \"/{*}_dtm\")\ndtm &lt;- rasterize_terrain(ctg, 1, tin())\ndtm\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 1500, 1500, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 1, 1  (x, y)\n#&gt; extent      : 338000, 339500, 5238000, 5239500  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : WGS 84 / UTM zone 19N (EPSG:32619) \n#&gt; source      : rasterize_terrain.vrt \n#&gt; name        :       Z \n#&gt; min value   : 534.024 \n#&gt; max value   : 803.364\nclass(dtm)\n#&gt; [1] \"SpatRaster\"\n#&gt; attr(,\"package\")\n#&gt; [1] \"terra\"\n\nLight rasters can be used as if it were an in memory raster or a single file raster and is thus much more convenient than having hundred of files on disk without any structure to hold them all. This feature is called Virtual Dataset and is part of the Geospatial Data Abstraction Library (GDAL) (see also gdalbuildvrt).\n\n\nSummary\nIn this section we learned several the following about the engine in this section\n\nThe engine takes care of returning a single and manipulable object.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>`LAScatalog` processing engine (1/2)</span>"
    ]
  },
  {
    "objectID": "engine.html#sec-engine-normalization",
    "href": "engine.html#sec-engine-normalization",
    "title": "14  LAScatalog processing engine (1/2)",
    "section": "14.9 Height normalization",
    "text": "14.9 Height normalization\nPrevious sections detail considerations for using the LAScatalog engine to perform point cloud normalization using normalize_height() (see Chapter 5. We created a DTM in the previous section, so we use it here to normalize.\n\nopt_output_files(ctg) &lt;-  paste0(tempdir(), \"/{*}_norm\")\nctg_norm &lt;- normalize_height(ctg, dtm)\n\nA point cloud-based normalization without a raster is also possible:\nopt_output_files(ctg) &lt;-  paste0(tempdir(), \"/{*}_norm\")\nctg_norm &lt;- normalize_height(ctg, tin())",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>`LAScatalog` processing engine (1/2)</span>"
    ]
  },
  {
    "objectID": "engine.html#sec-engine-aba",
    "href": "engine.html#sec-engine-aba",
    "title": "14  LAScatalog processing engine (1/2)",
    "section": "14.10 Area Based Approach",
    "text": "14.10 Area Based Approach\nArea based approach (pixel_metrics()) outputs are raster objects like a DTM. Two more options of the engine do exist that are not controllable in every function. When reading a LAS file with readLAS() two options select and filter are offered (see Chapter 2)). When processing a collection of files readLAS() is called internally and it is not possible to modify these parameters. Instead we can use the options opt_filter() and opt_select() to propagate these arguments.\nopt_select(ctg) &lt;- \"xyzci\"\nopt_filter(ctg) &lt;- \"-keep_first\"\nUsed in conjunction with pixel_metrics(), these enable computation on a selected set of points (first returns for example), loading only user specified attributes. This is useful mainly to save processing memory but may also have a small favorable impact on computation time.\n\nopt_select(ctg_norm) &lt;- \"xyz\"\nhmean &lt;- pixel_metrics(ctg_norm, ~mean(Z), 10)\n\nopt_select(ctg_norm) &lt;- \"xyz\"\nopt_filter(ctg_norm) &lt;- \"-keep_first\"\nhmean &lt;- pixel_metrics(ctg_norm, ~mean(Z), 10)\n\nplot(hmean, col = height.colors(25))\n\n\n\n\n\n\n\n\nNot all functions respect these two options. For example it does not make sense to load only XYZ when creating a DTM because the classification of points is required to isolate ground points. It is also non-beneficial to load intensity, user data, return number and so on to compute a DTM. The function rasterize_terrain() knows which options to choose and does not respect user inputs. This is the case of several other functions in lidR. But when its suitable, users can tune these options.\n\nSummary\nWe have learned several things about the engine in this section\n\nSome options from readLAS()can be used to optimize and tune processing\nSome lidR functions already know the best options and do not respect all user inputs.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>`LAScatalog` processing engine (1/2)</span>"
    ]
  },
  {
    "objectID": "engine.html#sec-engine-itd",
    "href": "engine.html#sec-engine-itd",
    "title": "14  LAScatalog processing engine (1/2)",
    "section": "14.11 Individual Tree Detection",
    "text": "14.11 Individual Tree Detection\nAt this stage with have all the tools required to find each individual tree using locate_trees() in a collection of files. The following example extracts every tree found over the acquisition. The output is returned in memory as a single, continuous object. Each chunk is processed with a buffer ensuring that trees will be properly detected at the edges of the files. A total of 100,000 trees were found in this example.\n\nttops &lt;- locate_trees(ctg_norm, lmf(4))\nttops\n#&gt; Simple feature collection with 102492 features and 2 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 338000 ymin: 5238000 xmax: 339500 ymax: 5239500\n#&gt; Projected CRS: WGS 84 / UTM zone 19N\n#&gt; First 10 features:\n#&gt;    treeID      Z                 geometry\n#&gt; 1       1 17.441 POINT (338007.6 5238481)\n#&gt; 2       2 16.313 POINT (338012.7 5238481)\n#&gt; 3       3 16.342 POINT (338007.1 5238488)\n#&gt; 4       4 16.199 POINT (338007.4 5238491)\n#&gt; 5       5 14.140 POINT (338002.9 5238497)\n#&gt; 6       6 18.999 POINT (338019.3 5238496)\n#&gt; 7       7 16.330 POINT (338012.1 5238499)\n#&gt; 8       8 18.669   POINT (338018 5238492)\n#&gt; 9       9 18.508 POINT (338016.3 5238493)\n#&gt; 10     10 17.622 POINT (338014.6 5238491)\nplot(ttops[\"Z\"], cex = 0.001, pch = 19, pal = height.colors, nbreaks = 30)\n\n\n\n\n\n\n\n\nWe see some issues with this however. Each tree is expected to be associated to a single ID but if we count the number of trees with the ID = 1 we can see that we have 9 trees fitting that query. Similarly we have 9 trees labelled “2” and so on.\n\nsum(ttops$treeID == 1)\n#&gt; [1] 9\nplot(ttops[\"treeID\"], cex = 0.3, pch = 19, nbreaks = 50)\n\n\n\n\n\n\n\n\nThis error has been voluntarily introduced at this stage to illustrate how the LAScatalog engine works. In the engine, each chunk is processed independently of the others. They can even be processed on different machines and in a more or less random order. Thus there is no way to know how many trees were found in the first chunk to restart the numbering in the second chunk. This is mainly because the ‘first and second’ chunk may not have any meaning when processing in parallel. The numbering is thus restarted to 1 in each chunk. This is why there is 9 trees numbered 1. One in each chunk. When the output is returned in memory this can be easily fixed by reassigning new IDs\nttops$treeID &lt;- 1:nrow(ttops)\nIt is however more difficult to achieve the same task if each chunk is written in a file, and almost impossible to achieve in the case of individual tree segmentation as we will see later. To solve this lidR has two strategies to generate reproducible unique IDs no matter the processing order or the number of trees found. For more details the reader can refer to the documentation for locate_trees(). This option can be selected with the parameter uniqueness.\n\nttops &lt;- locate_trees(ctg_norm, lmf(4), uniqueness = \"bitmerge\")\nplot(ttops[\"treeID\"], cex = 0.01, pch = 19)\n\n\n\n\n\n\n\n\nThe attribute treeID is now an arbitrary number (here it is negative but not necessarily) computed from the XY coordinates, that is guaranteed to be 1. unique, and 2. reproducible. No matter how the collection of files is processed (by files or by small chunks, in parallel or sequentially, with large or narrow buffer, in memory or on disk) the IDs will always be the same for a given tree.\nIn all the previous examples we have seen that the engine takes care of merging intermediate results into a single usable object. When intermediate results are stored on disk, the final results in R is a single lightweight object such as a LAScatalog or a Virtual Data set. This is not true for locate_trees() and other functions that return spatial vectors. The default behaviour is to write Geopackage (.gpkg) and there is no way to build a light weight object. What is returned is thus a vector of written files.\n\nopt_output_files(ctg_norm) &lt;- paste0(tempdir(), \"/{*}\")\nlocate_trees(ctg_norm, lmf(4), uniqueness = \"bitmerge\")\n#&gt; [1] \"/tmp/Rtmp2DfSGa/tiles_338000_5238000_1_norm.shp\"\n#&gt; [2] \"/tmp/Rtmp2DfSGa/tiles_338000_5238500_1_norm.shp\"\n#&gt; [3] \"/tmp/Rtmp2DfSGa/tiles_338000_5239000_1_norm.shp\"\n#&gt; [4] \"/tmp/Rtmp2DfSGa/tiles_338500_5238000_1_norm.shp\"\n#&gt; [5] \"/tmp/Rtmp2DfSGa/tiles_338500_5238500_1_norm.shp\"\n#&gt; [6] \"/tmp/Rtmp2DfSGa/tiles_338500_5239000_1_norm.shp\"\n#&gt; [7] \"/tmp/Rtmp2DfSGa/tiles_339000_5238000_1_norm.shp\"\n#&gt; [8] \"/tmp/Rtmp2DfSGa/tiles_339000_5238500_1_norm.shp\"\n#&gt; [9] \"/tmp/Rtmp2DfSGa/tiles_339000_5239000_1_norm.shp\"\n\n\nSummary\nWe have learned several things about the engine in this section\n\nEach chunk is processed strictly independently of the others.\nWhen the intermediate outputs are LAS files, the final output is a lightweight LASctalog. When the intermediate outputs are raster files, the final output is a lightweight Virtual Data set. However for spatial vectors and some other data types it is not always possible to aggregate files into a single lightweight object. Thus the names of the files are returned.\nStrict continuity of the output is not always trivial because each chunk is processed independently of the others but lidR always guarantee the validity and the continuity of the outputs.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>`LAScatalog` processing engine (1/2)</span>"
    ]
  },
  {
    "objectID": "engine.html#sec-engine-its",
    "href": "engine.html#sec-engine-its",
    "title": "14  LAScatalog processing engine (1/2)",
    "section": "14.12 Individual Tree Segmentation",
    "text": "14.12 Individual Tree Segmentation\nIndividual tree segmentation with segment_trees() is the most complicated function to use with a LAScatalog because there are many different algorithms. Some imply the use of an in memory raster and an in memory vector, some require only an in memory raster and some require only a point cloud (see Chapter 7). Moreover it is complex to manage edge artifacts. Imagine there is a tree that is situated exactly on the edge between two files. The first half on one file, the second half on another. The two files will be processed independently - maybe on different remote computers. Both sides of the tree must be attributed the same ID independently to get something that is strictly continuous. The function segment_trees() manages all these points. The example below uses the dalponte2016() algorithm.\nFirst we create a 1 m resolution CHM stored on disk and returned as a light virtual raster.\n\nopt_output_files(ctg_norm) &lt;- paste0(tempdir(), \"/chm_{*}\")\nchm &lt;- rasterize_canopy(ctg_norm, 1, p2r(0.15))\nplot(chm, col = height.colors(50))\n\n\n\n\n\n\n\n\nSecond we compute the seeds by finding the trees as seen above. The result must be loaded in memory because there is no way to combine many vectors stored on disk like rasters. In this example it is possible because there are 100,000 trees. But for bigger collections it may not be possible to apply this algorithm in a simple way.\nopt_output_files(ctg_norm) &lt;- \"\"\nttops &lt;- locate_trees(ctg_norm, lmf(4), uniqueness = \"bitmerge\")\nTo finish, we apply segment_trees(). Here we don’t need to specify any strategy to get unique IDs because the seeds are already uniquely labelled. These IDs will be preserved. But for an algorithm without any seed such as watershed() it is important to have a strategy.\n\nopt_output_files(ctg_norm) &lt;- paste0(tempdir(), \"/{*}_segmented\")\nalgo &lt;- dalponte2016(chm, ttops)\nctg_segmented &lt;- segment_trees(ctg_norm, algo)\n\nThe new LAScatalog that is returned is made up of files that have an extra attribute named treeID where each point is labelled with an ID thats unique for each tree, even those that belong between two or more files that were processed independently. We can load a plot between two files to check:\n\nopt_output_files(ctg_segmented) &lt;- \"\"\nlasplot &lt;- clip_circle(ctg_segmented, 338500, 5238600, 40)\npol = crown_metrics(lasplot, NULL, geom = \"convex\")\n\nplot(sf::st_geometry(pol), col = pastel.colors(250), axes = T)\nplot(ctg, add = T)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can observe that the segmentation is perfect with respect to the labelling problem. Trees that were segmented twice independently on each edge were attributed the same IDs on both sides and that the final output is wall-to-wall.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>`LAScatalog` processing engine (1/2)</span>"
    ]
  },
  {
    "objectID": "engine.html#sec-engine-retile",
    "href": "engine.html#sec-engine-retile",
    "title": "14  LAScatalog processing engine (1/2)",
    "section": "14.13 Retile a catalog",
    "text": "14.13 Retile a catalog\nThe function catalog_retile() allows for retiling an ALS acquisition of files. This is the best example to combine everything we have seen in this section.\n\nRetile a catalog into smaller files\ncatalog_retile() allows for retiling the acquisition of files in tiles of any size.\n\nopt_output_files(ctg) &lt;- paste0(tempdir(), \"/{XLEFT}_{YBOTTOM}\") # label outputs based on coordinates\nopt_chunk_buffer(ctg) &lt;- 0\nopt_chunk_size(ctg) &lt;- 250 # retile to 250 m\nsmall &lt;- catalog_retile(ctg) # apply retile\nplot(small) # some plotting\n\n\n\n\n\n\n\n\n\n\nAdd a buffer around each file\ncatalog_retile() is a special case where the buffer is not removed after computation. The function does not compute anything, but only streams point from inputs files to output files. It can be used to add a buffer around each file.\n\nopt_chunk_buffer(ctg) &lt;- 20 # set buffer to 20\nopt_chunk_size(ctg) &lt;- 0 # no change to chunk size\nopt_output_files(ctg) &lt;- paste0(tempdir(), \"/{ORIGINALFILENAME}_buffered\") # name outputs with original name and \"_buffered\"\nbuffered &lt;- catalog_retile(ctg) # apply buffer\nplot(buffered) # some plotting\n\n\n\n\n\n\n\n\nBe careful. This may be useful for processing in other softwares, but lidR loads a buffers on-the-fly and does not support already buffered files. When processing a buffered collection in lidR the output is likely to be incorrect with the default parameters.\n\n\nCreate a new collection with only first returns\nIn combination with opt_filter(), catalog_retile() can be used to generate a new collection of files that contains only first returns. This is not very useful in lidR because this can be achieved on-the-fly when reading the files using filter() (see Section 2.1.2), though could be useful to process point clouds in other software.\nopt_chunk_buffer(ctg) &lt;- 0\nopt_chunk_size(ctg) &lt;- 0\nopt_output_files(ctg) &lt;- paste0(tempdir(), \"/{ORIGINALFILENAME}_first\")\nopt_filter(ctg) &lt;- \"-keep_first\"\nfirst &lt;- catalog_retile(ctg)\n\n\nCreate a new collection of small and buffered ground returns in parallel\nWe can combine all the options seen in this section to generate a buffered set of tiny files containing only ground returns. We present that here in parallel.\n\nlibrary(future)\nplan(multisession)\n\nopt_output_files(ctg) &lt;- paste0(tempdir(), \"/{XLEFT}_{YBOTTOM}_first_buffered\")\nopt_chunk_buffer(ctg) &lt;- 10\nopt_chunk_size(ctg) &lt;- 250\nopt_filter(ctg) &lt;- \"-keep_class 2\"\nnewctg &lt;- catalog_retile(ctg)\nplot(newctg)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>`LAScatalog` processing engine (1/2)</span>"
    ]
  },
  {
    "objectID": "engine.html#sec-engine-independent-files",
    "href": "engine.html#sec-engine-independent-files",
    "title": "14  LAScatalog processing engine (1/2)",
    "section": "14.14 The case of ground inventories",
    "text": "14.14 The case of ground inventories\nThe case of ground inventories or more generally independent files is particular. We generated such collections in the first example of this section.\n\n\n\n\n\n\n\n\n\nrois is made of unrelated circular files. Loading a buffer is meaningless because there is no neighbouring files. Creating chunks is meaningless as well because we usually want one output per file without generating a wall-to-wall object. Worse even, with the wrong options the output might be incorrect.\nFor example on the top right of the previous collection we can see two overlapping files. If processed by chunk and with a buffer this will load the same points twice in the overlapping area, plus some extra points from the other plot that are not related at all to the first plot. In short, in the case of ground plot inventories 99% of the cases consist in processing iteratively by file, without a buffer and without returning a single aggregated object. This case actually corresponds to a regular for loop, as shown in the introduction of this section. Thus there is no need for all the features provided by the LAScatalog engine and anyone with any background in programming can write a small script that does this job.\nThe engine can however do it with appropriate options. In this case the LAScatalog processing engine becomes more or less a parallelizable for loop with a real time monitoring.\n\nopt_chunk_size(rois) &lt;- 0 # processing by files\nopt_chunk_buffer(rois) &lt;- 0 # no buffer\nopt_wall_to_wall(rois) &lt;- FALSE # disable internal checks to ensure a valid output. Free wheel mode\nopt_merge(rois) &lt;- FALSE\nopt_output_files(rois) &lt;- \"\"\n\ndtm &lt;- rasterize_terrain(rois, 1, tin())\nplot(dtm[[1]], col = gray(1:50/50))\n\nThis can be set in one command:\n\nopt_independent_files(rois) &lt;- TRUE\n\nEverything seen so far remains true. But with these options we are sure to not make mistakes when processing independent files.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>`LAScatalog` processing engine (1/2)</span>"
    ]
  },
  {
    "objectID": "engine.html#sec-engine-summary-sheet",
    "href": "engine.html#sec-engine-summary-sheet",
    "title": "14  LAScatalog processing engine (1/2)",
    "section": "14.15 Summary of available options",
    "text": "14.15 Summary of available options\nThe engine processes the covered area by chunk. A chunk is an arbitrary square region queried from the collection of files. Each chunk is loaded with a buffer, and each chunk correspond to an independent results. All together the chunks tessellate the coverage unless files are not overlapping. Results are merged into a single object at the end of the processing when possible.\n\nopt_chunk_size(ctg) &lt;- 800 controls the size of the chunks. Set 0 use the tiling pattern (processing by file)\nopt_chunk_buffer(ctg) &lt;- 40 controls the size of the buffer around each chunks.\nopt_chunk_alignment(ctg) &lt;- c(100, 200) controls the alignment of the chunks\nopt_output_file(ctg) &lt;- \"{TEMPLATE} redirects the results in files named after the templated name.\nopt_select(ctg) &lt;- \"xyzi\" loads only the attributes of interest when querying a chunk, in this case the xyz and intensity values (see readLAS() - Section 2.1))\nopt_filter(ctg) &lt;- \"-keep_first\" loads only the points of interest when querying a chunk (see readLAS() - Section 2.1)\nopt_wall_to_wall(ctg) &lt;- FALSE disables some internal control that guarantee that output will be strictly wall-to-wall. It should not be used actually because opt_independent_files() should cover the vast majority of cases\nopt_independent_files() &lt;- TRUE to configure the processing for the special case of independent files (typically plot inventories). It turns the engine into a simple loop and the notion of chunks covering the area is lost.\nopt_merge(ctg) &lt;- FALSE disables the final merging. In this case a list is returned in all cases.\nopt_progress(ctg) &lt;- FALSE disables progress estimation ticker.\nopt_stop_early(ctg) &lt;- FALSE disable early exit. If a chunk throws an error the processing keeps going and this chunk will be missing in the output.\nUse future and register a parallelization plan to process several chunks at a time taking advantage of multiple cores or multiple machines architectures.\n\nOne can use the function summary() to display the main current processing options of the catalog:\n\nsummary(ctg)\n#&gt; class       : LAScatalog (v1.0 format 1)\n#&gt; extent      : 338000, 339500, 5238000, 5239500 (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : WGS 84 / UTM zone 19N \n#&gt; area        : 2.25 km²\n#&gt; points      : 9.45 million points\n#&gt; density     : 4.2 points/m²\n#&gt; density     : 3.4 pulses/m²\n#&gt; num. files  : 9 \n#&gt; proc. opt.  : buffer: 10 | chunk: 250\n#&gt; input opt.  : select: * | filter: -keep_class 2\n#&gt; output opt. : on disk | w2w guaranteed | merging enabled\n#&gt; drivers     :\n#&gt;  - Raster : format = GTiff  NAflag = -999999  \n#&gt;  - stars : NA_value = -999999  \n#&gt;  - Spatial : overwrite = FALSE  \n#&gt;  - SpatRaster : overwrite = FALSE  NAflag = -999999  \n#&gt;  - SpatVector : overwrite = FALSE  \n#&gt;  - LAS : no parameter\n#&gt;  - sf : quiet = TRUE  \n#&gt;  - data.frame : no parameter",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>`LAScatalog` processing engine (1/2)</span>"
    ]
  },
  {
    "objectID": "engine2.html",
    "href": "engine2.html",
    "title": "15  LAScatalog processing engine (2/2)",
    "section": "",
    "text": "15.1 catalog_apply()\nIn this section we outline the catalog_apply() function, which is used subliminally in every function seen in the Chapter 14. Here we show how to leverage its versatility to design tools that do not yet exist, like the normalization + colorization example from above, but also more innovative processes.\nAgain, the LAScatalog class and the LAScatalog engine are deeply documented in a two dedicated vignettes available here and here. The purpose of these examples is to provide alternative, hands-on documentation with more applied examples and real use cases.\nIn this section we will use the same 9 files used in Chapter 14 and an RGB raster that encompasses the collection of files.\nctg &lt;- readLAScatalog(\"data/ENGINE/catalog/\")\nplot(ctg)\nThe RGB image is available for download here.\nrgbmap &lt;- terra::rast(\"data/ENGINE/catalog/rgb.tif\")\nterra::plotRGB(rgbmap)",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>LAScatalog processing engine (2/2)</span>"
    ]
  },
  {
    "objectID": "engine2.html#sec-engine-user-function-generic",
    "href": "engine2.html#sec-engine-user-function-generic",
    "title": "15  LAScatalog processing engine (2/2)",
    "section": "15.2 Create user-defined function",
    "text": "15.2 Create user-defined function\nFirst we need to create a function that combines the normalization + colorization steps. Let’s call it norm_color(). It’s simple and uses only 2 functions.\n\nnorm_color &lt;- function(las, rgbmap) # create user-defined function\n{\n  nlas &lt;- normalize_height(las, tin()) # normalize\n  colorized &lt;- merge_spatial(nlas, rgbmap) # colorize\n  return(colorized) # output\n}\n\nLet’s try it on a sample plot.\n\nlas &lt;- clip_circle(ctg, 338800, 5238500, 40)\nnlasrgb &lt;- norm_color(las, rgbmap) # apply user defined function\n\nplot(nlasrgb, color = \"RGB\", bg = \"white\", size = 6) # some plotting\n\n\n\n\n\n\n\n\nIt works! We have a workable function that performs on a point cloud. Remember that norm_color() is just an example. You could instead choose to make an algorithm for individual tree detection or a method to sample point of interest, let your imagination run wild!\nSo far it works only when using a LAS object. Now let’s make it working with a LAScatalog.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>LAScatalog processing engine (2/2)</span>"
    ]
  },
  {
    "objectID": "engine2.html#sec-engine-user-function-extended",
    "href": "engine2.html#sec-engine-user-function-extended",
    "title": "15  LAScatalog processing engine (2/2)",
    "section": "15.3 Create an intermediate function for catalog_apply()",
    "text": "15.3 Create an intermediate function for catalog_apply()\nThe simple way is:\nnew_ctg &lt;- catalog_map(ctg, norm_color, rgbmap = rgbmap)\nHowever, here we will learn the hard way in order to understand better how it works internally and understand the full potential of catalog_apply() for which catalog_map() is only a simplified version.\nThe core engine is the function catalog_apply() but this function does not work with any user-defined function. User-defined functions must respect a specific template (take a look at the documentation in R) and is expected to perform some specific tasks. First, the primary variable must be a chunk from the catalog (see chunk below), the function must read the chunk, check that it is not empty, then perform the computation.\nA valid function is the following:\n\nnorm_color_chunk &lt;- function(chunk, rgbmap) # user defined function\n{\n  las &lt;- readLAS(chunk)                  # read the chunk\n  if (is.empty(las)) return(NULL)        # check if it actually contain points\n  nlasrgb &lt;- norm_color(las, rgbmap) # apply computation of interest\n  return(nlasrgb) # output\n}\n\nThis introduces an additional level of complexity that is crucial. First, catalog chunks will be sequentially fed into the user’s function. Each chunk is read inside the user-defined function with all processing options being automatically respected (filter, select, chunk_size, chunk_buffer etc.). The las variable in the code snippet above is a point cloud extracted from the catalog that contains the chunks + a buffer. It’s important to check that the loaded portion of the collection is not empty or subsequent code will likely fail. This may happen depending on the size of the chunk and the filter options chosen.\nThis function is now catalog_apply() compatible and can be applied over the entire ALS acquisition. The output will be a point cloud, so we need to pay attention mitigate memory issues and save to disk. At this stage, we haven’t mitigated that problem yet so we add it below.\n\nopt_output_files(ctg) &lt;- paste0(tempdir(), \"/{*}_norm_rgb\") # write to disk\noutput &lt;- catalog_apply(ctg, norm_color_chunk, rgbmap = rgbmap) # implement user-defined function using catalog_apply\nstr(output)\n#&gt; List of 9\n#&gt;  $ : chr \"/tmp/RtmpTQwSlm/tiles_338000_5238000_1_norm_rgb.las\"\n#&gt;  $ : chr \"/tmp/RtmpTQwSlm/tiles_338000_5238500_1_norm_rgb.las\"\n#&gt;  $ : chr \"/tmp/RtmpTQwSlm/tiles_338000_5239000_1_norm_rgb.las\"\n#&gt;  $ : chr \"/tmp/RtmpTQwSlm/tiles_338500_5238000_1_norm_rgb.las\"\n#&gt;  $ : chr \"/tmp/RtmpTQwSlm/tiles_338500_5238500_1_norm_rgb.las\"\n#&gt;  $ : chr \"/tmp/RtmpTQwSlm/tiles_338500_5239000_1_norm_rgb.las\"\n#&gt;  $ : chr \"/tmp/RtmpTQwSlm/tiles_339000_5238000_1_norm_rgb.las\"\n#&gt;  $ : chr \"/tmp/RtmpTQwSlm/tiles_339000_5238500_1_norm_rgb.las\"\n#&gt;  $ : chr \"/tmp/RtmpTQwSlm/tiles_339000_5239000_1_norm_rgb.las\"\n\nWe can see that the output is a list with the name of each file written to disk. This is the default behavior of the engine. It returns a list with one element per chunk. This isn’t really convenient. We have seen in Chapter 14 that in the case of LAS files we can return a LAScatalog, which is far more convenient.\nWe can use the option automerge = TRUE so catalog_apply() will automatically merge the list into something more user-friendly.\n\nopt_output_files(ctg) &lt;- paste0(tempdir(), \"/{*}_norm_rgb\")\noptions &lt;- list(automerge = TRUE) # merge all the outputs\noutput &lt;- catalog_apply(ctg, norm_color_chunk, rgbmap = rgbmap, .options = options)\noutput\n#&gt; class       : LAScatalog (v1.0 format 3)\n#&gt; extent      : 338000, 339500, 5238000, 5239500 (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : WGS 84 / UTM zone 19N \n#&gt; area        : 2.62 km²\n#&gt; points      : 1.07 million points\n#&gt; density     : 0.4 points/m²\n#&gt; density     : 0.2 pulses/m²\n#&gt; num. files  : 9\n\nWe now have a LAscatalog. It is much better but:\n\nplot(output)\n\n\n\n\n\n\n\n\nWe still have a problem here. The files are overlapping because we read each file with a buffer and then the outputs have been written with their buffer. This is bad practice. It’s important to always remove the buffer after the computation. In this specific case, the output is a LAS file. When readLAS() reads a chunk from the catalog the points in the buffer are flagged so they can be easily manipulated. Since they are flagged, we can easily remove these points by adding this step to our user-defined function. It is always the role of the user to remove the buffer of the output.\n\nnorm_color_chunk &lt;- function(chunk, rgbmap)\n{\n  las &lt;- readLAS(chunk)\n  if (is.empty(las)) return(NULL)\n  nlasrgb &lt;- norm_color(las, rgbmap)\n  nlasrgb &lt;- filter_poi(nlasrgb, buffer == 0) # remove buffer\n  return(nlasrgb)\n}\n\nopt_output_files(ctg) &lt;- paste0(tempdir(), \"/{*}_norm_rgb\")\noptions &lt;- list(automerge = TRUE)\noutput &lt;- catalog_apply(ctg, norm_color_chunk, rgbmap = rgbmap, .options = options)\nplot(output)\n\n\n\n\n\n\n\n\nWhen the outputs are spatial vectors or spatial rasters they can be cropped using the bounding box of the chunk accessible via extent(chunk) or bbox(chunk) or st_bbox(chunk) or ext(chunk) that match respectively with raster, sp, sf/stars and terra.\nNice! We created a custom function – norm_color() – and we upscale its definition to capably and efficiently process an entire ALS acquisition made up of hundreds of files. Using options described in Chapter 14, we can also introduce parallelization, chunk size control, and buffer size control.\nlibrary(future)\nplan(multisession)\nopt_filter(ctg) &lt;- \"-keep_class 2\"\nopt_chunk_size(ctg) &lt;- 300\nopt_chunk_buffer(ctg) &lt;- 40\nopt_output_files(ctg) &lt;- paste0(tempdir(), \"/{*}_norm_rgb\")\noptions &lt;- list(automerge = TRUE)\noutput &lt;- catalog_apply(ctg, norm_color_chunk, rgbmap = rgbmap, .options = options)\nWe can check that it worked by loading a sample from somewhere in our new catalog. According to the options put above, we are expecting to get a normalized (we normalized), colored (we merged RGB image), ground points (we used -keep_class 2).\n\nopt_output_files(output) &lt;- \"\"\nlas &lt;- clip_circle(output, 338800, 5238500, 60)\nplot(las, color = \"RGB\", size = 6, bg = \"white\")",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>LAScatalog processing engine (2/2)</span>"
    ]
  },
  {
    "objectID": "engine2.html#sec-engine-user-function-lascatalog",
    "href": "engine2.html#sec-engine-user-function-lascatalog",
    "title": "15  LAScatalog processing engine (2/2)",
    "section": "15.4 Make a user-friendly function for third party users",
    "text": "15.4 Make a user-friendly function for third party users\nWhen designing tools that will be used by third parties, we would like to hide catalog_apply() and intermediate functions inside a more high level and user-friendly function similarly to lidR functions that work the same both with a LAS or a LAScatalog. Moreover we would like to make the function foolproof for users. To do this we can create wrap our codes if if-else statements to apply to correct code to the correct object.\n\n# Create a generic function\nnorm_color &lt;- function(las, rgbmap)\n{\n  if (is(las, \"LAScatalog\")) {\n    options &lt;- list(automerge = TRUE)\n    output &lt;- catalog_apply(ctg, norm_color, rgbmap = rgbmap, .options = options)\n    return(output)\n  } else if (is(las, \"LAScluster\")) {\n    x &lt;- readLAS(las)\n    if (is.empty(x)) return(NULL)\n    nlasrgb &lt;- norm_color(x, rgbmap)\n    nlasrgb &lt;- filter_poi(nlasrgb, buffer == 0)\n    return(nlasrgb)\n  } else if (is(las, \"LAS\")) {\n    nlas &lt;- normalize_height(las, tin())\n    colorized &lt;- merge_spatial(nlas, rgbmap)\n    return(colorized)\n  } else {\n    stop(\"Not supported input\")\n  }\n}\n\nWe now have a single function that can be used seamlessly on a LAS or a LAScatalog object.\nlas_norm_colored &lt;-  norm_color(las, rgbmap)\n\nopt_output_files(ctg) &lt;- paste0(tempdir(), \"/{*}_norm_rgb\")\nctg_norm_colored &lt;-  norm_color(ctg, rgbmap)\nThe different steps to apply to a LAScluster (i.e. readLAS + remove buffer of the output) are, in this case, very common. It is possible to use catalog_map() instead, that is capable of doing this tasks for you. But in some cases the output may be not supported, because the cropping step is more complex. In those cases, catalog_apply() must be used because it allows for a more accurate control.\n\n# Create a generic function\nnorm_color &lt;- function(las, rgbmap)\n{\n  if (is(las, \"LAScatalog\")) {\n    output &lt;- catalog_map(ctg, norm_color, rgbmap = rgbmap, .options = options)\n    return(output)\n  } else if (is(las, \"LAS\")) {\n    nlas &lt;- normalize_height(las, tin())\n    colorized &lt;- merge_spatial(nlas, rgbmap)\n    return(colorized)\n  } else {\n    stop(\"Not supported input\")\n  }\n}",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>LAScatalog processing engine (2/2)</span>"
    ]
  },
  {
    "objectID": "engine2.html#sec-engine-user-function-safe",
    "href": "engine2.html#sec-engine-user-function-safe",
    "title": "15  LAScatalog processing engine (2/2)",
    "section": "15.5 Make a safe function for third party users",
    "text": "15.5 Make a safe function for third party users\nAt this stage our function is almost finished. However it is not foolproof. What if the user of this new function does not provide any output path template? The point cloud in each chunk will be loaded in memory and will be retained in memory until it eventually becomes full and R crashes. We must prevent the use of the function if no path to the disk is given.\nAlso, the computation of a DTM requires a buffer. If the user sets a 0 m buffer the output of this function will be incorrect. We must prevent the use of the function if a buffer of 0 is given. These two cases are covered by the engine with the options need_output_file = TRUE and need_buffer = TRUE. To finish we would like to disable the ability to tune the select option to ensure no attribute is lost.\n\nnorm_color &lt;- function(las, rgbmap)\n{\n  if (is(las, \"LAScatalog\")) {\n    opt_select(las) &lt;- \"*\" # disable select tuning\n    options &lt;- list(automerge = TRUE, need_output_file = TRUE, need_buffer = TRUE) # require output path & buffer size\n    output &lt;- catalog_map(ctg, norm_color, rgbmap = rgbmap, .options = options)\n    return(output)\n  } else if (is(las, \"LAS\")) {\n    nlas &lt;- normalize_height(las, tin())\n    colorized &lt;- merge_spatial(nlas, rgbmap)\n    return(colorized)\n  } else {\n    stop(\"Not supported input\")\n  }\n}\n\nWe can test what happens if we use this function naively.\n\nopt_output_files(ctg) &lt;- \"\"\nctg_norm_colored &lt;- norm_color(ctg, rgbmap)\n#&gt; Error: This function requires that the LAScatalog provides an output file template.\n\nIt failed with an informative message!\nWe are done. This is exactly how the lidR package works and we have presented all the tools needed to extend it with new applications.\nMore examples can be found further along in this book. For example, Section 17.1.1 presents how to compute a raster of the average distance between first and last returns, and Section 17.1.2 presents how to compute a rumple index of the canopy from the point cloud and, of course, the documentation of the package itself is more comprehensive than this book.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>LAScatalog processing engine (2/2)</span>"
    ]
  },
  {
    "objectID": "modelling.html",
    "href": "modelling.html",
    "title": "16  The Area-Based Approach (ABA) to forest modelling",
    "section": "",
    "text": "16.1 Read in data\nThe function readLAScatalog() seen in Chapter 14 builds a LAScatalog object from a folder. Make sure the ALS files have associated index files - for details see: Speed-up the computations on a LAScatalog.\nNow that our ALS data is prepared lets read in our inventory data using the st_read() function from the sf package. We read in a .shp file where each features is a point with a unique plot ID and corresponding plot level inventory summaries.\nOur plots object contains 203 plots with coordinates of plot centers, plot name, and stand attributes: Lorey’s height (HL), Basal area (BA), and gross timber volume (GTV).\nTo visualize where the plots are in our study area we can overlay them on the LAScatalog.\nWe have now prepared both our ALS and plot inventory data and can begin ABA processing.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The Area-Based Approach (ABA) to forest modelling</span>"
    ]
  },
  {
    "objectID": "modelling.html#sec-modeling-read-data",
    "href": "modelling.html#sec-modeling-read-data",
    "title": "16  The Area-Based Approach (ABA) to forest modelling",
    "section": "",
    "text": "ctg &lt;- readLAScatalog(\"folder/\")\nprint(ctg)\n#&gt; class       : LAScatalog\n#&gt; extent      : 297317.5 , 316001 , 5089000 , 5099000 (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : +proj=utm +zone=18 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \n#&gt; area        : 125.19 km²\n#&gt; points      : 1.58 billion points\n#&gt; density     : 12.6 points/m²\n#&gt; num. files  : 138\nplot(ctg)\n\n\n\n\nplots &lt;- st_read(\"PRF_plots.shp\")\n\nplots\n#&gt; Simple feature collection with 203 features and 4 fields\n#&gt; geometry type:  POINT\n#&gt; dimension:      XY\n#&gt; bbox:           xmin: 299051.3 ymin: 5089952 xmax: 314849.4 ymax: 5098426\n#&gt; epsg (SRID):    NA\n#&gt; proj4string:    +proj=utm +zone=18 +ellps=GRS80 +units=m +no_defs\n#&gt; First 5 features:\n#&gt;    PlotID       HL       BA      GTV                 geometry\n#&gt; 1  PRF002 20.71166 38.21648 322.1351 POINT (313983.8 5094190)\n#&gt; 2  PRF003 19.46735 28.92692 251.1327 POINT (312218.6 5091995)\n#&gt; 3  PRF004 21.74877 45.16215 467.1033 POINT (311125.1 5092501)\n#&gt; 4  PRF005 27.76175 61.55561 783.9303 POINT (313425.2 5091836)\n#&gt; 5  PRF006 27.26387 39.78153 508.0337 POINT (313106.2 5091393)\n\nplot(ctg)\nplot(plots, add = TRUE, col = \"red\")",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The Area-Based Approach (ABA) to forest modelling</span>"
    ]
  },
  {
    "objectID": "modelling.html#sec-modeling-build-data",
    "href": "modelling.html#sec-modeling-build-data",
    "title": "16  The Area-Based Approach (ABA) to forest modelling",
    "section": "16.2 ABA database building",
    "text": "16.2 ABA database building\nTo build a database ready for modeling the steps are the following:\n\nClip the regions of interest (stands) using a shapefile of stand locations.\nCalculate derived metrics for each stand.\nAssociate stand attributes (ID,HL,BA,GTV) to the metrics.\n\nThis can be done with a single function plot_metrics(). We also use the opt_filter() function on the LAScatalog to ignore noise below 0 m at read time so that they do not influence future processing. We clip discs with a radius of 14.1 meters because our plot radius is 14.1 m.\nopt_filter(ctg) &lt;- \"-drop_z_below 0\" # Ignore points with elevations below 0\n\nD &lt;- plot_metrics(ctg, .stdmetrics_z, plots, radius = 14.1)\nWe have now calculated a variety of ALS metrics for each plot.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The Area-Based Approach (ABA) to forest modelling</span>"
    ]
  },
  {
    "objectID": "modelling.html#sec-modeling-build-model",
    "href": "modelling.html#sec-modeling-build-model",
    "title": "16  The Area-Based Approach (ABA) to forest modelling",
    "section": "16.3 Statistical modeling",
    "text": "16.3 Statistical modeling\nHere we provide a simple example of how to create an OLS model (lm) for Lorey’s Mean Height (HL). Using the summary and plot functions we found that the 85th percentile of height (zq85) for ALS metrics explain a large amount of variation in HL values. We are more than happy with this simple model.\nm &lt;- lm(HL ~ zq85, data = D)\nsummary(m)\n#&gt; Call:\n#&gt; lm(formula = HL ~ zq85, data = D)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -4.640 -1.053 -0.031  1.030  4.258 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  2.29496    0.31116   7.376  4.2e-12 ***\n#&gt; zq85         0.93389    0.01525  61.237  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n#&gt; \n#&gt; Residual standard error: 1.5 on 201 degrees of freedom\n#&gt; Multiple R-squared:  0.9491, Adjusted R-squared:  0.9489 \n#&gt; F-statistic:  3750 on 1 and 201 DF,  p-value: &lt; 2.2e-16\nHere we visualize the relationship between the observed (measured) and predicted (ALS-based) HL values.\nplot(D$HL, predict(m))\nabline(0,1)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The Area-Based Approach (ABA) to forest modelling</span>"
    ]
  },
  {
    "objectID": "modelling.html#wall-to-wall-modelling",
    "href": "modelling.html#wall-to-wall-modelling",
    "title": "16  The Area-Based Approach (ABA) to forest modelling",
    "section": "16.4 Wall to wall modelling",
    "text": "16.4 Wall to wall modelling\nNow that we have created a model using plot data, we need to apply that model across our entire study area.\nTo do so we first need to generate the same suite of ALS metrics (.stdmetrics_z) for our ALS data using the pixel_metrics() function on our original collection of files. Note that the res parameter is set to 25 because we want the resolution of our metrics to match the area of our sample plots (14.1 m radius = 625m2).\nmetrics_w2w &lt;- pixel_metrics(ctg, .stdmetrics_z, res = 25, pkg = \"terra\")\nTo visualize any of the metrics you can use the plot function.\nplot(metrics_w2w$zq85)\nplot(metrics_w2w$zmean)\nplot(metrics_w2w$pzabovezmean)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The Area-Based Approach (ABA) to forest modelling</span>"
    ]
  },
  {
    "objectID": "modelling.html#calculate-wall-to-wall-predictions",
    "href": "modelling.html#calculate-wall-to-wall-predictions",
    "title": "16  The Area-Based Approach (ABA) to forest modelling",
    "section": "16.5 Calculate wall-to-wall predictions",
    "text": "16.5 Calculate wall-to-wall predictions\nWe can use the predict() function to produce HL_pred which is a wall-to-wall raster of predicted Lorey’s mean height.\nHL_pred &lt;- predict(metrics_w2w, m)\nTo visualize wall-to-wall predictions use\nplot(HL_pred)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>The Area-Based Approach (ABA) to forest modelling</span>"
    ]
  },
  {
    "objectID": "outbox.html",
    "href": "outbox.html",
    "title": "17  Thinking outside the box",
    "section": "",
    "text": "17.1 New complex metrics in ABA",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Thinking outside the box</span>"
    ]
  },
  {
    "objectID": "outbox.html#sec-outbox-custom-metrics",
    "href": "outbox.html#sec-outbox-custom-metrics",
    "title": "17  Thinking outside the box",
    "section": "",
    "text": "Distance between returns\nIn forestry, simple metrics can be derived from first returns. But what about metrics that estimate the average distance between first and last returns? Is it a valuable metric for characterizing of forest stucture? Can we refine the prediction with such metric? Nobody knows, but we can try.\nFirst, we need to retrieve each emitted pulse and keep only the first and last point of each sequence excluding sequences with only one return.\n\nlas &lt;- retrieve_pulses(las)\nlas &lt;- filter_firstlast(las)\nlas &lt;- filter_poi(las, NumberOfReturns &gt; 1)\n\nUsing the data.table syntax we can compute the distance between first and last returns for each pulse. It could be done with dplyr as well using group_by() %&gt;%  mutate() but we are proponent of using data.table().\nlas@data[, d := Z[1]-Z[2], by = pulseID]\nAt this stage our data set contains distances twice. 1 associated with the first return and 2 with the last return. We only need a single value, so lets keep only first returns. Some distances are NA and correspond to pulses at the very edge of the file. We can discard those case for the moment.\n\nlas &lt;- filter_first(las)\nlas &lt;- filter_poi(las, !is.na(d))\n\nWe can now plot the first returns colored by their distance to the last return.\n\nplot(las, bg = \"white\", color = \"d\", pal = viridis::magma(50), size = 4)\n\n\n\n\n\n\n\n\nNow we can compute the average in an ABA (see Chapter 10).\n\nd_first_last &lt;- pixel_metrics(las, ~mean(d), 15)\nplot(d_first_last, col = height.colors(50))\n\n\n\n\n\n\n\n\nNow we have a working method, we can extend the idea to apply it over a broad coverage using the LAScatalog processing engine (Chapter 14). First we create a function that does this job. This is not mandatory but factorizing the code is good practice.\n\ngrid_distance_first_last &lt;- function(las, res) {\n  las &lt;- retrieve_pulses(las)\n  las &lt;- filter_firstlast(las)\n  las &lt;- filter_poi(las, NumberOfReturns &gt; 1)\n  las@data[, d := Z[1]-Z[2], by = pulseID]\n  las &lt;- filter_first(las)\n  las &lt;- filter_poi(las, !is.na(d))\n  return(pixel_metrics(las, ~mean(d), res))\n}\n\nTo finish we use the LAScatalog processing engine with catalog_map(). We need to process with a buffer to avoid the case where pulses are separated in two files, so we use opt_chunk_buffer(ctg) &lt;- 2. In addition, with the output being a raster, we ensure alignment of the chunk with the raster.\nctg &lt;- readLAScatalog(\"folder/\")\nopt_chunk_buffer(ctg) &lt;- 2\noptions &lt;- list(raster_alignment = 15)\nmetrics &lt;- catalog_map(ctg, grid_distance_first_last, res = 15, .options = options)\nAnd we are done. Our innovative metric is computing with real-time monitoring. It can run in parallel on a single machine or in parallel on several machines. At the end, the output is a wall-to-wall raster map of the average distance between first and last returns for beams that return multiple returns. In 19 lines of code!\nplot(metrics, col = height.colors(50))\n\n\n\n\n\n\n\n\n\n\n\nRumple index\nAnother interesting metric that could be computed is the rumple index (see Kane et al. (2008)).\nTo compute a rumple index we can triangulate points, measure the surface created, and divide by the projected surface. Rumple index is however expected to measure surface roughness of the canopy and it doesn’t makes sense to triangulate every point. An option is to triangulate each first return but yet first returns are not all representative of the canopy. In the following example we are using the surface points every 1 m.\n\ngrid_rumple_index &lt;- function(las, res) { # user-defined function\n  las &lt;- filter_surfacepoints(las, 1)\n  return(pixel_metrics(las, ~rumple_index(X, Y, Z), res))\n}\n\nThen the code is pretty much the same than in previous section\nctg &lt;- readLAScatalog(\"folder/\")\nopt_chunk_buffer(ctg) &lt;- 1\noptions &lt;- list(raster_alignment = 20)\nmetrics &lt;- catalog_map(ctg, grid_rumple_index, res = 20, .options = options)\nplot(metrics, col = height.colors(50))",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Thinking outside the box</span>"
    ]
  },
  {
    "objectID": "outbox.html#sec-outbox-multispectral-coloring",
    "href": "outbox.html#sec-outbox-multispectral-coloring",
    "title": "17  Thinking outside the box",
    "section": "17.2 Multi-spectral coloring",
    "text": "17.2 Multi-spectral coloring\nWe have already seen a multi-spectral coloring example in Section 13.2.3). This is truly a good example of how to think outside the box with lidR, so we show several variants here. The goal is to use multi-spectral data to generate false coloring and see how false coloring may be used for further analyses.\nMulti-spectral ALS data are sampled with 3 devices each emitting a different wavelength. The point cloud is usually provided in the form of 3 LAS files, each file corresponding to a spectral wavelength. No matter the actual wavelength, we can consider the first band as blue, the second as red, and the third as green, and thus consider that each point has a pure color.\n\nf1 &lt;- \"data/chap11/PR1107_c1_ar_c.laz\"\nf2 &lt;- \"data/chap11/PR1107_c2_ar_c.laz\"\nf3 &lt;- \"data/chap11/PR1107_c3_ar_c.laz\"\nlas &lt;- readMSLAS(f1, f2, f3,  filter = \"-keep_z_below 300\")\nplot(las, color = \"ScannerChannel\", size = 6)\n\n\n\n\n\n\n\n\nEach channel’s returns vary in intensities as seen in the figure below because of the wavelength reflectance properties of the targets. A first step could be to normalize the intensities. For this example we won’t do that to keep things simple.\n\nlibrary(ggplot2)\nggplot(las@data) +\n  aes(x = Intensity, fill = as.factor(ScannerChannel)) + \n  geom_density(alpha = 0.5) + \n  theme_minimal() +\n  theme(legend.position = c(.9, .90),\n        legend.title = element_blank())\n#&gt; Warning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n#&gt; 3.5.0.\n#&gt; ℹ Please use the `legend.position.inside` argument of `theme()` instead.\n\n\n\n\n\n\n\n\nIn this specific example, notice that only few intensities are above 255, So intensity ranges from 0 to almost 255. This is very convenient because we will be able to use the raw values as color without transformation. We will simply force values above 255 to be 255. This is a dirty solution but fair enough for the example which is not about intensity normalization.\n\nlas@data[Intensity &gt; 255L, Intensity := 255L]\n\nTo finish, we define a function that takes the intensities and the associated channel as inputs. This function decomposes the 3 channels, computes the average intensity of each channel, and returns these 3 values that will later be interpreted as RGB. Also if one color is missing we force RGB to be NA.\n\ncolorize &lt;- function(intensity, channel)\n{\n  # Split the intensities of each channel\n  i1 &lt;- intensity[channel == 1L]\n  i2 &lt;- intensity[channel == 2L]\n  i3 &lt;- intensity[channel == 3L]\n  \n  # If one channel is missing return RGB = NA\n  if (length(i1) == 0 | length(i2) == 0 | length(i3) == 0)\n    return(list(R = NA_integer_, G = NA_integer_, B = NA_integer_))\n  \n  i1 &lt;- as.integer(mean(i1))\n  i2 &lt;- as.integer(mean(i2))\n  i3 &lt;- as.integer(mean(i3))\n  \n  return(list(R = i1, G = i2, B = i3))\n}\n\nNow we can use this function with different levels of regularization. We can use colorize() with grid_metrics() in an ABA. It returns a multi-layer raster that can be interpreted as RGB\n\nrgb &lt;- pixel_metrics(las, ~colorize(Intensity, ScannerChannel), 2)\nterra::plotRGB(rgb)\n\n\n\n\n\n\n\n\nWe can also attribute an RGB color per voxel, discarding the voxels where RGB = NA by using colorize() with voxel_metrics() (see Chapter 12).\n\nrgb &lt;- voxel_metrics(las, ~colorize(Intensity, ScannerChannel), 2)\nrgb &lt;- rgb[!is.na(R)] # Remove NAs\nrgb &lt;- LAS(rgb, las@header) # Convert to LAS for display\nplot(rgb, color = \"RGB\", nbits = 8, size = 5)\n#&gt; The argument 'nbits' is not longer supported. It is now infered automatically.\n\n\n\n\n\n\n\n\nTo finish we can colorize() with point_metrics() (see Chapter 13). This has the advantage of maintaining much more points compared to the voxel-based version and to preserve coordinates.\n\nrgb &lt;- point_metrics(las, ~colorize(Intensity, ScannerChannel), r = 0.5)\nlas &lt;- add_lasrgb(las, rgb$R, rgb$G, rgb$B)\ncolored &lt;- filter_poi(las, !is.na(R)) # Remove NAs\nplot(colored, color = \"RGB\", size = 4)",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Thinking outside the box</span>"
    ]
  },
  {
    "objectID": "spatialindex.html",
    "href": "spatialindex.html",
    "title": "18  Spatial indexing",
    "section": "",
    "text": "18.1 Introduction to spatial indexes\nThis section presents a layman overview of how spatial indexing works. If the reader is already knowledgeable about spatial indexing, they can skip this section.\nImagine we have a lidar point cloud with 1 million points (sounds like alot… but it isn’t!). We want to query all points that fall within the extent of a circle centered on the coordinates p = c(300, 350) with a radius (R) of 25 meters. This is a typical query made thousands of times per second by many algorithms including the local maximum filter (Section 7.1.1) to locate individual trees. Without spatial indexing, the method consists of computing the distance to p for every single point. This is called a ‘sequential scan’, and it means that the computation for distance and comparisons must be conducted 1,000,000 times each.\nIn R we can write:\np = c(300, 350)\nR = 25\nX = runif(1e4, 0, 1000)\nY = runif(1e4, 0, 1000)\nquery = sqrt((X - p[1])^2 + (Y - p[2])^2) &lt; R\nXq = X[query]\nYq = Y[query]\nThis is what the filter_poi() function does. It a non-specialized function that enables querying points of interest (POI) based on their attribute values (including non-spatial queries such as Intensity &gt; x).\nNow imagine we want to perform 1,000,000 queries like that for 1,000,000 different points. That translates to 1,000,000 x 1,000,000 = 1 billion operations. This does not scale-up and quickly becomes unrealistic (or at least dramatically slow).\nWith a spatial index, the points are organized in such a way that the computer does not need to perform all the comparisons. In a quadtree, for example, the point cloud can be subdivided in 4 quadrants that are themselves subdivided in four quadrants and so on hierarchically (see figure below).\nIn this example we can immediately exclude 75% of the points (750,000 points) in 4 operations at the top level (in red). The bounding box of our query being [275,325]x[325,375] we know that the POIs do not belong in top-left quadrant ([0,500] x [500,1000]) nor in top-right quadrant ([500,1000] x [500,1000]) nor in bottom-right quadrant. At the second level (in blue) in 4 more operations we can exclude another 75% of the remaining points to search only in one quadrant. At this stage we can perform a sequential scan on only 1/16th of the points (i.e. 62,500 points) meaning that we discarded 937,500 points in 8 operations! Consequently our query is (roughly) 16 times faster and could be even faster yet with more subdivision levels. In lidR a typical quadtree has 8 levels i.e. the space is subdivided in (28)2 = 65,536 quadrants.\nAs we can see, spatial indexing provides a way to dramatically speed-up many common spatial queries using discs, rectangles, polygons, 2D, 3D and so on. Different types of spatial indexes exist for different purposes but in all cases the use of a spatial index is not free and comes at the cost of greater memory usage.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Spatial indexing</span>"
    ]
  },
  {
    "objectID": "spatialindex.html#sec-spatial-indexing-intro",
    "href": "spatialindex.html#sec-spatial-indexing-intro",
    "title": "18  Spatial indexing",
    "section": "",
    "text": "filter_poi(las, sqrt((X - p[1])^2 + (Y - p[2])^2) &lt; R)",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Spatial indexing</span>"
    ]
  },
  {
    "objectID": "spatialindex.html#sec-spatial-indexing-las",
    "href": "spatialindex.html#sec-spatial-indexing-las",
    "title": "18  Spatial indexing",
    "section": "18.2 Spatial indexes for LAS objects",
    "text": "18.2 Spatial indexes for LAS objects\n\nOverview\nlidR makes use of spatial indexes in many functions and can choose different types of spatial indexes on-the-fly. So far, the book only presented the function readLAS() (see Chapter 2) but the package has some variations of readLAS() named readALSLAS(), readTLSLAS(), readUAVLAS() and so on that enable the registration of a point cloud type allowing lidR to adequately choose the most appropriated spatial indexing method to perform a given computation as fast as possible.\nAs an example we can use the TLS point cloud pine_plot.laz from the TreeLS package.\nFirst we read the file with readLAS(), which considers the point cloud to be ALS because lidR was originally designed for ALS and by legacy readLAS() from version &lt;= 3.1 behaves optimally for ALS. In the second case we use readTLSLAS() to inform lidR that this point cloud was sampled with a terrestrial device.\nIn the following test we can see that the computation time was reduced from ~2.5 sec to ~1.3 sec by registering the proper point type. Improvements may range from 2 to 10 times faster depending on the point cloud and the method used.\n\nlas &lt;- readLAS(\"data/pine_plot.laz\", select='xyz')\ntls &lt;- readTLSLAS(\"data/pine_plot.laz\", select='xyz')\n\nsystem.time(segment_shapes(las, shp_plane(k = 15), \"Coplanar\"))\n#&gt; utilisateur     système      écoulé \n#&gt;       1.419       0.000       0.289\nsystem.time(segment_shapes(tls, shp_plane(k = 15), \"Coplanar\"))\n#&gt; utilisateur     système      écoulé \n#&gt;       0.838       0.000       0.159\n\nThis works for each method that implies many sequential spatial queries.\nIn the following example we can observe a ~8 fold processing time reduction.\n\nsystem.time(point_metrics(las, r = 1, ~length(Z)))\n#&gt; utilisateur     système      écoulé \n#&gt;       3.616       0.000       3.616\nsystem.time(point_metrics(tls, r = 1, ~length(Z)))\n#&gt; utilisateur     système      écoulé \n#&gt;       1.012       0.000       1.013\n\nNow lets try with an ALS point cloud. We can see that it’s better to read an ALS point cloud as ALS rather than as TLS (~2 fold difference). This is because registering the correct point cloud type enables the selection of the optimal spatial indexing algorithm internally.\n\nals = readALSLAS(\"data/ENGINE/catalog/tiles_338000_5238500_1.laz\")\ntls = readTLSLAS(\"data/ENGINE/catalog/tiles_338000_5238500_1.laz\")\n\nsystem.time(classify_noise(als, sor()))\n#&gt; utilisateur     système      écoulé \n#&gt;       2.934       0.008       0.577\nsystem.time(classify_noise(tls, sor()))\n#&gt; utilisateur     système      écoulé \n#&gt;       8.110       0.008       1.581\n\n\nTake away: It is always a good idea to use the functions readALSLAS(), readTLSLAS(), readDAPLAS(), and so on introduced in lidR v3.1.0.\n\nThere are however caveats resulting from using the optimal read*LAS() function that may not guarantee optimal processing performance.\n\nAll functions do not use spatial indexing or do not use the spatial index framework of lidR. For example, the kriging() function is based on the gstat package.\nThe choice of spatial index relies on some assumptions that may not be met in specific point clouds. The internal dispatch is designed to work with ‘typical’ point clouds under some assumptions. An ALS point-cloud is typically spatially large (1 km² or more) with little Z dispersion (0 to 40 meters) relative to the XY dispersion (0 to 1000 meters). On the contrary, a TLS point cloud is typically spatially narrow (~3000 m²) with larger variations in Z relative to XY.\n\nread*LAS() should be sufficient for most use cases but for some specific cases users can manually choose which spatial index is best suited. We cover this in the next section.\n\n\nSpatial indexes and selection strategies\nlidR currently has 4 spatial indexes: a grid partition, a voxel partition, a quadtree and an octree. Each has its own pros and cons.\nGrid partition and quadtree are 2D indexes while voxel partition and octree are 3D indexes. They are all able to perform any kind of spatial query similarly. This is why it doesn’t matter if the point cloud is read with readALSLAS() or readTLSLAS(). The result will be the same. However their efficiency depends on the point cloud type and the query type. This is why using the proper read*LAS() function can matter.\n\nALS strategies\nFor ALS we use a 2D index even for 3D queries.\nIndeed, an ALS point cloud is ‘mostly 2D’ because more than 99% of the dispersion is in XY. When querying the knn of a given point (3D query) from a 2D index the vast majority of the points are discarded on a 2D basis. The remaining sequential scan occurs only on a very tiny fraction of the data set. This is also true for a 3D index but querying a 3D spatial index is slower and thus in lidR our 2D indexes perform best for ALS. A grid partition is used by default because it is often faster than a quadtree because ALS points are uniformly distributed on XY.\nThe following example demonstrates how to manually register a spatial index and compare the computation times for a quadtree and an octree.\n\nlas = readLAS(\"data/ENGINE/catalog/tiles_338000_5238500_1.laz\", select = \"xyz\")\n\nindex(las) &lt;- \"quadtree\"\nsystem.time(classify_noise(las, sor()))\n#&gt; utilisateur     système      écoulé \n#&gt;       2.416       0.000       0.519\n\nindex(las) &lt;- \"octree\"\nsystem.time(classify_noise(las, sor()))\n#&gt; Error in eval(expr, envir, enclos): Error: octree no longer supported.\n#&gt; Chronométrage arrêté à : 0 0 0.001\n\n\n\nTLS strategies\nFor TLS we use a 3D index because the points are almost evenly distributed in XYZ and thus a 2D query does not allow for discarding a large fraction of the points - the sequential scan remains important. An octree is used because points are expected to be not uniformly distributed on XYZ.\n\nfile &lt;- system.file(\"extdata\", \"pine_plot.laz\", package=\"TreeLS\")\nlas &lt;- readLAS(file, select='xyz')\n#&gt; Error: File does not exist.\n\nindex(las) &lt;- \"quadtree\"\nsystem.time(classify_noise(las, sor()))\n#&gt; utilisateur     système      écoulé \n#&gt;       2.387       0.000       0.506\n\nindex(las) &lt;- \"octree\"\nsystem.time(classify_noise(las, sor()))\n#&gt; Error in eval(expr, envir, enclos): Error: octree no longer supported.\n#&gt; Chronométrage arrêté à : 0.001 0 0.001\n\n\n\nDAP and UAV strategies\nFor digital photogrammetry and UAV data we apply the same rules as TLS. When encountering a data set that does not follow these rules, it may be optimal to manually select a spatial index. This is the case of the data set seen in chapter Section 13.2.1, which is an ALS data set but in practice it’s a small subset in which we can no longer say that more than 99% of the point dispersion is in XY only. In that sense it’s more of a TLS ish point cloud. But in the meantime the points are uniformly spread on XY because it’s actually an ALS data set. Thus making 3D queries using a 3D index most viable. Let’s try it:\n\nlas &lt;- readLAS(\"data/chap11/building_WilliamsAZ_Urban_normalized.laz\")\n\nindex(las) &lt;- \"gridpartition\"\nsystem.time(segment_shapes(las, shp_plane(k = 20), \"planar\", filter = ~Classification != LASGROUND))\n#&gt; utilisateur     système      écoulé \n#&gt;       1.557       0.004       0.326\n\nindex(las) &lt;- \"voxelpartition\"\nsystem.time(segment_shapes(las, shp_plane(k = 20), \"planar\", filter = ~Classification != LASGROUND))\n#&gt; utilisateur     système      écoulé \n#&gt;       1.668       0.000       0.330\n\nWe see that both tests are almost equal, and that octree is slower. But one may find limit cases where its worth it to perform manual selection and thus lidR allows for overwriting the default rules. More details in help(\"lidR-spatial-index\").\n\n\n\nC++ API\nFor more advanced users and developers, the lidR spatial index framework is provided as header-only C++ classes meaning that users can link to lidR to develop R/C++ applications using lidR spatial indexes. If the reader is not comfortable with the terms C++, Rcpp, header-only, external pointer and other C++ related concepts, we understand! You can skip this section, which is dedicated to advanced users and package developers who want to develop complex and efficient tools.\nFor the purpose of this example we will create a function clip_disc() similar to clip_circle() available in lidR. clip_circle() performs a sequential scan and is thus not suitable to perform many queries in a loop. The function clip_disc() on the contrary will take advantage of spatial indexing.\nThere is only one C++ class to know named SpatialIndex. It has one constructor that accepts an S4 class and has two public members knn and lookup.\nFirst we can write a C++ function that returns a pointer on a SpatialIndex. Here we are using an external pointer because it’s simple to write, and implies fewer lines of code. We can however also imagine taking advantage of Rcpp modules.\n// [[Rcpp::depends(lidR)]]\n#include &lt;SpatialIndex.h&gt;\nusing namespace Rcpp;\nusing namespace lidR;\n\n// [[Rcpp::export]]\nXPtr&lt;SpatialIndex&gt; spatial_index(S4 las) {\n  SpatialIndex* idx = new SpatialIndex(las);\n  XPtr&lt;SpatialIndex&gt; p(idx, true);\n  return p;\nNow we can instantiate a SpatialIndex at the R level.\n\nlas = readLAS(\"data/ENGINE/catalog/tiles_338000_5238500_1.laz\")\nindex = spatial_index(las)\nindex\n#&gt; &lt;pointer: 0x60374eeecb30&gt;\n\nWhat has been created here is either a grid partition, a voxel partition, a quadtree or an octree depending on which readLAS() function was used to read the files or depending on the spatial index that was manually registered.\nThen we can write the C++ side of the query.\n// [[Rcpp::export]]\nIntegerVector filter_disc_with_index(SEXP xptr, double xc, double yc, double r) {\n  XPtr&lt;SpatialIndex&gt; tree(xptr);\n  Circle circ(xc, yc, r);\n  std::vector&lt;PointXYZ&gt; pts;\n  tree-&gt;lookup(circ, pts);\n\n  IntegerVector ids(pts.size());\n  for(int i = 0 ; i &lt; pts.size(); i++) { ids[i] = pts[i].id; }\n  return ids + 1; // C++ is 0-indexed\n}\nAnd the R side of the query\n\nclip_disc = function(las, index, xcenter, ycenter, radius) {\n  ii &lt;- filter_disc_with_index(index, xcenter, ycenter, radius)\n  return(las[ii])\n}\n\nNow we can make a query and verify that both functions return the same points.\n\nsub1 = clip_disc(las, index, 338200, 5238585, 10)\nsub2 = clip_circle(las, 338200, 5238585, 10)\nsub1\n#&gt; class        : LAS (v1.0 format 1)\n#&gt; memory       : 78.6 Kb \n#&gt; extent       : 338190, 338209.9, 5238575, 5238595 (xmin, xmax, ymin, ymax)\n#&gt; coord. ref.  : WGS 84 / UTM zone 19N \n#&gt; area         : 305.5 m²\n#&gt; points       : 1.1 thousand points\n#&gt; density      : 3.75 points/m²\n#&gt; density      : 2.92 pulses/m²\nsub2\n#&gt; class        : LAS (v1.0 format 1)\n#&gt; memory       : 78.6 Kb \n#&gt; extent       : 338190, 338209.9, 5238575, 5238595 (xmin, xmax, ymin, ymax)\n#&gt; coord. ref.  : WGS 84 / UTM zone 19N \n#&gt; area         : 305.5 m²\n#&gt; points       : 1.1 thousand points\n#&gt; density      : 3.75 points/m²\n#&gt; density      : 2.92 pulses/m²\n\nWhile there is no gain with a single query because of the overhead of creating an index, it is indispensable to perform many successive queries. In the following we perform 50 queries in a loop.\n\nn = 50\nx = runif(n, 338000, 338500)\ny = runif(n, 5238500, 5239000)\n\nsystem.time(for (i in 1:n) u = clip_circle(las, x[i], y[i], 10))\n#&gt; utilisateur     système      écoulé \n#&gt;       1.550       0.063       1.216\nsystem.time(for (i in 1:n) u = clip_disc(las, index, x[i], y[i], 10))\n#&gt; utilisateur     système      écoulé \n#&gt;       0.191       0.000       0.099\n\nFor more functionalities one can look at the source code of SpatialIndex where we can see there are actually 2 constructors and 5 members including 2D and 3D knn, 2D and 3D knn with maximum radius and lookup that is templated to allow queries within any kind of user-defined shapes. The source code of many lidR functions such as lmf() or detect_shape() might be useful resources as well.\nSpatialIndex(const Rcpp::S4 las);\nSpatialIndex(const Rcpp::S4 las, const std::vector&lt;bool&gt;& filter);\ntemplate&lt;typename T&gt; void lookup(T& shape, std::vector&lt;PointXYZ&gt;& res);\nvoid knn(const PointXY& p, const unsigned int k, std::vector&lt;PointXYZ&gt;& res);\nvoid knn(const PointXYZ& p, const unsigned int k, std::vector&lt;PointXYZ&gt;& res);\nvoid knn(const PointXY& p, const unsigned int k, const double r, std::vector&lt;PointXYZ&gt;& res);\nvoid knn(const PointXYZ& p, const unsigned int k, const double r, std::vector&lt;PointXYZ&gt;& res);\n\n\nBenchmark\nlidR’s spatial index framework is very fast, especially when large point clouds are used. In the following we compare how fast lidR searches for the 10-nearest neighbours of every point in a 2.3 million point ALS point cloud relative to the RANN, FANN and nabor packages.\n\n\n\n\n\n\n\n\n\nWe see that it is competitive with the very fast libnabo library but does more than libnabo since it also performs range queries such as point in discs, rectangles, cylinders, triangles, polygons.\nWe don’t know any R library providing such capability to produce benchmark comparisons. Moreover, lidR leverages the C++ classes to allow the creation of efficient third party applications. This functionality is heavily used in the lidRplugins package.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Spatial indexing</span>"
    ]
  },
  {
    "objectID": "spatialindex.html#sec-spatial-indexing-files",
    "href": "spatialindex.html#sec-spatial-indexing-files",
    "title": "18  Spatial indexing",
    "section": "18.3 Spatial index for LAS files",
    "text": "18.3 Spatial index for LAS files\nPrevious sections were dedicated to explaining spatial indexing for LAS objects i.e. point clouds read with readLAS() and loaded in memory. This section focuses on spatial indexing for LAS files i.e. point clouds stored in las/laz files and not (yet) loaded in memory. The problem of spatial queries at read time is the same but the solution is different because it was developed in an independent context.\nFast spatial queries are made possible by indexing the .las or .laz files with .lax files. A .lax file is a tiny file associated with a .las or .laz file that spatially indexes the points. This file type was created by Martin Isenburg in LAStools. For a better understanding of how it works one can refer to a talk given by Martin Isenburg about lasindex. In short it uses quadtree.\nBy adding .lax files along with your .las/.laz files it is possible to make fast 2D queries without reading the whole file. The best way to create a .lax file is to use laxindex from LAStools. It is a free and open-source part of LAStools. If you cannot or do not want to use LAStools the rlas package has a function to creates lax files but lasindex should be preferred.\nrlas::writelax(\"file.las\")\nThe gain is really significant and transparent for users. If you have a .lax file it will be used.\nHere we test with 150 queries from the same indexed and a non-indexed LAScatalog with 400 files:\nindexed = readLAScatalog(\"LiDAR with lax/\")\nnoindex = readLAScatalog(\"LiDAR no lax/\")\n\nclip_circle(indexed, xc, yc, radius = 12)\n#&gt; 45 sec\nclip_circle(noindex, xc, yc, radius = 12)\n#&gt; 4 sec\nIf the reader did not skip Section 18.2.3 they might have noticed that clip_circle() can use a spatial index with a LAScatalog but not with a LAS. This is because they behave very differently internally and rely on two independent mechanisms. With a LAScatalog it inherits the capabilities of the library used to read the files while with a LAS object nothing has been implemented (yet) for taking advantage of spatial indexing at the R level (but the section above provide the solution).\nIt’s easy to guess that every clip_something() function can take advantage of spatial indexing with .lax files but the LAScatalog processing engine also makes heavy usage of such features. Users can significantly reduce the processing time by loading a buffer faster. Indeed loading a buffer implies spatial queries. This topic is covered by the vignette: Speed-up the computations on a LAScatalog.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Spatial indexing</span>"
    ]
  },
  {
    "objectID": "plugins.html",
    "href": "plugins.html",
    "title": "19  lidR plugin system",
    "section": "",
    "text": "19.1 Understanding lidR algorithms\nIn lidR, an algorithm such as tin(), p2r() or lmf() is a function factory. The output is functions with extra classes so regular users wont immediately recognize that they are functions.\nalgo &lt;- knnidw(k = 10)\nalgo\n#&gt; Object of class lidR algorithm\n#&gt; Algorithm for: spatial interpolation \n#&gt; Designed to be used with: normalize_height or rasterize_terrain or p2r or spatial_interpolation \n#&gt; Native C++ parallelization: yes \n#&gt; Parameters: \n#&gt;  - k = 10 &lt;numeric&gt;\n#&gt;  - p = 2 &lt;numeric&gt;\n#&gt;  - rmax = 50 &lt;numeric&gt;\nclass(algo) # algo is a function\n#&gt; [1] \"lidRAlgorithm\"        \"SpatialInterpolation\" \"function\"            \n#&gt; [4] \"omp\"\nRemoving the extra classes we can see its a function and we can see the source code.\nclass(algo) &lt;- \"function\"\nalgo\n#&gt; function(las, where)\n#&gt;   {\n#&gt;     assert_is_valid_context(LIDRCONTEXTSPI, \"knnidw\")\n#&gt;     return(interpolate_knnidw(las, where, k, p, rmax))\n#&gt;   }\n#&gt; &lt;bytecode: 0x589c424700a8&gt;\n#&gt; &lt;environment: 0x589c42478f60&gt;\n#&gt; attr(,\"class\")\n#&gt; [1] \"function\"\nWe can see how a function designed to be used in rasterize_terrain() is designed. The signature is\nWhen creating a new algorithm for spatial interpolation, the function factory must return a function similar to what you see above. In the case of spatial interpolation las is a LAS with X Y and Z coordinates of ground points (cleaned of duplicates). where is a data.frame with the X Y coordinates of the location where we want to interpolate Z.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>`lidR` plugin system</span>"
    ]
  },
  {
    "objectID": "plugins.html#sec-plugin-system",
    "href": "plugins.html#sec-plugin-system",
    "title": "19  lidR plugin system",
    "section": "",
    "text": "function(las, where)",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>`lidR` plugin system</span>"
    ]
  },
  {
    "objectID": "plugins.html#sec-plugin-creation",
    "href": "plugins.html#sec-plugin-creation",
    "title": "19  lidR plugin system",
    "section": "19.2 Creation of the mba algorithm",
    "text": "19.2 Creation of the mba algorithm\nNow let’s create our mba algorithm.\n\n# mba is our function factory\nmba &lt;- function(n = 1, m = 1, h = 8, extend = TRUE) {\n  # f is created inside mba and receive the ground points in a LAS (gnd)\n  # and the location where to compute the interpolation (where) \n  f &lt;- function(gnd, where) {\n    # computation of the interpolation (see the documentation of MBA package)\n    res &lt;- MBA::mba.points(gnd@data, where, n, m , h, extend)\n    return(res$xyz.est[,3])\n  }\n  \n  # f is a function but we can set compatible classes. Here it is an\n  # algorithm for DTM \n  f &lt;- plugin_dtm(f)\n  return(f)\n}\n\nNow let see what happens if we instantiate the mba algorithm:\n\nalgo &lt;- mba(h = 6)\nalgo\n#&gt; Object of class lidR algorithm\n#&gt; Algorithm for: spatial interpolation \n#&gt; Designed to be used with: normalize_height or rasterize_terrain or p2r or spatial_interpolation \n#&gt; Native C++ parallelization: no \n#&gt; Parameters: \n#&gt;  - extend = TRUE &lt;logical&gt;\n#&gt;  - h = 6 &lt;numeric&gt;\n#&gt;  - m = 1 &lt;numeric&gt;\n#&gt;  - n = 1 &lt;numeric&gt;\n\nWe can now use it like any other lidR algorithm:\n\nLASfile &lt;- system.file(\"extdata\", \"Topography.laz\", package=\"lidR\")\nlas &lt;- readLAS(LASfile)\ndtm &lt;- rasterize_terrain(las, algorithm = mba())\nplot(dtm, col = gray(1:50/50))\n\n\n\n\n\n\n\n\n\nplot_dtm3d(dtm, bg = \"white\")\n\n\n\n\n\n\n\n\nIt will even fail nicely if used inappropriately!\n\nrasterize_canopy(las, 1, mba())\n#&gt; Error: The algorithm used is not an algorithm for digital surface model.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>`lidR` plugin system</span>"
    ]
  },
  {
    "objectID": "plugins.html#sec-plugin-extensions",
    "href": "plugins.html#sec-plugin-extensions",
    "title": "19  lidR plugin system",
    "section": "19.3 What about other algorithms?",
    "text": "19.3 What about other algorithms?\nlidR has algorithms for canopy height models, individual tree segmentation, individual tree detection, sensor tracking, snag segmentation and so on. They all have different behaviours and this is why it’s difficult to document. If you want to create a new algorithm the best first step is to communicate directly with lidR developers :). The lidRplugins package makes heavy use of the plugins system to provide extra methods for diverse tasks.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>`lidR` plugin system</span>"
    ]
  }
]