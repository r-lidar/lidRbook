[["index.html", "The lidR package 1 Introduction", " The lidR package Jean-Romain Roussel, Tristan R.H. Goodbody, Piotr Tompalski 2023-08-03 1 Introduction lidR is an R package for manipulating and visualizating airborne laser scanning (ALS) data with an emphasis on forestry applications. The package is entirely open source and is integrated within the geospatial R ecosytem (i.e. raster/terra/stars and sp/sf). This guide has been written to help both the ALS novice, as well as seasoned point cloud processing veterans. Key functionality of lidR includes functions to: Read and write .las and .laz files and render customized point-cloud display (section 2) Process point clouds including point classification (section 3), digital terrain models (section 4), normalization (section 5) and digital surface models (section 6) Perform individual tree segmentation (section 7) Compute standard metrics at different levels of regularization (sections 8, 9, 10, 11, 12 and 13) Manage processing for sets of point-cloud files - referred to as a LAScatalog (sections 14 and 15) Guidelines for implementing area-based approaches to forest modelling using ALS data (section 16) Facilitate user-defined processing streams for research and development (section 17) Understand spatial indexing (section 18) Discover the plugin system (section 19) The current release version of lidR can be found on CRAN and the source code is hosted on GitHub. Development of the lidR package between 2015 and 2018 was made possible thanks to the financial support of the AWARE project NSERC CRDPJ 462973-14; grantee Prof. Nicholas C. Coops. Development of the lidR package between 2018 and 2021 was made possible thanks to the financial support of the Ministère des Forêts, de la Faune et des Parcs of Québec. The book is shared under CC-BY-NC-SA 2.0 This book was created to provide hands on descriptions and tutorials for using lidR and is not the formal package documentation. The comprehensive package documentation is shipped with the package. ## &lt;STYLE type=&#39;text/css&#39; scoped&gt; ## PRE.fansi SPAN {padding-top: .25em; padding-bottom: .25em}; ## &lt;/STYLE&gt; "],["io.html", "2 Reading, Plotting, Querying &amp; Validating 2.1 Reading LiDAR data using readLAS 2.2 Validating lidar data 2.3 Plotting", " 2 Reading, Plotting, Querying &amp; Validating 2.1 Reading LiDAR data using readLAS Discrete return ALS sensors record a number of pieces of data. First and foremost, positional data in three dimensions (X,Y,Z), followed by additional information like the intensity for each point, the position of each point in the return sequence, or the beam incidence angle of each point. Reading, writing, and efficient storage of these ALS data is a critical step prior to any subsequent analysis. ALS data is most commonly distributed in LAS format, which is specifically designed to store ALS data in a standardized way. These data are officially documented and maintained by the American Society for Photogrammetry &amp; Remote Sensing (ASPRS). LAS files do however require a large amount of memory because they are not compressed. The LAZ format has become the standard compression scheme, because it is free and open-source. The widespread use, standardization and open source nature of the LAS and LAZ formats promoted the development of the lidR package, which has been designed to process LAS and LAZ files both as input and output, taking advantage of the LASlib and LASzip C++ libraries via the rlas package. The function readLAS() reads a LAS or LAZ file and returns an object of class LAS. The LAS formal class is documented in depth in a dedicated vignette. To briefly summarize, a LAS file is made of two parts: The header that stores summary information about its content including the bounding box of the file, coordinate reference system, and point format. The payload - i.e. the point cloud itself. The function readLAS() reads and creates an object that contains both the header and the payload. las &lt;- readLAS(&quot;files.las&quot;) When printed it displays a summary of its content. print(las) #&gt; class : LAS (v1.2 format 1) #&gt; memory : 4.4 Mb #&gt; extent : 684766.4, 684993.3, 5017773, 5018007 (xmin, xmax, ymin, ymax) #&gt; coord. ref. : NAD83 / UTM zone 17N #&gt; area : 51572 m² #&gt; points : 81.6 thousand points #&gt; density : 1.58 points/m² #&gt; density : 1.08 pulses/m² For a more in-depth print out of the data use the function summary() instead of print(). 2.1.1 Parameter select A LAS file stores the X Y Z coordinates of each point as well as many other data such as intensity, incidence angle, and return sequence position. We call these data attributes. In pratice many attributes are not actually useful but they are loaded anyway by default. This can take up a lot of processing memory because R is a language that does not allow for choosing data storage modes (see this vignette for more details). To save memory, readLAS() can take an optional parameter select which enables the user to selectively load the attributes of interest. For example, one can choose to load only the X Y Z attributes. las &lt;- readLAS(&quot;file.las&quot;, select = &quot;xyz&quot;) # load XYZ only las &lt;- readLAS(&quot;file.las&quot;, select = &quot;xyzi&quot;) # load XYZ and intensity only Examples of other attribute abbreviations are: t - gpstime, a - scan angle, n - number of returns, r - return number, c - classification, s - synthetic flag, k - keypoint flag, w - withheld flag, o - overlap flag (format 6+), u - user data, p - point source ID, e - edge of flight line flag, d - direction of scan flag 2.1.2 Parameter filter While select enables the user to select “columns” (or attributes) while reading files, filter allows selection of “rows” (or points) while reading. Removing superfluous data at reading time saves memory and increases computation speed. For example, it’s common practice in forestry to process using first returns. las &lt;- readLAS(&quot;file.las&quot;, filter = &quot;-keep_first&quot;) # Read only first returns It is important to understand that the option filter in readLAS() keeps or discards point at read time i.e. while reading at the C++ level without implying any R code. For example the R function to filter points of interest (POI) is filter_poi() may return the exact same output as the filter option in readLAS(): las1 &lt;- readLAS(&quot;file.las&quot;, filter = &quot;-keep_first&quot;) las2 &lt;- readLAS(&quot;file.las&quot;) las2 &lt;- filter_poi(las2, ReturnNumber == 1L) In the above example we are (1) reading only the first returns or (2) Reading all the points then filtering the first returns in R. Both outputs are strictly identical but the first one is faster and more memory efficient because it doesn’t load the whole file in R and does not use extra processing memory. It should always be preferred when possible. Multiple filter commands can be used at once to e.g. read only first return between 5 and 50 m. las &lt;- readLAS(&quot;file.las&quot;, filter = &quot;-keep_first -drop_z_below 5 -drop_z_above 50&quot;) The full list of available commands is given by readLAS(filter = \"-help\"). Users of LAStools may recognize these commands because both LAStools and lidR use the same library (LASlib and LASzip) to read and write LAS and LAZ files. 2.2 Validating lidar data An important first step in ALS data processing is ensuring that your data is complete and valid according to the ASPRS LAS specifications. Users commonly report bugs arising from invalid data. This is why we introduced the las_check() function to perform a deep inspection of LAS objects. This function checks if a LAS object meets the ASPRS LAS specifications and whether it is valid for processing, giving warnings if otherwise. A simple example that happens fairly often is that a LAS file contains duplicate points. This may lead to problems like trees being detected twice, to invalid metrics, or to errors in DTM generation. We can also encounter invalid return numbers, incoherent return numbers and number of returns attributes, invalid coordinate reference system etc. Always make sure to run the las_check() function before digging deep into your data. las_check(las) #&gt; #&gt; Checking the data #&gt; - Checking coordinates... ✓ #&gt; - Checking coordinates type... ✓ #&gt; - Checking coordinates range... ✓ #&gt; - Checking coordinates quantization... ✓ #&gt; - Checking attributes type... ✓ #&gt; - Checking ReturnNumber validity... #&gt; ⚠ Invalid data: 1 points with a return number equal to 0 found. #&gt; - Checking NumberOfReturns validity... ✓ #&gt; - Checking ReturnNumber vs. NumberOfReturns... ✓ #&gt; - Checking RGB validity... ✓ #&gt; - Checking absence of NAs... ✓ #&gt; - Checking duplicated points... #&gt; ⚠ 1 points are duplicated and share XYZ coordinates with other points #&gt; - Checking degenerated ground points... #&gt; ⚠ There were 1 degenerated ground points. Some X Y Z coordinates were repeated #&gt; - Checking attribute population... #&gt;  &#039;PointSourceID&#039; attribute is not populated #&gt;  &#039;EdgeOfFlightline&#039; attribute is not populated #&gt; - Checking gpstime incoherances ✓ #&gt; - Checking flag attributes... ✓ #&gt; - Checking user data attribute... ✓ #&gt; Checking the header #&gt; - Checking header completeness... ✓ #&gt; - Checking scale factor validity... ✓ #&gt; - Checking point data format ID validity... ✓ #&gt; - Checking extra bytes attributes validity... ✓ #&gt; - Checking the bounding box validity... ✓ #&gt; - Checking coordinate reference system... ✓ #&gt; Checking header vs data adequacy #&gt; - Checking attributes vs. point format... ✓ #&gt; - Checking header bbox vs. actual content... ✓ #&gt; - Checking header number of points vs. actual content... ✓ #&gt; - Checking header return number vs. actual content... #&gt; ⚠ Invalid file: the header states the file contains 55756 returns numbered &#039;1&#039; but 55755 were found. #&gt; Checking coordinate reference system... #&gt; - Checking if the CRS was understood by R... ✓ #&gt; Checking preprocessing already done #&gt; - Checking ground classification... yes #&gt; - Checking normalization... yes #&gt; - Checking negative outliers... ✓ #&gt; - Checking flightline classification... no #&gt; Checking compression #&gt; - Checking attribute compression... #&gt; - EdgeOfFlightline is compressed #&gt; - Synthetic_flag is compressed #&gt; - Keypoint_flag is compressed #&gt; - Withheld_flag is compressed #&gt; - UserData is compressed #&gt; - PointSourceID is compressed A check is performed at read time regardless, but the read time check is not as thorough as las_check() for computation time reasons. For example duplicated points are not checked at read time. las &lt;- readLAS(&quot;data/chap1/corrupted.laz&quot;) #&gt; Warning: Invalid data: 174638 points with a &#39;return number&#39; greater than the &#39;number of returns&#39;. 2.3 Plotting The lidR package takes advantage of the rgl package to provide a versatile and interactive 3D viewer with points coloured by Z coordinates on a black background as default. 2.3.1 Basic 3D rendering The very basic way to render a point cloud is the function plot(). plot(las) Users can change the attributes used for coloring by providing the name of the attribute used to colorize the points. The background color of the viewer can also be changed by assigning a color using the bg argument. Axes can also be added and point sizes can be changed. # Plot las object by scan angle, # make the background white, # display XYZ axis and scale colors plot(las, color = &quot;ScanAngleRank&quot;, bg = &quot;white&quot;, axis = TRUE, legend = TRUE) Note that if your file contains RGB data the string \"RGB\" is supported: plot(las, color =&quot;RGB&quot;) The argument breaks enables to defined more adequate breaks in the color palette for example when intensity contains large outliers. Otherwise the palette range would be too large and most of the values would be considered as “very low”, so everything would appear in the same color. plot(las, color = &quot;Intensity&quot;, breaks = &quot;quantile&quot;, bg = &quot;white&quot;) 2.3.2 Overlays The package also provides some easy to use functions for common overlay. For example add_dtm3d() to add a digital terrain model (section 4) and add_treetops3d() to visualize the output of an individual tree detection (section 7.1) x &lt;- plot(las, bg = &quot;white&quot;, size = 3) add_dtm3d(x, dtm) x &lt;- plot(las, bg = &quot;white&quot;, size = 3) add_treetops3d(x, ttops) It is also possible to combine two point clouds with different colour palettes. In the following example we are using a previously classified point cloud. We first separate the vegetation and non vegetation points using filter_poi() and then plot both on top of each other with different colour schemes using add options in plot() nonveg &lt;- filter_poi(las, Classification != LASHIGHVEGETATION) veg &lt;- filter_poi(las, Classification == LASHIGHVEGETATION) x &lt;- plot(nonveg, color = &quot;Classification&quot;, bg = &quot;white&quot;, size = 3) plot(veg, add = x) 2.3.3 Advanced 3D rendering With lidR being based on rgl it is easy to add objects in the main rendering using rgl functions such as rgl::point3d(), rgl::text(), rgl::surface3d() and so on to produce publication ready rendering. However lidR introduced an additional challenge because it does not display the points with their actual coordinates. The points are shifted to be rendered close to (0, 0) (a matter of accuracy because rgl uses float (decimal numbers on 32 bits) instead of double (decimal numbers on 64 bits)). When plot() is used it invisibly returns the shift values that can be used later to realign other objects. offsets &lt;- plot(las) print(offsets) #&gt; [1] 391867.8 3901019.3 The coordinates of the objects must be corrected to align with the point cloud. In the following we will add lines to render the trunks. We read a file, we locate the trees (see section 7.1), we extract the coordinates and sizes of the trees and plot lines with rgl::segment3d(). LASfile &lt;- system.file(&quot;extdata&quot;, &quot;MixedConifer.laz&quot;, package=&quot;lidR&quot;) las &lt;- readLAS(LASfile, select = &quot;xyzc&quot;) # get the location of the trees ttops &lt;- locate_trees(las, lmf(ws = 5)) # plot the point cloud offsets &lt;- plot(las, bg = &quot;white&quot;, size = 3) add_treetops3d(offsets, ttops) # extract the coordinates of the trees and # apply the shift to display the lines # in the rendering coordinate system x &lt;- sf::st_coordinates(ttops)[,1] - offsets[1] y &lt;- sf::st_coordinates(ttops)[,2] - offsets[2] z &lt;- ttops$Z # Build a GL_LINES matrix for fast rendering x &lt;- rep(x, each = 2) y &lt;- rep(y, each = 2) tmp &lt;- numeric(2*length(z)) tmp[2*1:length(z)] &lt;- z z &lt;- tmp M &lt;- cbind(x,y,z) # Display lines rgl::segments3d(M, col = &quot;black&quot;, lwd = 2) C:35881a9349d7.png 2.3.4 Voxel rendering It is possible to render voxels. This is useful to render the output of the function voxelise_points() or voxel_metrics() for examples. vox &lt;- voxelize_points(las, 6) plot(vox, voxel = TRUE, bg = &quot;white&quot;) 2.3.5 Cross sections 2D rendering To better visualize the vertical structure of a point cloud, investigate classification results, or compare results of different interpolation routines, a cross section can be plotted. To do that we first need to decide where the cross section is located (i.e. define the beginning and the end) and specify it’s width. The point cloud can then be clipped and the X and Z coordinates used to create the plot. For example, to create a 100 m long cross section we may define the beginning and the end and then use clip_transect() function to subset the point cloud. p1 &lt;- c(273357, 5274357) p2 &lt;- c(273542, 5274542) las_tr &lt;- clip_transect(las, p1, p2, width = 4, xz = TRUE) Rendering can be achieved with base plot or ggplot2. Notice the use of @data to extract the data.frame from the LAS object. library(ggplot2) ggplot(las_tr@data, aes(X,Z, color = Z)) + geom_point(size = 0.5) + coord_equal() + theme_minimal() + scale_color_gradientn(colours = height.colors(50)) The two steps required to create a cross section (clipping the point cloud and plotting) can be combined. Below we create a simple function that will become handy at multiple occasions throughout this book. To make this function even easier to use we will specify the default values for p1 and p2 so that the cross section is located in the centre of the point cloud, along the X-axis. The default width will be 4 m. plot_crossection &lt;- function(las, p1 = c(min(las@data$X), mean(las@data$Y)), p2 = c(max(las@data$X), mean(las@data$Y)), width = 4, colour_by = NULL) { colour_by &lt;- rlang::enquo(colour_by) data_clip &lt;- clip_transect(las, p1, p2, width) p &lt;- ggplot(data_clip@data, aes(X,Z)) + geom_point(size = 0.5) + coord_equal() + theme_minimal() if (!is.null(colour_by)) p &lt;- p + aes(color = !!colour_by) + labs(color = &quot;&quot;) return(p) } Then we can used the function: plot_crossection(las, colour_by = factor(Classification)) "],["gnd.html", "3 Ground classification 3.1 Progressive Morphological Filter 3.2 Cloth Simulation Function 3.3 Multiscale Curvature Classification (MCC) 3.4 Edge artifacts 3.5 How to choose a method and its parameters? 3.6 Other methods", " 3 Ground classification Classification of ground points is an important step in processing point cloud data. Distinguishing between ground and non-ground points allows creation of a continuous model of terrain elevation (see section 4). Many algorithms have been reported in the literature and lidR currently provides three of them: Progressive Morphological Filter (PMF), Cloth Simulation Function (CSF) and Multiscale Curvature Classification (MCC) usable with the function classify_ground(). 3.1 Progressive Morphological Filter The implementation of PMF algorithm in lidR is based on the method described in Zhang et al. (2003) with some technical modifications. The original method is raster-based, while lidR performs point-based morphological operations because lidR is a point cloud oriented software. The main step of the methods are summarised in the figure below: The pmf() function requires defining the following input parameters: ws (window size or sequence of window sizes), and th (threshold size or sequence of threshold heights). More experienced users may experiment with these parameters to achieve best classification accuracy, however lidR contains util_makeZhangParam() function that includes the default parameter values described in Zhang et al. (2003). LASfile &lt;- system.file(&quot;extdata&quot;, &quot;Topography.laz&quot;, package=&quot;lidR&quot;) las &lt;- readLAS(LASfile, select = &quot;xyzrn&quot;) las &lt;- classify_ground(las, algorithm = pmf(ws = 5, th = 3)) We can now visualize the result: plot(las, color = &quot;Classification&quot;, size = 3, bg = &quot;white&quot;) To better illustrate the classification results we can generate and plot a cross section of the point cloud (see section 2.3.5). p1 &lt;- c(273420, 5274455) p2 &lt;- c(273570, 5274460) plot_crossection(las, p1 , p2, colour_by = factor(Classification)) We can see that although the classification worked, there are multiple points above terrain that are classified 2 (i.e. “ground” according to ASPRS specifications). This clearly indicates that additional filtering steps are needed and that both ws and th parameters should be adjusted. Below we use multiple values for the two parameters instead of a single value in the example above. ws &lt;- seq(3, 12, 3) th &lt;- seq(0.1, 1.5, length.out = length(ws)) las &lt;- classify_ground(las, algorithm = pmf(ws = ws, th = th)) After this adjustment the classification result changed, and points in the canopy are no longer classified as “ground”. plot_crossection(las, p1 = p1, p2 = p2, colour_by = factor(Classification)) 3.2 Cloth Simulation Function Cloth simulation filtering (CSF) uses the Zhang et al 2016 algorithm and consists of simulating a piece of cloth draped over a reversed point cloud. In this method the point cloud is turned upside down and then a cloth is dropped on the inverted surface. Ground points are determined by analyzing the interactions between the nodes of the cloth and the inverted surface. The cloth simulation itself is based on a grid that consists of particles with mass and interconnections that together determine the three-dimensional position and shape of the cloth. The csf() functions use the default values proposed by Zhang et al 2016 and can be used without providing any arguments. las &lt;- classify_ground(las, algorithm = csf()) Similar to the previous examples, classification results can be assessed using a cross section: plot_crossection(las, p1 = p1, p2 = p2, colour_by = factor(Classification)) While the default parameters of csf() are designed to be universal and provide accurate classification results, according to the original paper, it’s apparent that the algorithm did not work properly in our example because a significant portion of points located in the ground were not classified. In such cases the algorithm parameters need to be tuned to improve the result. For this particular data set a set of parameters that resulted in an improved classification result were formulated as follows: mycsf &lt;- csf(sloop_smooth = TRUE, class_threshold = 1, cloth_resolution = 1, time_step = 1) las &lt;- classify_ground(las, mycsf) plot_crossection(las, p1 = p1, p2 = p2, colour_by = factor(Classification)) We can also subset only the ground points to display the results in 3D gnd &lt;- filter_ground(las) plot(gnd, size = 3, bg = &quot;white&quot;) 3.3 Multiscale Curvature Classification (MCC) Multiscale Curvature Classification (MCC) uses the Evans and Hudak 2016 algorithm originally implemented in the mcc-lidar software. las &lt;- classify_ground(las, mcc(1.5,0.3)) plot_crossection(las, p1 = p1, p2 = p2, colour_by = factor(Classification)) 3.4 Edge artifacts No matter which algorithm is used in lidR or other software, ground classification will be weaker at the edges of point clouds as the algorithm must analyze the local neighbourhood (which is missing on edges). To find ground points, an algorithm need to analyze the local neighborhood or local context that is missing at edge areas. When processing point clouds it’s important to always consider a buffer around the region of interest to avoid edge artifacts. lidR has tools to manage buffered tiles and this advanced use of the package will be covered in section 14. 3.5 How to choose a method and its parameters? Identifying the optimal algorithm parameters is not a trivial task and often requires several trial runs. lidR proposes several algorithms, and may propose even more in future versions, however the main goal is to provide a mean to compare outputs. We don’t know which one is better, and we don’t know which parameters best suit a given terrain. It’s likely that parameters need to be dynamically adjusted to the local context, providing reasoning for why parameters works in one file may provide inadequate results in another. If available, we recommend using classifications given by the data provider. classify_ground()is useful for small to medium size unclassified region of interests because it is feasible to visually assess classification results. For large acquisitions where visual assessment is no longer feasible, we do not recommend performing ground classification without studying its accuracy ahead of time. 3.6 Other methods Ground segmentation is not limited to the 3 methods described above. Many more have been presented and described in the literature. In section 19 we will learn how to create a plugin algorithm and test it seamlessly in R with lidR using a syntax like: las &lt;- classify_ground(las, algorithm = my_new_method(param1 = 0.05)) "],["dtm.html", "4 Digital terrain model 4.1 Triangular irregular network 4.2 Invert distance weighting 4.3 Kriging 4.4 Pros and cons 4.5 Other methods 4.6 Render shaded DTM", " 4 Digital terrain model Generating a Digital Terrain Model (DTM) is usually the second step in processing that follows classification of ground points (section 3). Put simply, a DTM can be described as an “image” of the ground. Methods to generate DTMs have been intensively studied and several algorithms have been proposed for various terrain situations. DTMs are used for a variety of purposes in practice, such as determination of the catchment basins of water retention and stream flow, or the identification of drivable roads to access resources. It also enables users to normalize point clouds i.e. subtract the local terrain from the elevation of points to allow a manipulation of point clouds as if they were acquired on a flat surface (section 5). The construction of a DTM starts with known or sampled ground points and uses various spatial interpolation techniques to infer ground points at unsampled locations. Accuracy of the DTM is very important because errors will propagate to future processing stages like tree height estimation. A wide range of methods exist for spatial interpolation of points. In the following section we will use the classified Topography.laz data set, which is included internally within lidR to create reproducible examples. LASfile &lt;- system.file(&quot;extdata&quot;, &quot;Topography.laz&quot;, package=&quot;lidR&quot;) las &lt;- readLAS(LASfile, select = &quot;xyzc&quot;) plot(las, size = 3, bg = &quot;white&quot;) 4.1 Triangular irregular network This method is based on triangular irregular network (TIN) of ground point data to derive a bivariate function for each triangle, which is then used to estimate the values at unsampled locations (between known ground points). Planar facets of each generated triangle are used to interpolate. Used with a Delaunay triangulation, this is the most simple solution because it involves no parameters. The Delaunay triangulation is unique and the linear interpolation is parameter-free. The drawbacks of the method are that it creates a non-smooth DTM and that it cannot extrapolate the terrain outside the convex hull delimited by the ground points since there are no triangle facets outside the convex hull. Moreover, the interpolation is weak at the edges because large irrelevant triangles are often created. It’s therefore important to compute the triangulation with a buffer to be able to crop the DTM and clear the edge artifacts. To generate a DTM model with the TIN algorithm we use rasterize_terrain() where algorithm = tin(). dtm_tin &lt;- rasterize_terrain(las, res = 1, algorithm = tin()) plot_dtm3d(dtm_tin, bg = &quot;white&quot;) Notice the ugly edge interpolations. This occurs because we didn’t process with a buffer. 4.2 Invert distance weighting Invert distance weighting (IDW) is one of the simplest and most readily available methods that can be applied to create DTMs. It is based on an assumption that the value at an unsampled point can be approximated as a weighted average of values at points within a certain cut-off distance d, or from a given number k of closest neighbours. Weights are usually inversely proportional to a power p of the distance between the location and the neighbour, which leads to the computing of an estimator. Compared to tin() this method is more robust to edge artifacts because it uses a more relevant neighbourhood but generates terrains that are “bumpy” and probably not as realistic as those generated using TINs. There are always trade-offs to different methods! To generate a DTM model with the IDW algorithm we use rasterize_terrain() where algorithm = knnidw(). dtm_idw &lt;- rasterize_terrain(las, algorithm = knnidw(k = 10L, p = 2)) plot_dtm3d(dtm_idw, bg = &quot;white&quot;) Notice the bumpy nature of the DTM compared to the previous one generated with tin(). In 1D and IDW interpolation looks like: 4.3 Kriging Kriging is the most advanced approach and utilizes advanced geostatistical interpolation methods that take into account the relationships between the returns and their respective distances from each other. lidR uses the package gstat to perform the kriging. This method is very advanced, difficult to manipulate, and extremely slow to compute, but probably provides the best results with minimal edge artifacts. To generate a DTM model with the kriging algorithm we use rasterize_terrain() where algorithm = kriging(). dtm_kriging &lt;- rasterize_terrain(las, algorithm = kriging(k = 40)) plot_dtm3d(dtm_kriging, bg = &quot;white&quot;) Notice that the algorithm has issues interpolating regions with missing point such as lakes. 4.4 Pros and cons Triangulation is a very fast and efficient method that generates very good DTMs and is robust to empty regions inside the point cloud. It is however weak at edges. Although lidR uses the nearest neighbour to complete the missing pixel out of the convex hull of the ground points the interpolation remains poor. This algorithm must therefore always be used with a buffer of extra points to ensure that the region of interest is not on an edge. The TIN method is recommended for broad DTM computation but should be avoided for small regions of interest loaded without buffers. Invert distance weighting is fast, but approximately twice as slower than TIN. The terrain is not very realistic, but edges are likely to be free of strong edge artifacts. IDW is a compromise between TIN and KRIGING. It is recommended if you want a simple method, if you cannot load a buffer, and if edge regions are important. Kriging is very slow because it is computationally demanding. It is not recommended for use on medium to large areas. It can be used for small plots without buffers to get a nice DTM without strong edges artifact. Whatever the method used, edges are critical. Results will always be weak if the method needs to guess the local topography with only partial information on the neighborhood. Though different methods provide better and worse estimates in these regions, best practice is to always use a buffer to obtain some information about the neighborhood and remove the buffer once the terrain is computed. 4.5 Other methods Spatial interpolation is not limited to the 3 methods described above. Many more have been presented and described in the literature. In section 19 we will learn how to create a plugin algorithm compatible with rasterize_terrain() based on a multilevel B-spline approximation (MBA) using the MBA package. dtm_mba &lt;- rasterize_terrain(las, algorithm = mba()) plot_dtm3d(dtm_mba, bg = &quot;white&quot;) 4.6 Render shaded DTM Generating a hillshade layer in R is relatively straight forward and is done using functions from the terra package. The terrain() and hillShade() functions can be combined to take the DTM raster layers as input and return a hillshade raster: library(terra) #&gt; terra 1.7.39 dtm &lt;- rasterize_terrain(las, algorithm = tin(), pkg =&quot;terra&quot;) dtm_prod &lt;- terrain(dtm, v = c(&quot;slope&quot;, &quot;aspect&quot;), unit = &quot;radians&quot;) dtm_hillshade &lt;- shade(slope = dtm_prod$slope, aspect = dtm_prod$aspect) plot(dtm_hillshade, col =gray(0:30/30), legend = FALSE) The rayshader package also provides interesting tools to generate shaded DTM. The dtm must be a RasterLayer library(rayshader) dtm &lt;- raster::raster(dtm) elmat &lt;- raster_to_matrix(dtm) #&gt; [1] &quot;Dimensions of matrix are: 286x286&quot; map &lt;- elmat %&gt;% sphere_shade(texture = &quot;imhof1&quot;, progbar = FALSE) %&gt;% add_water(detect_water(elmat), color = &quot;imhof1&quot;) %&gt;% add_shadow(ray_shade(elmat, progbar = FALSE), 0.5) %&gt;% add_shadow(ambient_shade(elmat, progbar = FALSE), 0) 2D plot plot_map(map) 3D plot plot_3d(map, elmat, zscale = 1, windowsize = c(800, 800)) "],["norm.html", "5 Height normalization 5.1 DTM normalization 5.2 Point cloud normalization 5.3 Hybrid method 5.4 Pros and cons 5.5 Reversing normalization", " 5 Height normalization The purpose of the DTM, apart from using it as a stand alone product for water drainage, archaeology, road planning etc. is to facilitate terrain normalization. Described simply, point cloud normalization removes the influence of terrain on above ground measurements. This makes comparison of above ground vegetation heights possible and simplifies analyses across acquisition areas. When reading a non-normalized file we can see the terrain variation are visible. LASfile &lt;- system.file(&quot;extdata&quot;, &quot;Topography.laz&quot;, package=&quot;lidR&quot;) las &lt;- readLAS(LASfile) plot(las, size = 3, bg = &quot;white&quot;) To get a better idea of what the terrain looks like lets remove all non-ground points gnd &lt;- filter_ground(las) plot(gnd, size = 3, bg = &quot;white&quot;, color = &quot;Classification&quot;) The goal of normalization is to get a flat terrain. Two normalization approaches are most commonly used: Subtract the derived raster DTM (section 4) elevation from all non-ground returns. Interpolate ground points directly and subtract beneath the non-ground returns. 5.1 DTM normalization To normalize points using a DTM we first need to create the DTM itself. For this we use the rasterize_terrain() function (see section 4). For this example we chose to use a grid resolution of 1 m and to use the knnidw() algorithm with default parameters. dtm &lt;- rasterize_terrain(las, 1, knnidw()) plot(dtm, col = gray(1:50/50)) Now that we have our surface and are satisfied with it we can use it to normalize our point cloud through subtraction. nlas &lt;- las - dtm plot(nlas, size = 4, bg = &quot;white&quot;) We can see that the point cloud has been normalized, making the point cloud flat. All the elevations are now relative to the ground surface. The ground surface being the reference 0 all the ground points are expected to be at Z = 0 by definition. But are they? Lets look at the distribution of ground points. hist(filter_ground(nlas)$Z, breaks = seq(-0.6, 0.6, 0.01), main = &quot;&quot;, xlab = &quot;Elevation&quot;) We can see that the ground points are not all at Z=0 and the histogram shows some points at +/- 25 cm. This occurs because the DTM is a discretized raster. The location of the pixels do not match the locations of the ground points. Lets assume we have two ground points with elevations of 257.5 and 258 meters respectively in a given pixel at 257.9 m. After normalization, their respective elevation will be -0.4 m and 0.1 m because each pixel has a single value meaning that all the points within a given pixel get normalized with the exact same elevation value. In a raster, the elevations are a succession of flat areas with discontinuities at each pixel. Thus a simple subtraction of the raster gives good results visually, but in practice can lead to many inaccuracies because of the discretized nature of the storage format. 5.2 Point cloud normalization Point cloud normalization without a DTM interpolates the elevation of every single point locations using ground points. It no longer uses elevations at discrete predefined locations. Thus the methods is exact, computationally speaking. It means that it is equivalent to using a continuous DTM but it is important to recall that all interpolation methods are interpolation and by definition make guesses with different strategies. Thus by “exact” we mean “continuous”. To compute the continuous normalization, we can feed normalize_height() with an algorithm for spatial interpolation instead of a raster. nlas &lt;- normalize_height(las, knnidw()) All the ground points should be exactly 0. Let check it: hist(filter_ground(nlas)$Z, breaks = seq(-0.6, 0.6, 0.01), main = &quot;&quot;, xlab = &quot;Elevation&quot;) One can reproduce this with other algorithm such as tin(). It’s also important to recall buffer and edge artifacts also apply here 5.3 Hybrid method nlas &lt;- las - dtm makes very simple subtractions without any interpolation. normalize_height(las, knnidw()) computes on-the-fly the interpolation of the ground points and estimates the exact elevation of every points. An hybrid method consists in the interpolation of the pixel of an already computed DTM. nlas &lt;- normalize_height(las, tin(), dtm = dtm) In this case, the ground points in las are not considered for interpolation. The DTM is used as regularly spaced ground points that are triangulated. hist(filter_ground(nlas)$Z, breaks = seq(-0.6, 0.6, 0.01), main = &quot;&quot;, xlab = &quot;Elevation&quot;) 5.4 Pros and cons Point cloud based normalization is superior in terms of computational accuracy by normalizing with a continuous terrain instead of a discretized terrain. It is however computationally intensive compared to a raster-based method. In addition raster DTMs are storable on disk and can be loaded quicky to be used to normalize different data sets, while point cloud based methods need to be recomputed for each point cloud. It’s up to the user to choose which method best suits their needs. lidR provides the options. The hybrid method is also computationally demanding because it interpolates even more data (a DTM is usually 1 point per square meter while ground points are usually less than that.) 5.5 Reversing normalization lidR also has the capacity to reverse normalization using the unnormalize_height function. This reverts the normalized point cloud to its pre-normalized state. las &lt;- unnormalize_height(nlas) #&gt; NULL "],["chm.html", "6 Digital Surface Model and Canopy Height model 6.1 Point-to-raster 6.2 Triangulation 6.3 Pit-free algorithm 6.4 Post-processing a CHM", " 6 Digital Surface Model and Canopy Height model Digital Surface Models (DSM) and Canopy Height Models (CHM) are raster layers that represent - more or less - the highest elevation of ALS returns. In the case of a normalized point cloud, the derived surface represents the canopy height (for vegetated areas) and is referred to as CHM. When the original (non-normalized) point cloud with absolute elevations is used, the derived layer represents the elevation of the top of the canopy above sea level, and is referred to as DSM. Both surface models are derived using the same algorithms, with the only difference being the elevation values of the point cloud. Different methods exist to create DSMs and CHMs. In the most simple case, a grid can be created with a user-defined pixel size and the elevations of the highest point can be assigned to each grid cell. This is called point-to-raster. More complex methods have been presented in the literature. In this section we will use the normalized MixedConifer.laz data set, which is included internally within lidR to create reproducible examples. LASfile &lt;- system.file(&quot;extdata&quot;, &quot;MixedConifer.laz&quot;, package =&quot;lidR&quot;) las &lt;- readLAS(LASfile) plot(las, size = 3, bg = &quot;white&quot;) 6.1 Point-to-raster Point-to-raster algorithms are conceptually simple, consisting of establishing a grid at a user defined resolution and attributing the elevation of the highest point to each pixel. Algorithmic implementations are computationally simple and extremely fast. In the first example we will set the pixel size to 1 and set algorithm = p2r(). chm &lt;- rasterize_canopy(las, res = 1, algorithm = p2r()) col &lt;- height.colors(25) plot(chm, col = col) One drawback of the point-to-raster method is that some pixels can be empty if the grid resolution is too fine for the available point density. Some pixels may then fall within a location that does not contain any points, and as a result the value is not defined. In the following example we will use the exact same method, but increase the spatial resolution of the raster by changing the pixel size to 0.5 m. chm &lt;- rasterize_canopy(las, res = 0.5, algorithm = p2r()) plot(chm, col = col) We can clearly see that there are a lot of empty pixels in the derived surface that correspond to NA pixels in the point cloud. The spatial resolution was increased, however the CHM contains to many voids. One option to reduce the number of voids in the surface model is to replace every point in the point cloud with a disk of a known radius (e.g. 15 cm). This operation is meant to simulate the fact that the laser footprint is not a point, but rather a circular area. It is equivalent to computing the CHM from a densified point cloud in a way that tries to have a physical meaning. chm &lt;- rasterize_canopy(las, res = 0.5, algorithm = p2r(subcircle = 0.15)) plot(chm, col = col) This CHM is the CHM computed “as if” the point cloud was the following We can zoom in to see the small disks that replace each point. The p2r() function contains one additional argument that allows to interpolate the remaining empty pixels. Empty pixels are interpolated using methods described in section 4. chm &lt;- rasterize_canopy(las, res = 0.5, p2r(0.2, na.fill = tin())) plot(chm, col = col) 6.2 Triangulation The triangulation algorithm works by first creating a triangular irregular network (TIN) using first returns only, followed by interpolation within each triangle to compute an elevation value for each pixel of a raster. In its simplest form, this method consists of a strict 2-D triangulation of first returns. Despite being more complex compared to the point-to-raster algorithm, an advantage of the triangulation approach is that it is parameter free and it does not output empty pixels, regardless of the resolution of the output raster (i.e. the entire area is interpolated). Like point-to-raster however, the TIN method can lead to gaps and other noise in the surface - referred to as “pits” - that are attributable to first returns that penetrated deep into the canopy. Pits can make individual tree segmentation more difficult and change the texture of the canopy in a unrealistic way. To avoid this issue the CHM is often smoothed in post processing in an attempt to produce a more realistic surface with fewer pits and less noise. To create a surface model using triangulation we use algorithm = dsmtin(). chm &lt;- rasterize_canopy(las, res = 0.5, algorithm = dsmtin()) plot(chm, col = col) The triangulation method may also be weak when a lot of points are missing. We can generate an example using the Topography.laz data set that contains empty lakes. LASfile &lt;- system.file(&quot;extdata&quot;, &quot;Topography.laz&quot;, package = &quot;lidR&quot;) las2 &lt;- readLAS(LASfile) las2 &lt;- normalize_height(las2, algorithm = tin()) plot(las2, size = 3, bg = &quot;white&quot;) In this case the CHM is incorrectly computed in the empty lakes chm &lt;- rasterize_canopy(las2, res = 0.5, algorithm = dsmtin()) plot(chm, col = col) To fix this, one option is to use the max_edge argument, which defines the maximum edge of a triangle allowed in the Delaunay triangulation. By default this argument is set to 0 meaning that no triangle are removed. If set to e.g. 8 it means that every triangle with an edge longer than 8 will be discarded from the triangulation. chm &lt;- rasterize_canopy(las2, res = 0.5, algorithm = dsmtin(max_edge = 8)) plot(chm, col = col) 6.3 Pit-free algorithm More advanced algorithms have also been designed that avoid pits during the computation step instead of requiring post-processing. Khosravipour et al. (2014) proposed a ‘pit-free’ algorithm, which consists of a series of sequential height thresholds where Delaunay triangulations are applied to first returns. For each threshold, the triangulation is cleaned of triangles that are too large, similar to the example given in the previous section. In a final step, the partial rasters are stacked and only the highest pixels of each raster are retained (figure below). The output is a DSM that is expected to be natively free of pits without using any post-processing or correction methods. To understand this method better, we can reproduce the figure above with algorithm = dsmtin(): # The first layer is a regular triangulation layer0 &lt;- rasterize_canopy(las, res = 0.5, algorithm = dsmtin()) # Triangulation of first return above 10 m above10 &lt;- filter_poi(las, Z &gt;= 10) layer10 &lt;- rasterize_canopy(above10, res = 0.5, algorithm = dsmtin(max_edge = 1.5)) # Triangulation of first return above 20 m above20 &lt;- filter_poi(above10, Z &gt;= 20) layer20 &lt;- rasterize_canopy(above20, res = 0.5, algorithm = dsmtin(max_edge = 1.5)) # The final surface is a stack of the partial rasters dsm &lt;- layer0 dsm[] &lt;- pmax(as.numeric(layer0[]), as.numeric(layer10[]), as.numeric(layer20[]), na.rm = T) layers &lt;- c(layer0, layer10, layer20, dsm) names(layers) &lt;- c(&quot;Base&quot;, &quot;Layer10m&quot;, &quot;Layer20m&quot;, &quot;pitfree&quot;) plot(layers, col = col) In practice, the internal implementation of pitfree() works much like the example above but is easier to use. chm &lt;- rasterize_canopy(las, res = 0.5, pitfree(thresholds = c(0, 10, 20), max_edge = c(0, 1.5))) plot(chm, col = col) By increasing the max_edge argument for the pit-free triangulation part from 1.5 to 2 the CHM becomes smoother but also less realistic: chm &lt;- rasterize_canopy(las, res = 0.5, pitfree(max_edge = c(0, 2.5))) plot(chm, col = col) Similarly to point-to-raster, the pit-free algorithm in lidR also includes a subcircle option that replaces each first return by a disk made of 8 points. Because it would be too computationally demanding to triangulate 8 times more points, the choice was made to select only the highest point of each pixel after subcircling to perform the triangulation. chm &lt;- rasterize_canopy(las, res = 0.5, pitfree(subcircle = 0.15)) plot(chm, col = col) 6.4 Post-processing a CHM CHMs are usually post-processed to smooth or to fill empty pixels. This is mainly because most publications use a point-to-raster approach without any tweaks. One may want to apply a post-processing step to the CHM. Sadly lidR does not provide any tools for that. lidR is a point cloud oriented software. Once a function has returned a raster, the job of the lidR package has ended. Further work requires other dedicated tools to process rasters. The terra package is one of those. The code below presents a simple option to fill NAs and smooth a CHM with the terra package. For more details see the documentation of the package terra. fill.na &lt;- function(x, i=5) { if (is.na(x)[i]) { return(mean(x, na.rm = TRUE)) } else { return(x[i]) }} w &lt;- matrix(1, 3, 3) chm &lt;- rasterize_canopy(las, res = 0.5, algorithm = p2r(subcircle = 0.15), pkg = &quot;terra&quot;) filled &lt;- terra::focal(chm, w, fun = fill.na) smoothed &lt;- terra::focal(chm, w, fun = mean, na.rm = TRUE) chms &lt;- c(chm, filled, smoothed) names(chms) &lt;- c(&quot;Base&quot;, &quot;Filled&quot;, &quot;Smoothed&quot;) plot(chms, col = col) #&gt; NULL #&gt; NULL "],["itd-its.html", "7 Indivitual tree dectection and segmentation 7.1 Individual Tree Detection (ITD) 7.2 Individual Tree Segmentation (ITS)", " 7 Indivitual tree dectection and segmentation Individual tree detection (ITD) is the process of spatially locating trees and extracting height information. Individual tree segmentation (ITS) is the process of individually delineating detected trees. In lidR, detecting and segmenting functions are decoupled to maximize flexibility. Tree tops are first detected using the locate_trees() function, followed crown delineation using segment_trees(). In the following section we will use the MixedConifer.laz data set, which is included internally within lidR to demonstrate both ITD and ITS with reproducible examples. We will also generate a CHM (section 6) to help visualize the results. LASfile &lt;- system.file(&quot;extdata&quot;, &quot;MixedConifer.laz&quot;, package=&quot;lidR&quot;) las &lt;- readLAS(LASfile, select = &quot;xyzr&quot;, filter = &quot;-drop_z_below 0&quot;) chm &lt;- rasterize_canopy(las, 0.5, pitfree(subcircle = 0.2)) plot(las, bg = &quot;white&quot;, size = 4) 7.1 Individual Tree Detection (ITD) Tree tops can be detected by applying a Local Maximum Filter (LMF) on the loaded data set. The LMF in lidR is point cloud-based, meaning that it finds the tree tops from the point cloud without using any raster. The processing, however is actually very similar. For a given point, the algorithm analyzes neighbourhood points, checking if the processed point is the highest. This algorithm can be applied with the lmf() function. 7.1.1 Local Maximum Filter with fixed windows size The LMF can be applied with a constant size windows. Here with a windows size of ws = 5 meters meaning that for a given point the algorithm looks to the neigbourhood points within a 2.5 radius circle to figure out if the point is the local highest. While the algorithm does not need any CHM to work we chose to display the results on top of a CHM for better visualization. ttops &lt;- locate_trees(las, lmf(ws = 5)) plot(chm, col = height.colors(50)) plot(sf::st_geometry(ttops), add = TRUE, pch = 3) Tree detection results can also be visualized in 3D! x &lt;- plot(las, bg = &quot;white&quot;, size = 4) add_treetops3d(x, ttops) The number of detected trees is correlated to the ws argument. Small windows sizes usually gives more trees, while large windows size generally miss smaller trees that are “hidden” by big trees that contain the highest points in the neighbourhood. This can be seen in the figure below where too many trees are found with a small window size and only the dominant trees are found with a large windows size. ttops_3m &lt;- locate_trees(las, lmf(ws = 3)) ttops_11m &lt;- locate_trees(las, lmf(ws = 11)) par(mfrow=c(1,2)) plot(chm, col = height.colors(50)) plot(sf::st_geometry(ttops_3m), add = TRUE, pch = 3) plot(chm, col = height.colors(50)) plot(sf::st_geometry(ttops_11m), add = TRUE, pch = 3) 7.1.2 Local Maximum Filter with variable windows size The examples above demonstrate the lmf() function with a constant window size. A large windows is suitable for large scattered trees while a small windows size if preferable for small and close trees. In reality trees of variable sizes may be present in a single scene leading to sub-optimal outputs. To overcome this issue, a windows size that adapts to the height of pixels or height of the points in our case becomes necessary. For example, a point at 30 m (a big tree) will be tested with a large window, while a point at 10 m (a smaller tree) will be tested with a smaller window. The window size can therefore be defined as a function of height. Tall trees have comparatively larger crowns, needing larger window sizes to accurately detect their treetops. Variable window sizes are especially suitable for stands of more complex structures, or when tree detection is performed on larger areas, covering heterogeneous stands. In lidR a user can design a function that computes a windows size as a function of point (or pixel) height. When designing a function to define the window size based on point heights we need to determine what the minimum and maximum window size should be related to the minimum and maximum tree heights. In general, the minimum window size should not be smaller than 3 meters. Below we show an example where the windows size is related to the point height with an affine relationship. When a point is at 0 the windows size is 3 meters. At 10 m it is 4 m and so on. f &lt;- function(x) {x * 0.1 + 3} heights &lt;- seq(0,30,5) ws &lt;- f(heights) plot(heights, ws, type = &quot;l&quot;, ylim = c(0,6)) When applied within lmf(), the function yields the following result: ttops &lt;- locate_trees(las, lmf(f)) plot(chm, col = height.colors(50)) plot(sf::st_geometry(ttops), add = TRUE, pch = 3) There is no intrinsic limitation to the user-defined function. One does however need to pay attention to special cases. For example, if a point is below 0 m the function above may compute a negative windows size and the detection will fail. Similarly, if an outlier is encountered a massive window may be computed and lead to suspicious results. We therefore recommend using a more robust function with some built in thresholds. In the next example any points below 2 m will equate to a window size of 3 m, while points above 20 meters equate to a window size of 5 m. Anything between 2 and 20 meter will have a non-linear relationship (for the need of the demo). f &lt;- function(x) { y &lt;- 2.6 * (-(exp(-0.08*(x-2)) - 1)) + 3 y[x &lt; 2] &lt;- 3 y[x &gt; 20] &lt;- 5 return(y) } heights &lt;- seq(-5,30,0.5) ws &lt;- f(heights) plot(heights, ws, type = &quot;l&quot;, ylim = c(0,5)) 7.1.3 Local Maximum Filter on a CHM So far tree detection was performed on a point cloud. A CHM can however also be used to find trees instead of the point cloud. This is also a perfectly valid way to process ALS data. Performing the detection on a CHM is faster because there are less data to process, but it is also more complex because the output depends on how the CHM has been built. The spatial resolution (i.e. pixel size), the algorithm used (see section 6), and any additional post-processing steps will influence tree detection results. In the examples below we run tree detection on CHMs generated with different algorithms (p2r, pitfree), different resolutions (0.5 and 1 m), and different post-processing smoothing steps with a median filter applied. First the different CHMs are generated: # Point-to-raster 2 resolutions chm_p2r_05 &lt;- rasterize_canopy(las, 0.5, p2r(subcircle = 0.2), pkg = &quot;terra&quot;) chm_p2r_1 &lt;- rasterize_canopy(las, 1, p2r(subcircle = 0.2), pkg = &quot;terra&quot;) # Pitfree with and without subcircle tweak chm_pitfree_05_1 &lt;- rasterize_canopy(las, 0.5, pitfree(), pkg = &quot;terra&quot;) chm_pitfree_05_2 &lt;- rasterize_canopy(las, 0.5, pitfree(subcircle = 0.2), pkg = &quot;terra&quot;) # Post-processing median filter kernel &lt;- matrix(1,3,3) chm_p2r_05_smoothed &lt;- terra::focal(chm_p2r_05, w = kernel, fun = median, na.rm = TRUE) chm_p2r_1_smoothed &lt;- terra::focal(chm_p2r_1, w = kernel, fun = median, na.rm = TRUE) Then the same tree detection routine with a constant windows size of 5 m is applied to each CHM: ttops_chm_p2r_05 &lt;- locate_trees(chm_p2r_05, lmf(5)) ttops_chm_p2r_1 &lt;- locate_trees(chm_p2r_1, lmf(5)) ttops_chm_pitfree_05_1 &lt;- locate_trees(chm_pitfree_05_1, lmf(5)) ttops_chm_pitfree_05_2 &lt;- locate_trees(chm_pitfree_05_2, lmf(5)) ttops_chm_p2r_05_smoothed &lt;- locate_trees(chm_p2r_05_smoothed, lmf(5)) ttops_chm_p2r_1_smoothed &lt;- locate_trees(chm_p2r_1_smoothed, lmf(5)) Finally the detection results are visualized to see the various output found as a function of the CHM building choices: par(mfrow=c(3,2)) col &lt;- height.colors(50) plot(chm_p2r_05, main = &quot;CHM P2R 0.5&quot;, col = col); plot(sf::st_geometry(ttops_chm_p2r_05), add = T, pch =3) plot(chm_p2r_1, main = &quot;CHM P2R 1&quot;, col = col); plot(sf::st_geometry(ttops_chm_p2r_1), add = T, pch = 3) plot(chm_p2r_05_smoothed, main = &quot;CHM P2R 0.5 smoothed&quot;, col = col); plot(sf::st_geometry(ttops_chm_p2r_05_smoothed), add = T, pch =3) plot(chm_p2r_1_smoothed, main = &quot;CHM P2R 1 smoothed&quot;, col = col); plot(sf::st_geometry(ttops_chm_p2r_1_smoothed), add = T, pch =3) plot(chm_pitfree_05_1, main = &quot;CHM PITFREE 1&quot;, col = col); plot(sf::st_geometry(ttops_chm_pitfree_05_1), add = T, pch =3) plot(chm_pitfree_05_2, main = &quot;CHM PITFREE 2&quot;, col = col); plot(sf::st_geometry(ttops_chm_pitfree_05_2), add = T, pch =3) 7.2 Individual Tree Segmentation (ITS) While individual tree detection provides very useful information about tree density and size, one may want to go further to segment and extract each tree individually. Several algorithms are available in lidR and can be divided in two families. Point cloud-based that perform without a CHM. Raster-based that perform with a CHM. Each family can be divided into two sub families Algorithms that work in two steps - Individual tree detection followed by segmentation. Algorithms that are all-in-one. In this section we won’t go through all of these possibilities, but its important to recognize that all algorithms are not well suited to every context. In the examples below we used the dalponte2016() algorithm, because we found it to perform best using the example data. 7.2.1 Segmentation of the point-cloud Even when the algorithm is raster-based (which is the case of dalponte2016()), lidR segments the point cloud and assigns an ID to each point by inserting a new attribute named treeID in the LAS object. This means that every point is associated with a particular tree. This is because lidR is point cloud oriented and we want to provide an immediate way to be able to access a segmented point cloud not the segmented raster. algo &lt;- dalponte2016(chm_p2r_05_smoothed, ttops_chm_p2r_05_smoothed) las &lt;- segment_trees(las, algo) # segment point cloud plot(las, bg = &quot;white&quot;, size = 4, color = &quot;treeID&quot;) # visualize trees We assign an ID to each point because it enables interesting analyses at the point-cloud level. This could involve extracting every tree to derive measurements. In the following we extracted and displayed the tree number 110. tree110 &lt;- filter_poi(las, treeID == 110) plot(tree110, size = 8, bg = &quot;white&quot;) lidR provides functions that can be used following segment_trees(), like delineation of crown shapes using crown_metrics(). This function computes the hulls (either convex or concave) of each tree as well as user-defined metrics (see also sections 8, 9, 10, 11, 12 and 13): crowns &lt;- crown_metrics(las, func = .stdtreemetrics, geom = &quot;convex&quot;) plot(crowns[&quot;convhull_area&quot;], main = &quot;Crown area (convex hull)&quot;) 7.2.2 Segmentation of the CHM While point cloud segmentation is standard in lidR, users may only have access to a CHM. There are many reasons for only using a CHM, and this is why raster-based methods can be run standalone outside segment_trees(). Whether using the point cloud or a raster, segmentation results will be exactly the same. The difference will be the data format of the segmentation result. In lidR, a LAS object will gain a treeID attribute, while for rasters, delineated crowns are returned in a raster format. To work outside segment_trees() it suffices to call the function standalone like this: algo &lt;- dalponte2016(chm_p2r_05_smoothed, ttops_chm_p2r_05_smoothed) crowns &lt;- algo() plot(crowns, col = pastel.colors(200)) The output is a raster, for which lidR does not provide any processing tools. At this stage it is up to the user to find the required tools to perform more analysis. The terra package is a good place to start! 7.2.3 Comparaison of tree segmentations At the point cloud level it’s pretty easy to compare tree segmentations by choosing a different attribute names for each method. For example we can compare dalponte2016 (which is a raster based methods in two steps) and li2012 (which is an “all-in-one” point cloud based method) side by side. algo1 &lt;- dalponte2016(chm_p2r_05_smoothed, ttops_chm_p2r_05_smoothed) algo2 &lt;- li2012() las &lt;- segment_trees(las, algo1, attribute = &quot;IDdalponte&quot;) las &lt;- segment_trees(las, algo2, attribute = &quot;IDli&quot;) x &lt;- plot(las, bg = &quot;white&quot;, size = 4, color = &quot;IDdalponte&quot;, colorPalette = pastel.colors(200)) #&gt; The argument &#39;coloPalette&#39; is deprecated. Use &#39;pal&#39; instead plot(las, add = x + c(100,0), bg = &quot;white&quot;, size = 4, color = &quot;IDli&quot;, colorPalette = pastel.colors(200)) #&gt; The argument &#39;coloPalette&#39; is deprecated. Use &#39;pal&#39; instead By comparing their crowns with crown_metrics() we can more easily see that the li2012 algorithm doesn’t perform well in this example with default parameters - maybe some parameter tuning can lead to better results! crowns_dalponte &lt;- crown_metrics(las, func = NULL, attribute = &quot;IDdalponte&quot;, geom = &quot;concave&quot;) crowns_li &lt;- crown_metrics(las, func = NULL, attribute = &quot;IDli&quot;, geom = &quot;concave&quot;) par(mfrow=c(1,2),mar=rep(0,4)) plot(sf::st_geometry(crowns_dalponte), reset = FALSE) plot(sf::st_geometry(crowns_li), reset = FALSE) #&gt; NULL "],["metrics.html", "8 Derived metrics 8.1 The basics 8.2 User-defined metrics 8.3 Pre-defined metrics 8.4 Metrics using 3rd party packages", " 8 Derived metrics The goal of this section is to describe the notion of metrics in lidR. Analyses of point cloud data are often based on metrics calculations. Metrics are scalar summaries of point distributions that can be computed using varying neighborhood definitions and varying reference locations. For example, when reference locations are regularly distributed on a grid and the neigbourhood is a square centered on those locations, we typically use an area-based-approach (ABA) that consists of computing metrics for pixels. Maximum height of points inside a 20 x 20 m grid cell (in this case of pixel) is an example of an ABA metric. This however is not the only option, and other definitions can enable other analyses and applications. The standard deviation of point heights within a single tree crown is an example of a metric calculated at the tree level. The average distance between a point and its k-nearest neighbours is a metrics calculated at the point level. More complex metrics can even be imagined like the average distance between first and last returns within a pixel or within a tree. In the end, regardless of the scale at which metrics are calculated, they serve as proxies of forest inventory attributes or can be used as independent variables in predictive models. The notion of metrics is at the core of the lidR package, which enables the computation of standard or custom user-defined metrics at varying levels of regularization: point cloud level with cloud_metrics() pixel level with pixel_metrics() tree crown level with crown_metrics() inventory plot level with plot_metrics() voxel level with voxel_metrics() polygon level with polygon_metrics() hexagonal cells level with hexagon_metrics() point level with point_metrics() Sections 9, 10, 11, 12 and 13 are respectively dedicated to go deeper into regularization levels. In the majority of cases, derived metrics are calculated based on point heights (Z coordinate) at the cell level since they are the most useful predictors for developing forest inventory attribute models. From a strictly technical point of view however, any point cloud attribute can be used to calculate metrics (e.g. mean Intensity, maximum ScanAngleRank), or several attributes can be used simultaneously (e.g. proportion of first returns above mean Z, standard deviation of Intensity of second returns only). A metric is simply a number derived from the attributes of a subset of points. Thus the potential number of metrics that can be calculated is very large and only limited by user’s imagination. There are however published studies that discuss the usefulness of different metrics for modelling and standard sets of metrics exist. 8.1 The basics While there are some differences between functions dedicated to metric computations the basic idea is the same for all of them. In addition to the input point cloud, the user needs to provide a formula to calculate the metric(s) of interest. For example, the average height of points for the point cloud, for each pixel, for each tree crown, for each voxel or for each hexagonal cell can be be calculated using mean(Z). cloud_metrics(las, func = ~mean(Z)) pixel_metrics(las, func = ~mean(Z)) tree_metrics(las, func = ~mean(Z)) hexagon_metrics(las, func = ~mean(Z)) voxel_metrics(las, func = ~mean(Z)) All the functions work the same way but the output format depends on the regularization level. In each case, outputs will be a different class (i.e. a list, a spatial raster, a spatial vector or a data.frame) to take advantage of the best storage method, but the same two metrics will be calculated for every unit of analysis. In the following example we are computing average intensity at different levels of regularization LASfile &lt;- system.file(&quot;extdata&quot;, &quot;MixedConifer.laz&quot;, package =&quot;lidR&quot;) las &lt;- readLAS(LASfile) m &lt;- ~list(avgI = mean(Intensity)) a &lt;- pixel_metrics(las, m, res = 5) b &lt;- crown_metrics(las, m, geom = &quot;point&quot;) c &lt;- crown_metrics(las, m, geom = &quot;convex&quot;) d &lt;- hexagon_metrics(las, m, area = 25) par(mfrow=c(2,2)) plot(a, col = heat.colors(15), legend = FALSE) plot(b[&quot;avgI&quot;], pal = heat.colors, pch = 19, cex = 1, axes = TRUE, key.pos = NULL, reset = FALSE) plot(c[&quot;avgI&quot;], pal = heat.colors, axes = TRUE, key.pos = NULL, reset = FALSE) plot(d[&quot;avgI&quot;], pal = heat.colors, axes = TRUE, key.pos = NULL, reset = FALSE) 8.2 User-defined metrics In the example above only a single metric is calculated - the mean intensity of the points. Calculations can however be easily extended to any number of user-defined metrics. To do this, users can design custom functions. The function can contain any number of metrics, but needs to return a labeled list. For example, to calculate the mean of elevation, and the standard deviation and mean of intensity, the following function can be used: f &lt;- function(z, i) { list( mean = mean(z), sd = sd(i), imean = mean(i)) } The user-defined function f can then be used. cloud_metrics(las, func = ~f(Z, Intensity)) pixel_metrics(las, func = ~f(Z, Intensity)) tree_metrics(las, func = ~f(Z, Intensity)) haxagon_metrics(las, func = ~f(Z, Intensity)) voxel_metrics(las, func = ~f(Z, Intensity)) While any metric can be computed at any level of regularization, it’s important to recognize that they may not all be meaningful. For example, the average elevation of points has a meaning at the pixel level or at the tree level but not at the voxel level. Indeed at the voxel level it corresponds to the average elevation of the points within the voxel i.e. more or less the Z coordinate of the voxel. This in the end needs to be considered by the user. Perhaps asking “does this metric make sense?” is a good place to start! 8.3 Pre-defined metrics The most commonly used metrics are already predefined in lidR - the stdmetrics*() group of functions contain metrics that summarize the vertical distribution of points, their intensities, and return structure. The complete list of all metrics can be found in the lidR wiki page and can be use that way: cloud_metrics(las, func = .stdmetrics) pixel_metrics(las, func = .stdmetrics) tree_metrics(las, func = .stdmetrics) voxel_metrics(las, func = .stdmetrics) 8.4 Metrics using 3rd party packages In some cases, users may want to calculate metrics from lidar data that are complex or less intuitive to code. In these examples it makes sense to use functions that are already available from other packages. We present 2 pertinent examples below that were brought to our attention by lidR users. 8.4.1 L-moments L-moments are linear combinations of ordered data values. In the context of lidar data this relates to elevation and intensity. The ratios of L-moments are comparable to variance, skewness and kurtosis, and have been shown valuable for a number of lidar-derived attribute modeling studies (eg. Valbuena et al. 2017, Adnan et al. 2021) as well as being standard metrics produced in FUSION lidar processing software. In order to calculate these metrics we can leverage the lmom package which provides functions to calculate L-moments and associated ratios. In the example we use the samlmu() function. cloud_metrics(las, func = ~as.list(lmom::samlmu(Z))) pixel_metrics(las, func = ~as.list(lmom::samlmu(Z)), 10) We see that we can supply the func argument in pixel_metrics() with a function from an external package. This allows us to be fairly flexible with the metrics we are able to calculate without needing to introduce dependencies within the lidR package. 8.4.2 Fractal dimensions Fractal dimensions are another statistical description that is showing promise within lidar and forestry related research (eg. Saarinen et al., 2021). The use of fractal dimensions seeks to help understand the relationship between structural complexity and stem/crown size and shape. The calculation of fractal dimensions — similarly to L-moments — can be tricky to implement efficiently, and would add an additional dependency to lidR’s growing list. To avoid lidR needing to depend on external packages, we can leverage them (in this case Rdimtools) to calculate fractal dimensions within a pixel_metrics() example. First, we can create a user-defined function that leverages the est.boxcount() function within Rdimtools. This allows us to define the variables we want the function to utilize within pixel_metrics() #create user-defined function fd = function(X,Y,Z) { M = cbind(X,Y,Z) est.boxcount(M)$estdim } Now the we have defined the fd function, which calculates our fractal dimensions using XYZ values, we can apply it on our lidar data and create output rasters. cloud_metrics(las, func = ~fd(X,Y,Z), 10) pixel_metrics(las, func = ~fd(X,Y,Z), 10) There are obviously many potential calculations to apply and statistics to derive from lidar data. The ability to include third party functions like the examples listed above within metric calculations makes lidR valuable and flexible from a research standpoint. #&gt; NULL "],["cba.html", "9 Derived metrics at the cloud level 9.1 Overview 9.2 Applications", " 9 Derived metrics at the cloud level 9.1 Overview The “cloud” level of regularization corresponds to the computation of derived metrics using all available points. As seen in section 8, calculating derived metrics for the whole point cloud is straightforward and users only need to provide a formula to calculate metric(s) of interest. For example, to calculate the average height (mean(Z)) of all points we can run the following: LASfile &lt;- system.file(&quot;extdata&quot;, &quot;Megaplot.laz&quot;, package=&quot;lidR&quot;) las &lt;- readLAS(LASfile) cloud_metrics(las, func = ~mean(Z)) # calculate mean height #&gt; [1] 13.27202 To calculate more than one metric a custom function needs to be used, or one of the pre-defined functions within lidR. To calculate the whole suite of 36 metrics defined in stdmetrics_z() we can use func = .stdmetrics_z. When several metrics are computed they are returned as a list. metrics &lt;- cloud_metrics(las, func = .stdmetrics_z) str(head(metrics)) # output is a list #&gt; List of 6 #&gt; $ zmax : num 30 #&gt; $ zmean : num 13.3 #&gt; $ zsd : num 7.45 #&gt; $ zskew : num -0.476 #&gt; $ zkurt : num 2.08 #&gt; $ zentropy: num 0.903 9.2 Applications Point cloud metrics become interesting when computed for a set of plot inventories. In this case it can serves to compute a set of metrics for each plot, where known attributes have been measured in the field to construct a predictive model. Users could easily clip and loop through plot inventory files but we defined an all-in-one convenient function plot_metrics(). In the following example we load a .las and compute the metrics for each plot inventory using a shapefile of plot centers LASfile &lt;- system.file(&quot;extdata&quot;, &quot;Megaplot.laz&quot;, package=&quot;lidR&quot;) las &lt;- readLAS(LASfile, filter = &quot;-keep_random_fraction 0.5&quot;) shpfile &lt;- system.file(&quot;extdata&quot;, &quot;efi_plot.shp&quot;, package=&quot;lidR&quot;) inventory &lt;- sf::st_read(shpfile, quiet = TRUE) metrics &lt;- plot_metrics(las, .stdmetrics_z, inventory, radius = 11.28) Look at the content of inventory and metrics. inventory contains the plot IDs, their coordinates, and VOI a Value Of Interest. metrics contains 36 derived metrics for each plot combined with the inventory data head(inventory) #&gt; Simple feature collection with 5 features and 2 fields #&gt; Geometry type: POINT #&gt; Dimension: XY #&gt; Bounding box: xmin: 684838.9 ymin: 5017796 xmax: 684976.6 ymax: 5017958 #&gt; Projected CRS: NAD83 / UTM zone 17N #&gt; IDPEP VOI geometry #&gt; 1 PEPQ1 14.157140 POINT (684976.6 5017821) #&gt; 2 PEPQ2 12.720584 POINT (684923.9 5017958) #&gt; 3 PEPQ3 11.396656 POINT (684838.9 5017942) #&gt; 4 PEPQ4 11.597471 POINT (684855 5017891) #&gt; 5 PEPQ5 8.263425 POINT (684944 5017796) head(metrics[,1:8]) #&gt; Simple feature collection with 5 features and 8 fields #&gt; Geometry type: POINT #&gt; Dimension: XY #&gt; Bounding box: xmin: 684838.9 ymin: 5017796 xmax: 684976.6 ymax: 5017958 #&gt; Projected CRS: NAD83 / UTM zone 17N #&gt; IDPEP VOI zmax zmean zsd zskew zkurt zentropy geometry #&gt; 1 PEPQ1 14.157140 26.51 15.402475 7.395206 -1.0168956 2.825871 0.8730854 POINT (684976.6 5017821) #&gt; 2 PEPQ2 12.720584 25.26 16.429223 5.923490 -1.0499688 3.568286 0.8865369 POINT (684923.9 5017958) #&gt; 3 PEPQ3 11.396656 26.18 16.174680 6.624851 -0.7560671 2.593582 0.9242759 POINT (684838.9 5017942) #&gt; 4 PEPQ4 11.597471 25.56 12.871502 6.588364 -0.2325813 2.489815 0.9148133 POINT (684855 5017891) #&gt; 5 PEPQ5 8.263425 21.47 8.573705 5.689968 0.1281190 2.153115 0.9344224 POINT (684944 5017796) We have computed many metrics for each plot and we know the value of interest VOI. We can use that to build a linear model with some metrics. Here we have only 5 plots so it is not going to be big science model &lt;- lm(VOI~zsd+zmax, data = metrics) summary(model) #&gt; #&gt; Call: #&gt; lm(formula = VOI ~ zsd + zmax, data = metrics) #&gt; #&gt; Residuals: #&gt; 1 2 3 4 5 #&gt; 0.86492 1.08095 -1.30483 -0.56486 -0.07619 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -12.1213 8.6603 -1.400 0.297 #&gt; zsd 0.4045 1.6778 0.241 0.832 #&gt; zmax 0.8458 0.5536 1.528 0.266 #&gt; #&gt; Residual standard error: 1.404 on 2 degrees of freedom #&gt; Multiple R-squared: 0.792, Adjusted R-squared: 0.5841 #&gt; F-statistic: 3.809 on 2 and 2 DF, p-value: 0.208 This example can be improved. In section 14 we will study how to extract a ground inventory and in section 16 we will study more in depth modelling presenting a complete workflow from the plot extraction to the mapping of the predictive model. #&gt; NULL "],["aba.html", "10 Derived metrics at the pixel level 10.1 Overview 10.2 Applications", " 10 Derived metrics at the pixel level 10.1 Overview The “pixel” level of regularization corresponds to the computation of derived metrics for regularly spaced locations in 2D. Derived metrics calculated at pixel level are the basis of the area-based approach (ABA) that we discuss with more details in section 16. In brief, the ABA allows the creation of wall-to-wall predictions of forest inventory attributes (e.g. basal area or total volume per hectare) by linking ALS variables with field measured references. ABA is one application of derived metrics at the pixel level but not the only one. As seen in sections 8 and 9 calculating derived metrics is straightforward. The user only needs to provide a formula to calculate the metric of interest. For example, to calculate the average height (mean(Z)) of all points within 10 x 10 m pixels we can run the following: hmean &lt;- pixel_metrics(las, ~mean(Z), 10) # calculate mean at 10 m plot(hmean, col = height.colors(50)) The returned hmean object is a raster. The default format is terra but an argument pkg allows for RasterLayer or stars outputs: hmean #&gt; class : SpatRaster #&gt; dimensions : 50, 50, 1 (nrow, ncol, nlyr) #&gt; resolution : 10, 10 (x, y) #&gt; extent : 338000, 338500, 5238500, 5239000 (xmin, xmax, ymin, ymax) #&gt; coord. ref. : WGS 84 / UTM zone 19N (EPSG:32619) #&gt; source(s) : memory #&gt; name : V1 #&gt; min value : 0.2670795 #&gt; max value : 14.0898504 As described in sections 8 and 9, to calculate more than one metric at a time a custom function needs to be created first. The function can contain any number of metrics but needs to return a labeled list. For example, to calculate the mean and standard deviation of point heights, the following function can be created. In this case the return object is a multilayer raster is returned. f &lt;- function(x) { # user-defined fucntion list(mean = mean(x), sd = sd(x)) } metrics &lt;- pixel_metrics(las, ~f(Z), 10) # calculate grid metrics plot(metrics, col = height.colors(50)) The functions that specify which metrics to calculate can of course contain any number of metrics. The most commonly used metrics are already predefined in lidR - the stdmetrics() function contains metrics that summarize the vertical distribution of points, their intensities, and return structure. The complete list of all metrics can be found in the lidR wiki page. To use the predefined list of 56 metrics we can run the pixel_metrics() function as follows: metrics &lt;- pixel_metrics(las, .stdmetrics, 10) # calculate standard metrics plot(metrics, col = height.colors(50)) Because of the flexibility in defining metrics, it is very easy to extend basic functionality to create new, non-standard metrics. For example, below we demonstrate how the coefficient of variation and inter-quartile range can be calculated: metrics_custom &lt;- function(z) { # user defined function list( coef_var &lt;- sd(z) / mean(z) * 100, # coefficient of variation iqr &lt;- IQR(z)) # inter-quartile range } metrics &lt;- pixel_metrics(las, ~metrics_custom(z=Z), 10) # calculate grid metrics plot(metrics, col = height.colors(25)) 10.2 Applications 10.2.1 Modeling All *_metrics functions can map any kind of formula as long as it returns a number or a list of numbers, meaning that that it’s possible to input an expression derived from a predictive model to map the resource. In the section 9 we made a model that can be written \\(0.7018 \\times pzabove2 + 0.9268 \\times zmax\\). We can map this predictive model with a resolution of 10 meters: prediction &lt;- pixel_metrics(las, ~0.7018 * sum(Z &gt; 2)/length(Z) + 0.9268 *max(Z), 20) # predicting model mapping plot(prediction, col = height.colors(50)) # some plotting 10.2.2 Density Point density is the number of points within a pixel divided by the area of the pixel. density &lt;- pixel_metrics(las, ~length(Z)/16, 4) # calculate density plot(density, col = gray.colors(50,0,1)) # some plotting When using only the first returns, the same formula gives the pulse density instead of the point density density &lt;- pixel_metrics(las, ~length(Z)/16, 4, filter = ~ReturnNumber == 1L) 10.2.3 Intensity It’s possible to generate a map of the average intensity of first return only imap &lt;- pixel_metrics(las, ~mean(Intensity), 4, filter = ~ReturnNumber == 1L) # mapping average intensity plot(imap, col = heat.colors(25)) 10.2.4 Other Many other raster-based applications can be derived with adequate metrics. In section 17 we will see some out of the box possibilities to demonstrate how the concept of metrics can be leveraged to design new applications. A simple uncommon application could be to map the ratio between multiple returns and single returns. To count single returns we can count the number of points where number of returns equal to 1. To count the number of multiple returns we can count the number of points with a return number equal to 1 AND a return number above 1. mymetric &lt;- function(return_number, number_of_returns) { #user-defined function nsingle &lt;- sum(number_of_returns == 1L) nmultiple &lt;- sum(return_number == 1L &amp; number_of_returns &gt; 1L) return(list(n_single = nsingle, n_multiple = nmultiple, ratio = nmultiple/nsingle)) } rmap &lt;- pixel_metrics(las, ~mymetric(ReturnNumber, NumberOfReturns), 8) # mapping retunrs plot(rmap, col = viridis::viridis(50)) #&gt; NULL "],["tba.html", "11 Derived metrics at the tree level 11.1 Overview 11.2 Applications", " 11 Derived metrics at the tree level 11.1 Overview The “tree” level of regularization corresponds to the computation of derived metrics centered on each tree using the points that belong to each tree. Derived metrics calculated at tree level are the basis for an inventory at the individual tree level or the basis for individual species identification. Similarly to what we have seen in sections 8, 9, and 10 calculating derived metrics is straightforward and works exactly the same way as in cloud_metrics() or pixel_metrics(). Derived tree metrics are calculated using the crown_metrics() function. The input data for this function is a point cloud that needs to contain information from tree segmentation (e.g. usually the treeID attribute). In the majority of cases crown_metrics() is run after segmenting tree crowns with segment_trees() (section 7) but the segmentation could also be performed in another way independently of lidR. In the example below we show the basic use of the crown_metrics() function on the files we used in section 7. This file already stores an ID for each point referring to each tree, so we don’t need to perform the segmentation first. We compute two metrics z_max and z_mean using the formula list(z_max = max(Z), z_mean = mean(Z). The output is a sf/sfc_POINT. LASfile &lt;- system.file(&quot;extdata&quot;, &quot;MixedConifer.laz&quot;, package=&quot;lidR&quot;) las &lt;- readLAS(LASfile, filter = &quot;-drop_z_below 0&quot;) # read the file metrics &lt;- crown_metrics(las, ~list(z_max = max(Z), z_mean = mean(Z))) # calculate tree metrics head(metrics) #&gt; Simple feature collection with 6 features and 3 fields #&gt; Geometry type: POINT #&gt; Dimension: XYZ #&gt; Bounding box: xmin: 481263.4 ymin: 3812992 xmax: 481294.7 ymax: 3813011 #&gt; z_range: zmin: 9.18 zmax: 26.95 #&gt; Projected CRS: NAD83 / UTM zone 12N #&gt; treeID z_max z_mean geometry #&gt; 1 1 16.00 7.232609 POINT Z (481294.7 3813011 16) #&gt; 2 2 26.95 19.401542 POINT Z (481281.9 3813003 2... #&gt; 3 3 23.58 18.242222 POINT Z (481278.4 3813002 2... #&gt; 4 4 15.83 8.820677 POINT Z (481272.9 3813006 1... #&gt; 5 5 20.91 10.642114 POINT Z (481265.7 3812992 2... #&gt; 6 6 9.18 2.578750 POINT Z (481263.4 3812992 9... The XYZ coordinates of the points correspond to the XYZ coordinates of the highest point within each tree and each point is associated to 3 attributes treeID + the two user-defined metrics. In the plot below the output is visualized using color to depict the z_max metrics. plot(metrics[&quot;z_max&quot;], pal = hcl.colors, pch = 19) # plot using z_max Like other functions seen in sections 9 and 10, users can create their own custom functions containing all of the metrics of interest. In the example below we show how to calculate metrics that are based both on point heights and intensities. custom_crown_metrics &lt;- function(z, i) { # user-defined function metrics &lt;- list( z_max = max(z), # max height z_sd = sd(z), # vertical variability of points i_mean = mean(i), # mean intensity i_max = max(i) # max intensity ) return(metrics) # output } ccm = ~custom_crown_metrics(z = Z, i = Intensity) crown_metrics() can also return sf/sfc_POLYGON by changing the geom argument metrics &lt;- crown_metrics(las, func = ccm, geom = &quot;convex&quot;) plot(metrics[&quot;z_max&quot;], pal = hcl.colors) metrics &lt;- crown_metrics(las, func = ccm, geom = &quot;concave&quot;) plot(metrics[&quot;z_max&quot;], pal = hcl.colors) 11.2 Applications 11.2.1 Selection of trees crown_metrics() gives the ID of trees and associated metrics. We can use these data to filter the scene and remove trees with a low intensity. metrics &lt;- crown_metrics(las, ~list(imean = mean(Intensity))) # calculate tree intensity metrics metrics &lt;- metrics[metrics$imean &gt; 80,] # filter intensity subset &lt;- filter_poi(las, treeID %in% metrics$treeID) x &lt;- plot(las, bg = &quot;white&quot;, size = 4) plot(subset, add = x + c(-100, 0), size = 5) # some plotting 11.2.2 Tree based inventory Assuming we know a relationship between the derived metrics and a value of interest G - such as the biomass of a tree - we can map the resource. Lets assume that \\(G = 0.7 \\times z_{max} + 0.1 \\times i_{mean}\\). In real life a value of interest is more likely to be related to the crown size, but this is a simplified example. First we can compute G for each tree: metrics &lt;- crown_metrics(las, func = ~custom_crown_metrics(Z, Intensity)) # calculate intensity metrics metrics$G &lt;- 0.7 * metrics$z_max + 0.1 * metrics$i_mean # set value of interest plot(metrics[&quot;G&quot;], pal = hcl.colors, pch = 19) # some plotting Then using a raster package, we can rasterize the map. Here we get the sum of G from each tree within each 15 m pixel to get the total value of G for a given pixel. The final output is a predictive model that mixes the area-based approach and the tree-based-approach. r &lt;- terra::rast(ext(las), resolution = 15) v &lt;- terra::vect(metrics[&quot;G&quot;]) map &lt;- terra::rasterize(v, r, field = &quot;G&quot;, fun = sum) # extract sum of G at 15m plot(map, col = hcl.colors(15)) # some plotting This is a small example that may not be of interest, but one may imagine or test the result on a bigger data set. #&gt; NULL "],["vba.html", "12 Derived metrics at the voxel level 12.1 Overview 12.2 Applications", " 12 Derived metrics at the voxel level 12.1 Overview The “voxel” level of regularization corresponds to the computation of derived metrics for regularly spaced location in 3D. The voxel_metrics() function allows calculation of voxel-based metrics on provided point clouds and works like cloud_metrics(), grid_metrics(), and tree_metrics() seen in section 8, 9, 10 and 11. In the examples below we use the Megaplot.laz data set, but the potential to use voxel_metrics() is particularly interesting for dense point clouds such as those produced by terrestrial lidar, or digital photogrammetry. 12.2 Applications We can count the number of points inside each 4 x 4 x 4 m voxel: LASfile &lt;- system.file(&quot;extdata&quot;, &quot;Megaplot.laz&quot;, package=&quot;lidR&quot;) las &lt;- readLAS(LASfile) # read file vox_met &lt;- voxel_metrics(las, ~list(N = length(Z)), 4) # calculate voxel metrics In this example the point cloud is first converted into 4 m voxels, then the function length(Z) is applied to all points located inside every voxel. The output is a data.table that contains the X, Y, and Z coordinates of voxels, and the calculated number of points and can be visualized in 3D using the plot() function as follows: plot(vox_met, color=&quot;N&quot;, colorPalette = heat.colors(50), size = 4, bg = &quot;white&quot;, voxel = TRUE) #&gt; The argument &#39;coloPalette&#39; is deprecated. Use &#39;pal&#39; instead Similarly to other *_metrics() functions designed to calculate derived metrics, voxel_metrics() can be used to calculate any number of pre- or user-defined summaries. For example, to calculate minimum, mean, maximum, and standard deviation of intensity in each voxel we can create a following function: custom_metrics &lt;- function(x) { # user-defined function m &lt;- list( i_min = min(x), i_mean = mean(x), i_max = max(x), i_sd = sd(x) ) return(m) # output } vox_met &lt;- voxel_metrics(las, ~custom_metrics(Intensity), 4) # calculate voxel metrics #&gt; NULL "],["pba.html", "13 Derived metrics at point level 13.1 Overview 13.2 Applications", " 13 Derived metrics at point level 13.1 Overview The “point” level of regularization corresponds to the computation of derived metrics for each point of the point cloud using its neighborhood to define a local subset. The point_metrics() function allows calculation of point-based metrics and works like cloud_metrics(), grid_metrics(), tree_metrics() or voxel_metrics() seen in section 8, 9, 10, 11 and 12. Refer to these sections for a more comprehensive overview. For each point, the neighbourhood can be either: The k nearest neighbours The points within a sphere centred on the processed point, or The k nearest neighbours but with a limited radius search. The syntax for these 3 options is given below: LASfile &lt;- system.file(&quot;extdata&quot;, &quot;Megaplot.laz&quot;, package=&quot;lidR&quot;) las &lt;- readLAS(LASfile) # read file metrics &lt;- point_metrics(las, ~list(imean = mean(Intensity)), k = 7) # 1 metrics &lt;- point_metrics(las, ~list(imean = mean(Intensity)), r = 3) # 2 metrics &lt;- point_metrics(las, ~list(imean = mean(Intensity)), k = 7, r = 3) # 3 The output is a data.table with the ID of the reference points and the metric(s) for each. head(metrics) #&gt; pointID imean #&gt; 1: 1 41.42857 #&gt; 2: 2 41.42857 #&gt; 3: 3 33.71429 #&gt; 4: 4 3.00000 #&gt; 5: 5 29.28571 #&gt; 6: 6 22.00000 Instead of the ID, one may prefer to get the coordinates with xyz = TRUE: metrics &lt;- point_metrics(las, ~list(imean = mean(Intensity)), k = 7, xyz = TRUE) It is also possible to process a selection of points (excluding some points) without creating a copy of the point cloud. In the following, the mean intensity is computed excluding outliers with an intensity above 100. No metric is computed for outliers, and they are not used to get the local neighborhood of the other points. metrics &lt;- point_metrics(las, ~list(imean = mean(Intensity)), k = 7, filter = ~Intensity &lt; 100 ) # calculate mean intensity and exclude outliers head(metrics) #&gt; pointID imean #&gt; 1: 1 41.42857 #&gt; 2: 2 41.42857 #&gt; 3: 3 33.71429 #&gt; 4: 4 16.57143 #&gt; 5: 5 29.28571 #&gt; 6: 6 23.14286 13.2 Applications 13.2.1 Roof segmentation Roofs are flat areas. Limberger et al. (2015) described a method to find flat areas in a point clouds using an eigen values decomposition. Using a user-defined function we can use point_metrics() to create this algorithm. In this example we will use the following urban scene: las &lt;- readLAS(&quot;data/chap11/building_WilliamsAZ_Urban_normalized.laz&quot;) plot(las, size = 3) First we need to define a function that computes the eigen value decomposition of a set of points and estimates if the set of points is flat according to the Limberger et al. (2015) criteria. is.planar &lt;- function(x, y, z, th1 = 25, th2 = 6) { xyz &lt;- cbind(x,y,z) cov_m &lt;- cov(xyz) eigen_m &lt;- eigen(cov_m)$value is_planar &lt;- eigen_m[2] &gt; (th1*eigen_m[3]) &amp;&amp; (th2*eigen_m[2]) &gt; eigen_m[1] return(list(planar = is_planar)) } We then apply this function using 20 neighbors. We also use filter = ~Classification != LASGROUND to not process points classified as ground. We do this first, because the scene is normalized so by definition each ground point is expected to be part of a flat planar surface, and second, because it will speed-up computation because fewer points will be considered. M &lt;- point_metrics(las, ~is.planar(X,Y,Z), k = 20, filter = ~Classification != LASGROUND) We finally merge the output with the point cloud to visualize the result: las &lt;- add_attribute(las, FALSE, &quot;planar&quot;) las$planar[M$pointID] &lt;- M$planar plot(las, color = &quot;planar&quot;) We can eventually set a valid classification (LASBUILDING) to those points: las$Classification[M$pointID] &lt;- LASBUILDING The function is.planar() is highly inefficient because eigen() is very slow. We can rewrite the eigen decomposition in C++ with Rcpp to make the function 10 times faster! Rcpp::sourceCpp(code = &quot; #include &lt;RcppArmadillo.h&gt; // [[Rcpp::depends(RcppArmadillo)]] // [[Rcpp::export]] arma::vec eigen_values(arma::mat A) { arma::mat coeff, score; arma::vec latent; arma::princomp(coeff, score, latent, A); return(latent); }&quot;) is.planar &lt;- function(x, y, z, th1 = 25, th2 = 6) { xyz &lt;- cbind(x,y,z) eigen_m &lt;- eigen_values(xyz) is_planar &lt;- eigen_m[2] &gt; (th1*eigen_m[3]) &amp;&amp; (th2*eigen_m[2]) &gt; eigen_m[1] return(list(planar = is_planar)) } We can do this… but we know that not everyone is well versed in C++. lidR therefore has a dedicated and even faster function (segment_shapes()) for this specific task because we believe that it’s of high interest to provide a specialized and faster version of this tool. las &lt;- segment_shapes(las, shp_plane(k = 25), &quot;planar&quot;) 13.2.2 Lake and wire segmentation Eigen value decomposition applied using point_metrics() opens a lot of possibilities. For example, by tweaking the previous example we can design a lake segmentation algorithm. A lake is a planar region with a vertical normal vector, so to create a lake segmentation we simply need to add such constraints in is.planar(). We can also design a wire segmentation algorithm. For a wire we can change the Limberger et al. (2015) constrains to enable the detection of elongated linear features instead of flat feature. is.linear &lt;- function(x, y, z, th = 10) { xyz &lt;- cbind(x,y,z) eigen_m &lt;- eigen_values(xyz) is_linear &lt;- th*eigen_m[3] &lt; eigen_m[1] &amp;&amp; th*eigen_m[2] &lt; eigen_m[1] return(list(linear = is_linear)) } Again, lidR has a dedicated function for this specific task because we believe that it is of high interest to provide a specialized and faster version of this tool. las &lt;- segment_shapes(las, shp_line(th = 8, k = 15), &quot;linear&quot;) 13.2.3 Multi-spectral coloring point_metrics() is not limited to eigen value related metrics. It is only limited by a users imagination. In the following example we will demonstrate how it an be used to attribute false colors to a multi-spectral point cloud. Multi-spectral ALS data are sampled with 3 scanners each emitting a different wavelength. The point cloud is usually provided in the form of 3 .las files where each file corresponds to a spectral wavelength. No matter the actual wavelength, we can consider the first band as blue, the second as red and the third as green, and thus consider that each point has a pure color. f1 &lt;- &quot;data/chap11/PR1107_c1_ar_c.laz&quot; f2 &lt;- &quot;data/chap11/PR1107_c2_ar_c.laz&quot; f3 &lt;- &quot;data/chap11/PR1107_c3_ar_c.laz&quot; las &lt;- readMSLAS(f1, f2, f3, filter = &quot;-keep_z_below 300&quot;) plot(las, color = &quot;ScannerChannel&quot;, size = 5) We now want to attribute an RGB value to each point. A single point being sampled with a single ‘color’ we need to use its neighborhood to define 3 bands. For each point we look in its neighbourhood, where some points are red, some are blue, and some are green. We average the intensities of each color and we consider these 3 values as the RGB color of the central point. Because some points are likely to have 0 red/blue/green neighbours we can set R, G, and B equal to NA for those points and later discard those points. set.color &lt;- function(intensity, channel) { # Split the intensities of each channel i1 &lt;- intensity[channel == 1] i2 &lt;- intensity[channel == 2] i3 &lt;- intensity[channel == 3] # If one channel is missing return RGB = NA if (length(i1) == 0 | length(i2) == 0 | length(i3) == 0) return(list(R = NA_integer_, G = NA_integer_, B = NA_integer_)) # Average and normalise the intensities i1 &lt;- as.integer(mean(i1)) i2 &lt;- as.integer(mean(i2)) i3 &lt;- as.integer(mean(i3)) if (i1 &gt; 255L) i1 &lt;- 255L if (i2 &gt; 255L) i2 &lt;- 255L if (i3 &gt; 255L) i3 &lt;- 255L return(list(R = i1, G = i2, B = i3)) } We can then apply this function with point_metrics() using a spherical neighborhood. The next steps being to assign the output of point_metrics() back into the LAS object for a nice display. M &lt;- point_metrics(las, ~set.color(Intensity, ScannerChannel), r = 0.5) las &lt;- add_lasrgb(las, M$R, M$G, M$B) colored &lt;- filter_poi(las, !is.na(R)) # remove RGB = NA plot(colored, color = &quot;RGB&quot;, size = 3) This method is a bit naive ‘as is’. First, the intensities returned by each channel are not comparable and required to be normalized. We could also argue about the choice of discarding RGB = NA. Instead we could have chosen to set a pure color. To finish, enforcing a maximum value to 255 works in this specific example because very few intensity values are actually above 255 but is meaningless in a general case. Either way, this is only a demo to show how to think out of the box with point_metrics(). 13.2.4 Outlier filtering An outlier is a point that is alone compared to other points. Outliers are usually high points with no close neighbors. The term “no close neighbor” can have several formal definitions such as no neighbors closer than x meters or less than n neighbors within a distance x meter among others. In all cases, it can be estimated with metrics and thus point_metrics() enables the design of an outlier filter method. Perhaps a potential exercise for the reader :) . #&gt; NULL #&gt; &lt;STYLE type=&#39;text/css&#39; scoped&gt; #&gt; PRE.fansi SPAN {padding-top: .25em; padding-bottom: .25em}; #&gt; &lt;/STYLE&gt; "],["engine.html", "14 LAScatalog processing engine (1/2) 14.1 Rationale for LAScatalog functionality 14.2 The LAScatalog engine 14.3 Read a collection of files 14.4 Validation 14.5 Extract Regions of interest 14.6 Modification of default behavior 14.7 Ground classification 14.8 Digital Terrain Model 14.9 Height normalization 14.10 Area Based Approach 14.11 Individual Tree Detection 14.12 Individual Tree Segmentation 14.13 Retile a catalog 14.14 The case of ground inventories 14.15 Summary of available options", " 14 LAScatalog processing engine (1/2) 14.1 Rationale for LAScatalog functionality So far we have demonstrated how to process a point cloud file using readLAS(). In practice, ALS acquisitions are made up of hundreds or even thousands of files, not being feasible (or possible!) for all to be loaded at once into memory. For example, the image below shows an acquisition split into 320 1 km² tiles: The extraction of regions of interest (ROIs) such as plot inventories becomes difficult in these situations because one must search in which file and sometimes which files the ROI belongs. It also makes the creation of continuous outputs such as a DTM, a raster of metrics, individual tree detection, or anything else far more complicated. One may be tempted to loop though individual files (and we have seen many users doing so) like so: files &lt;- list.files(&quot;folder/&quot;) for (file in files) { las &lt;- readLAS(file) output &lt;- do.something(las) write(output, &quot;path/to/output.ext&quot;) } This however is very bad practice because each file is loaded without a buffer, meaning outputs will be invalid or weak at the edges because of the absence of spatial context. Let’s compute a DTM (see section 4) as an example, where we use a for loop on 4 contiguous files. We can see the obvious invalidity of the DTM between the files, but also at the periphery, even if less obvious. For comparison, see the accurate DTM below, that was generated with the LAScatalog processing engine. The benefit here is the ability to manage (among a variety of other capabilities) on-the-fly buffering. 14.2 The LAScatalog engine The lidR package provides a powerful and versatile set of tools to work with collections of files and enables the application of workflow with an automatic management of tile buffering, on disk storage, parallelization of tasks and so on. While the engine is powerful and versatile, it’s also a complex tool, so we have dedicated two section to its description. This section presents the justification for the engine and demonstrates how to use it by applying common lidR functions seen in previous section. The next section goes deeper into the engine to demonstrate how developers can leverage the internal functionality of lidR to create their own applications. The LAScatalog class and the LAScatalog engine are intricately documented in two dedicated vignettes available here and here. The purpose of this book is to propose alternative documentation with more illustrated examples and real use cases. 14.3 Read a collection of files The function readLAScatalog() reads the header of all the LAS files of a given folder. The header of a LAS file contains, among others, the bounding box of the point cloud, meaning that it’s possible to know where the file is situated spatially without reading a potentially massive amount of data. readLAScatalog() builds a sf/sfc_POLYGON out of the bounding boxes of the files and presents them in an easily accessible manner. ctg &lt;- readLAScatalog(&quot;path/to/folder/&quot;) ctg #&gt; class : LAScatalog (v1.1 format 1) #&gt; extent : 678815.8, 709272.6, 5004393, 5028388 (xmin, xmax, ymin, ymax) #&gt; coord. ref. : WGS 84 / UTM zone 17N #&gt; area : 295.35 km² #&gt; points : 910.11 million points #&gt; density : 3.1 points/m² #&gt; density : 2.1 pulses/m² #&gt; num. files : 361 plot(ctg) In this and the following section (section 15) we will use a collection of nine 500 x 500 m files to create examples. The nine files can be downloaded here: tiles_338000_5238000.laz tiles_338000_5238500.laz tiles_338000_5239000.laz tiles_338500_5238000.laz tiles_338500_5238500.laz tiles_338500_5239000.laz tiles_339000_5238000.laz tiles_339000_5238500.laz tiles_339000_5239000.laz ctg &lt;- readLAScatalog(&quot;data/ENGINE/catalog/&quot;) ctg #&gt; class : LAScatalog (v1.0 format 1) #&gt; extent : 338000, 339500, 5238000, 5239500 (xmin, xmax, ymin, ymax) #&gt; coord. ref. : WGS 84 / UTM zone 19N #&gt; area : 2.25 km² #&gt; points : 9.45 million points #&gt; density : 4.2 points/m² #&gt; density : 3.4 pulses/m² #&gt; num. files : 9 plot(ctg) 14.4 Validation Similar to section 2, an important first step in ALS data processing is ensuring that your data is complete and valid. The las_check() function performs an inspection of LAScatalog objects for file consistency. las_check(ctg) #&gt; #&gt; Checking headers consistency #&gt; - Checking file version consistency... ✓ #&gt; - Checking scale consistency... ✓ #&gt; - Checking offset consistency... ✓ #&gt; - Checking point type consistency... ✓ #&gt; - Checking VLR consistency... ✓ #&gt; - Checking CRS consistency... ✓ #&gt; Checking the headers #&gt; - Checking scale factor validity... ✓ #&gt; - Checking Point Data Format ID validity... ✓ #&gt; Checking preprocessing already done #&gt; - Checking negative outliers... ✓ #&gt; - Checking normalization... no #&gt; Checking the geometry #&gt; - Checking overlapping tiles... ✓ #&gt; - Checking point indexation... yes For a deep (and longer) inspection of each file use deep = TRUE. This will sequentially load each file entirely. It’s thus important to be sure you have enough memory to manage this. 14.5 Extract Regions of interest 14.5.1 Extract a single ROI Functions using the clip_*() moniker are a good starting point for exploring the capabilities of the LAScataglog engine. clip_*() functions allow for the extraction of a region of interest (ROI) from a point cloud. The following example extracts a circle from a point cloud loaded into memory in a LAS object: plot &lt;- clip_circle(las, x = 338700, y = 5238650, radius = 15) This can be extended to a LAScatalog seamlessly. The engine searches in which file(s) the ROI belongs and extracts corresponding regions from all applicable files. The output is a LAS object. roi &lt;- clip_circle(ctg, x = 338700, y = 5238650, radius = 40) plot(roi, bg = &quot;white&quot;, size = 4) 14.5.2 Multiple extractions Multiple extractions is also possible and is performed the same way by searching the corresponding files and then querying in each file no matter if the region of interest is situated in one or several files. The output becomes list of LAS objects. x &lt;- c(339348.8, 339141.9, 338579.6, 338520.8, 338110.0, 339385) y &lt;- c(5239254, 5238717, 5238169, 5239318, 5239247, 5239290) r &lt;- 40 plot(ctg) points(x, y) rois &lt;- clip_circle(ctg, x, y, r) rois[1:2] #&gt; [[1]] #&gt; class : LAS (v1.0 format 1) #&gt; memory : 1.5 Mb #&gt; extent : 339308.9, 339388.7, 5239214, 5239294 (xmin, xmax, ymin, ymax) #&gt; coord. ref. : WGS 84 / UTM zone 19N #&gt; area : 5051 m² #&gt; points : 25.4 thousand points #&gt; density : 5.04 points/m² #&gt; density : 3.9 pulses/m² #&gt; #&gt; [[2]] #&gt; class : LAS (v1.0 format 1) #&gt; memory : 1.3 Mb #&gt; extent : 339102, 339181.9, 5238677, 5238757 (xmin, xmax, ymin, ymax) #&gt; coord. ref. : WGS 84 / UTM zone 19N #&gt; area : 5013 m² #&gt; points : 21.1 thousand points #&gt; density : 4.2 points/m² #&gt; density : 3.65 pulses/m² 14.6 Modification of default behavior When processing a LAScatalog, no matter with which function, it internally uses what we called the LAScatalog processing engine. This is what happened under the hood when using clip_circle() in above examples. The default behavior of the engine is set in such a way that it returns what is most likely to be expected by the users. However the behavior of the engine can be tuned to optimize processing. Engine options, at the user level, can be modified with opt_*() functions. The goal of this section is to present these options and how they affect the behavior of the engine. 14.6.1 Multiple extractions on disk The engine has the ability to write generated results to disk storage rather than keeping everything in memory. This option can be activated with opt_output_files() &lt;- that is used to designate the path where files will be written to disk. It expects a templated filename so each written file will be attributed a unique name. In the following example, several LAS files will be written to disk with names like 339348.8_5239254_1.las (center coordinates from each file) and the function returns a LAScatalog object that references all the new files instead of a list of LAS object. opt_output_files(ctg) &lt;- paste0(tempdir(), &quot;/{XCENTER}_{YCENTER}_{ID}&quot;) rois &lt;- clip_circle(ctg, x, y, r) rois #&gt; class : LAScatalog (v1.0 format 1) #&gt; extent : 338070, 339424.9, 5238129, 5239358 (xmin, xmax, ymin, ymax) #&gt; coord. ref. : WGS 84 / UTM zone 19N #&gt; area : 38290.55 m² #&gt; points : 123.8 thousand points #&gt; density : 3.2 points/m² #&gt; density : 2.7 pulses/m² #&gt; num. files : 6 plot(rois) We can check the files that were written on disk and see that the names match the template. rois$filename #&gt; [1] &quot;C:\\\\Users\\\\tgood\\\\AppData\\\\Local\\\\Temp\\\\RtmpYnwvYn\\\\339348.8_5239254_1.las&quot; #&gt; [2] &quot;C:\\\\Users\\\\tgood\\\\AppData\\\\Local\\\\Temp\\\\RtmpYnwvYn\\\\339141.9_5238717_2.las&quot; #&gt; [3] &quot;C:\\\\Users\\\\tgood\\\\AppData\\\\Local\\\\Temp\\\\RtmpYnwvYn\\\\338579.6_5238169_3.las&quot; #&gt; [4] &quot;C:\\\\Users\\\\tgood\\\\AppData\\\\Local\\\\Temp\\\\RtmpYnwvYn\\\\338520.8_5239318_4.las&quot; #&gt; [5] &quot;C:\\\\Users\\\\tgood\\\\AppData\\\\Local\\\\Temp\\\\RtmpYnwvYn\\\\338110_5239247_5.las&quot; #&gt; [6] &quot;C:\\\\Users\\\\tgood\\\\AppData\\\\Local\\\\Temp\\\\RtmpYnwvYn\\\\339385_5239290_6.las&quot; 14.6.2 Multiple extraction with point cloud indexation Point cloud indexation is a topic covered by this vignette. In short, LAS file indexation allows for faster queries when extracting ROIs in files. Under the hood lidR relies on LASlib to read files and inherits of the capability to leverage the use of LAX files developed by Martin Isenburg. While extracting hundreds of plots from hundreds of files may take many seconds, the use of index files can reduce processing to few seconds. 14.6.3 Summary We have learned several things about the LAScatalog engine in this section: Many functions of the package work the same either with a point cloud (LAS) or a collection of files (LAScatalog). The behavior of the engine can be modified to write objects to disk instead of loading everything into memory. The engine takes advantage of file indexes. 14.7 Ground classification 14.7.1 Chunk processing classify_ground() (see section 3) is an important function of lidR, which works seamlessly with the LAScatalog engine. An ALS acquisition is processed in pieces, referred to here as chunks. An acquisition is usually split into chunks where files correspond to a tiling pattern. Using classify_ground() within the LAScatalog engine will process each file sequentially. Given that we emphasize the importance of processing point clouds including a buffer, the engine loads a buffer on-the-fly around each tile before any processing is conducted. The most basic implementation is very similar to examples in section 3. In this case we first specify where to write the outputs using opt_output_files(). The files written on disk will be LAS files, while the output in R will be a LAScatalog. If we don’t write to disk, the result of each chunk will be stored into memory, potentially leading to memory issues. opt_output_files(ctg) &lt;- paste0(tempdir(), &quot;{*}_classified&quot;) classified_ctg &lt;- classify_ground(ctg, csf()) In reality, R won’t crash because the function won’t allow users to classify a collection of files without providing a path to save outputs. Most functions in lidR check user inputs to mitigate issues. opt_output_files(ctg) &lt;- &quot;&quot; classified_ctg &lt;- classify_ground(ctg, csf()) #&gt; Error: This function requires that the LAScatalog provides an output file template. 14.7.2 Modifying buffers It is possible to modify the behavior of the engine by modifying the size of the buffer with opt_chunk_buffer() &lt;-. The option chunk = TRUE of the function plot() allows visualization of how an option affects the processing pattern. The red boundaries show the chunks that will be sequentially processed and the green dotted lines show the extended regions used for buffering each chunk. opt_chunk_buffer(ctg) &lt;- 20 plot(ctg, chunk = TRUE) opt_chunk_buffer(ctg) &lt;- 50 plot(ctg, chunk = TRUE) Depending on the point cloud density and the processes applied, it might be necessary to increase the default buffer. Sometimes no buffer is needed at all. No matter the size, the buffered region is only useful to derive a spatial context that extends the actual size of the chunk. After processing, the buffer is removed and never returned in memory or saved in files. It is only loaded temporarily for computation. 14.7.3 Modify the chunk size In the example above we have seen that each chunk is actually a file. This is the default behavior because it corresponds to the physical splitting pattern. The engine can however sequentially process a LAScatalog in any arbitrarily sized chunk. This may be useful when each file is too big to be loaded in memory, and reducing the size of each processing region may be suitable. Sometimes increasing chunk size to process more than a single file might be useful as well. This is controlled by opt_chunk_size() &lt;-, and again the function plot() enables the preview of the processing pattern. # Process sequentially tiny 250 x 250 chunks with 10 m buffer opt_chunk_size(ctg) &lt;- 250 opt_chunk_buffer(ctg) &lt;- 10 plot(ctg, chunk = TRUE) # Process sequentially bigger 800 x 800 chunks with 40 m buffer opt_chunk_size(ctg) &lt;- 800 opt_chunk_buffer(ctg) &lt;- 40 plot(ctg, chunk = TRUE) 14.7.4 Parallel processing In lidR every iterative process can be computed in parallel on a single machine or on multiple machines accessible remotely. Remote parallelization is a very advanced case not covered in this book for which a wiki page has been written. In the following example only local parallelization is covered. Parallelization is driven by the package future. Understanding this package is thus a requirement for being able to parallelize tasks in lidR. In the following example we load future and register a parallelization plan to enable parallelized classification. library(future) plan(multisession) opt_chunk_size(ctg) &lt;- 400 opt_chunk_buffer(ctg) &lt;- 40 opt_output_files(ctg) &lt;- paste0(tempdir(), &quot;/{*}_classified&quot;) classified_ctg &lt;- classify_ground(ctg, csf()) Parallelization is a capability of the engine and every function is capable of benefiting from parallel processing. Registering a parallelization plan would have worked with clip_circle() as well. However doing so does no mean that classification is performed in parallel, but that several chunks will be processed simultaneously. This means that its important to pay attention to memory requirements and parallelization is not necessarily relevant in all cases or in all computers. 14.7.5 Summary We have learned several things about the engine in this section The engine iteratively processes regions of interest called chunks. Each chunk is loaded with a buffer on-the-fly. Users can change chunk sizes to reduce the amount of memory used if files are too big. The buffer is used to remove ridge artifacts, which are removed once the computation is done, and is not transferred to processing outputs. Iterative processes can be performed in parallel. Parallelization is performed by processing several chunks simultaneously. Users need processing memory to load several chunks at a time. 14.8 Digital Terrain Model So far we have learned enough about the engine to generate a nice and valid DTM using rasterize_terrain() (see section 4). 14.8.1 In memory DTM Generating a DTM that encompasses an entire ALS acquisition is as easy as generating a DTM that encompasses a single point cloud file. The following generates a DTM with default parameters (i.e. processing by file with a 30 m buffer). In each chunk rasterize_terrain() computes a DTM. The result is a collection of rasters, however one feature of the engine is to merge everything in single, seamless, manipulable object. A raster is usually less memory intensive that a point cloud, so returning everything in memory is feasible if the raster is not too large. dtm &lt;- rasterize_terrain(ctg, 2, tin(), pkg = &quot;terra&quot;) We can render a shaded DTM like that from section 4.6 to better visualize the output: dtm_prod &lt;- terra::terrain(dtm, v = c(&quot;slope&quot;, &quot;aspect&quot;), unit = &quot;radians&quot;) dtm_hillshade &lt;- terra::shade(slope = dtm_prod$slope, aspect = dtm_prod$aspect) plot(dtm_hillshade, col = gray(0:50/50), legend = FALSE) 14.8.2 On disk DTM When the DTM is too big for memory storage it can be written on disk. In this case users will have one raster file per chunk. The buffer is used to perform a valid computation but is removed after the computation, leaving no overlap between files. It is possible to build a virtual raster from multiple files to return a lightweight raster that references on disk files. opt_output_files(ctg) &lt;- opt_output_files(ctg) &lt;- paste0(tempdir(), &quot;/{*}_dtm&quot;) dtm &lt;- rasterize_terrain(ctg, 1, tin()) dtm #&gt; class : SpatRaster #&gt; dimensions : 1500, 1500, 1 (nrow, ncol, nlyr) #&gt; resolution : 1, 1 (x, y) #&gt; extent : 338000, 339500, 5238000, 5239500 (xmin, xmax, ymin, ymax) #&gt; coord. ref. : WGS 84 / UTM zone 19N (EPSG:32619) #&gt; source : rasterize_terrain.vrt #&gt; name : Z #&gt; min value : 534.024 #&gt; max value : 803.364 class(dtm) #&gt; [1] &quot;SpatRaster&quot; #&gt; attr(,&quot;package&quot;) #&gt; [1] &quot;terra&quot; Light rasters can be used as if it were an in memory raster or a single file raster and is thus much more convenient than having hundred of files on disk without any structure to hold them all. This feature is called Virtual Dataset and is part of the Geospatial Data Abstraction Library (GDAL) (see also gdalbuildvrt). 14.8.3 Summary In this section we learned several the following about the engine in this section The engine takes care of returning a single and manipulable object. 14.9 Height normalization Previous sections detail considerations for using the LAScatalog engine to perform point cloud normalization using normalize_height() (see section 5). We created a DTM in the previous section, so we use it here to normalize. opt_output_files(ctg) &lt;- paste0(tempdir(), &quot;/{*}_norm&quot;) ctg_norm &lt;- normalize_height(ctg, dtm) A point cloud-based normalization without a raster is also possible: opt_output_files(ctg) &lt;- paste0(tempdir(), &quot;/{*}_norm&quot;) ctg_norm &lt;- normalize_height(ctg, tin()) 14.10 Area Based Approach Area based approach (pixel_metrics()) outputs are raster objects like a DTM. Two more options of the engine do exist that are not controllable in every function. When reading a LAS file with readLAS() two options select and filter are offered (see section 2). When processing a collection of files readLAS() is called internally and it is not possible to modify these parameters. Instead we can use the options opt_filter() and opt_select() to propagate these arguments. opt_select(ctg) &lt;- &quot;xyzci&quot; opt_filter(ctg) &lt;- &quot;-keep_first&quot; Used in conjunction with pixel_metrics(), these enable computation on a selected set of points (first returns for example), loading only user specified attributes. This is useful mainly to save processing memory but may also have a small favorable impact on computation time. opt_select(ctg_norm) &lt;- &quot;xyz&quot; hmean &lt;- pixel_metrics(ctg_norm, ~mean(Z), 10) opt_select(ctg_norm) &lt;- &quot;xyz&quot; opt_filter(ctg_norm) &lt;- &quot;-keep_first&quot; hmean &lt;- pixel_metrics(ctg_norm, ~mean(Z), 10) plot(hmean, col = height.colors(25)) Not all functions respect these two options. For example it does not make sense to load only XYZ when creating a DTM because the classification of points is required to isolate ground points. It is also non-beneficial to load intensity, user data, return number and so on to compute a DTM. The function rasterize_terrain() knows which options to choose and does not respect user inputs. This is the case of several other functions in lidR. But when its suitable, users can tune these options. 14.10.1 Summary We have learned several things about the engine in this section Some options from readLAS()can be used to optimize and tune processing Some lidR functions already know the best options and do not respect all user inputs. 14.11 Individual Tree Detection At this stage with have all the tools required to find each individual tree using locate_trees() in a collection of files. The following example extracts every tree found over the acquisition. The output is returned in memory as a single, continuous object. Each chunk is processed with a buffer ensuring that trees will be properly detected at the edges of the files. A total of 100,000 trees were found in this example. ttops &lt;- locate_trees(ctg_norm, lmf(4)) ttops #&gt; Simple feature collection with 102492 features and 2 fields #&gt; Geometry type: POINT #&gt; Dimension: XY #&gt; Bounding box: xmin: 338000 ymin: 5238000 xmax: 339500 ymax: 5239500 #&gt; Projected CRS: WGS 84 / UTM zone 19N #&gt; First 10 features: #&gt; treeID Z geometry #&gt; 1 1 17.441 POINT (338007.6 5238481) #&gt; 2 2 16.313 POINT (338012.7 5238481) #&gt; 3 3 16.342 POINT (338007.1 5238488) #&gt; 4 4 16.199 POINT (338007.4 5238491) #&gt; 5 5 14.140 POINT (338002.9 5238497) #&gt; 6 6 18.999 POINT (338019.3 5238496) #&gt; 7 7 16.330 POINT (338012.1 5238499) #&gt; 8 8 18.669 POINT (338018 5238492) #&gt; 9 9 18.508 POINT (338016.3 5238493) #&gt; 10 10 17.622 POINT (338014.6 5238491) plot(ttops[&quot;Z&quot;], cex = 0.001, pch = 19, pal = height.colors, nbreaks = 30) We see some issues with this however. Each tree is expected to be associated to a single ID but if we count the number of trees with the ID = 1 we can see that we have 9 trees fitting that query. Similarly we have 9 trees labelled “2” and so on. sum(ttops$treeID == 1) #&gt; [1] 9 plot(ttops[&quot;treeID&quot;], cex = 0.3, pch = 19, nbreaks = 50) This error has been voluntarily introduced at this stage to illustrate how the LAScatalog engine works. In the engine, each chunk is processed independently of the others. They can even be processed on different machines and in a more or less random order. Thus there is no way to know how many trees were found in the first chunk to restart the numbering in the second chunk. This is mainly because the ‘first and second’ chunk may not have any meaning when processing in parallel. The numbering is thus restarted to 1 in each chunk. This is why there is 9 trees numbered 1. One in each chunk. When the output is returned in memory this can be easily fixed by reassigning new IDs ttops$treeID &lt;- 1:nrow(ttops) It is however more difficult to achieve the same task if each chunk is written in a file, and almost impossible to achieve in the case of individual tree segmentation as we will see later. To solve this lidR has two strategies to generate reproducible unique IDs no matter the processing order or the number of trees found. For more details the reader can refer to the documentation for locate_trees(). This option can be selected with the parameter uniqueness. ttops &lt;- locate_trees(ctg_norm, lmf(4), uniqueness = &quot;bitmerge&quot;) plot(ttops[&quot;treeID&quot;], cex = 0.01, pch = 19) The attribute treeID is now an arbitrary number (here it is negative but not necessarily) computed from the XY coordinates, that is guaranteed to be 1. unique, and 2. reproducible. No matter how the collection of files is processed (by files or by small chunks, in parallel or sequentially, with large or narrow buffer, in memory or on disk) the IDs will always be the same for a given tree. In all the previous examples we have seen that the engine takes care of merging intermediate results into a single usable object. When intermediate results are stored on disk, the final results in R is a single lightweight object such as a LAScatalog or a Virtual Data set. This is not true for locate_trees() and other functions that return spatial vectors. The default behaviour is to write Geopackage (.gpkg) and there is no way to build a light weight object. What is returned is thus a vector of written files. opt_output_files(ctg_norm) &lt;- paste0(tempdir(), &quot;/{*}&quot;) locate_trees(ctg_norm, lmf(4), uniqueness = &quot;bitmerge&quot;) #&gt; [1] &quot;C:\\\\Users\\\\tgood\\\\AppData\\\\Local\\\\Temp\\\\RtmpYnwvYn/tiles_338000_5238000_1_norm.shp&quot; #&gt; [2] &quot;C:\\\\Users\\\\tgood\\\\AppData\\\\Local\\\\Temp\\\\RtmpYnwvYn/tiles_338000_5238500_1_norm.shp&quot; #&gt; [3] &quot;C:\\\\Users\\\\tgood\\\\AppData\\\\Local\\\\Temp\\\\RtmpYnwvYn/tiles_338000_5239000_1_norm.shp&quot; #&gt; [4] &quot;C:\\\\Users\\\\tgood\\\\AppData\\\\Local\\\\Temp\\\\RtmpYnwvYn/tiles_338500_5238000_1_norm.shp&quot; #&gt; [5] &quot;C:\\\\Users\\\\tgood\\\\AppData\\\\Local\\\\Temp\\\\RtmpYnwvYn/tiles_338500_5238500_1_norm.shp&quot; #&gt; [6] &quot;C:\\\\Users\\\\tgood\\\\AppData\\\\Local\\\\Temp\\\\RtmpYnwvYn/tiles_338500_5239000_1_norm.shp&quot; #&gt; [7] &quot;C:\\\\Users\\\\tgood\\\\AppData\\\\Local\\\\Temp\\\\RtmpYnwvYn/tiles_339000_5238000_1_norm.shp&quot; #&gt; [8] &quot;C:\\\\Users\\\\tgood\\\\AppData\\\\Local\\\\Temp\\\\RtmpYnwvYn/tiles_339000_5238500_1_norm.shp&quot; #&gt; [9] &quot;C:\\\\Users\\\\tgood\\\\AppData\\\\Local\\\\Temp\\\\RtmpYnwvYn/tiles_339000_5239000_1_norm.shp&quot; 14.11.1 Summary We have learned several things about the engine in this section Each chunk is processed strictly independently of the others. When the intermediate outputs are LAS files, the final output is a lightweight LASctalog. When the intermediate outputs are raster files, the final output is a lightweight Virtual Data set. However for spatial vectors and some other data types it is not always possible to aggregate files into a single lightweight object. Thus the names of the files are returned. Strict continuity of the output is not always trivial because each chunk is processed independently of the others but lidR always guarantee the validity and the continuity of the outputs. 14.12 Individual Tree Segmentation Individual tree segmentation with segment_trees() is the most complicated function to use with a LAScatalog because there are many different algorithms. Some imply the use of an in memory raster and an in memory vector, some require only an in memory raster and some require only a point cloud (see section 7). Moreover it is complex to manage edge artifacts. Imagine there is a tree that is situated exactly on the edge between two files. The first half on one file, the second half on another. The two files will be processed independently - maybe on different remote computers. Both sides of the tree must be attributed the same ID independently to get something that is strictly continuous. The function segment_trees() manages all these points. The example below uses the dalponte2016() algorithm. First we create a 1 m resolution CHM stored on disk and returned as a light virtual raster. opt_output_files(ctg_norm) &lt;- paste0(tempdir(), &quot;/chm_{*}&quot;) chm &lt;- rasterize_canopy(ctg_norm, 1, p2r(0.15)) plot(chm, col = height.colors(50)) Second we compute the seeds by finding the trees as seen above. The result must be loaded in memory because there is no way to combine many vectors stored on disk like rasters. In this example it is possible because there are 100,000 trees. But for bigger collections it may not be possible to apply this algorithm in a simple way. opt_output_files(ctg_norm) &lt;- &quot;&quot; ttops &lt;- locate_trees(ctg_norm, lmf(4), uniqueness = &quot;bitmerge&quot;) To finish, we apply segment_trees(). Here we don’t need to specify any strategy to get unique IDs because the seeds are already uniquely labelled. These IDs will be preserved. But for an algorithm without any seed such as watershed() it is important to have a strategy. opt_output_files(ctg_norm) &lt;- paste0(tempdir(), &quot;/{*}_segmented&quot;) algo &lt;- dalponte2016(chm, ttops) ctg_segmented &lt;- segment_trees(ctg_norm, algo) The new LAScatalog that is returned is made up of files that have an extra attribute named treeID where each point is labelled with an ID thats unique for each tree, even those that belong between two or more files that were processed independently. We can load a plot between two files to check: opt_output_files(ctg_segmented) &lt;- &quot;&quot; lasplot &lt;- clip_circle(ctg_segmented, 338500, 5238600, 40) pol = crown_metrics(lasplot, NULL, geom = &quot;convex&quot;) plot(sf::st_geometry(pol), col = pastel.colors(250), axes = T) plot(ctg, add = T) We can observe that the segmentation is perfect with respect to the labelling problem. Trees that were segmented twice independently on each edge were attributed the same IDs on both sides and that the final output is wall-to-wall. 14.13 Retile a catalog The function catalog_retile() allows for retiling an ALS acquisition of files. This is the best example to combine everything we have seen in this section. 14.13.1 Retile a catalog into smaller files catalog_retile() allows for retiling the acquisition of files in tiles of any size. opt_output_files(ctg) &lt;- paste0(tempdir(), &quot;/{XLEFT}_{YBOTTOM}&quot;) # label outputs based on coordinates opt_chunk_buffer(ctg) &lt;- 0 opt_chunk_size(ctg) &lt;- 250 # retile to 250 m small &lt;- catalog_retile(ctg) # apply retile plot(small) # some plotting 14.13.2 Add a buffer around each file catalog_retile() is a special case where the buffer is not removed after computation. The function does not compute anything, but only streams point from inputs files to output files. It can be used to add a buffer around each file. opt_chunk_buffer(ctg) &lt;- 20 # set buffer to 20 opt_chunk_size(ctg) &lt;- 0 # no change to chunk size opt_output_files(ctg) &lt;- paste0(tempdir(), &quot;/{ORIGINALFILENAME}_buffered&quot;) # name outputs with original name and &quot;_buffered&quot; buffered &lt;- catalog_retile(ctg) # apply buffer plot(buffered) # some plotting Be careful. This may be useful for processing in other softwares, but lidR loads a buffers on-the-fly and does not support already buffered files. When processing a buffered collection in lidR the output is likely to be incorrect with the default parameters. 14.13.3 Create a new collection with only first returns In combination with opt_filter(), catalog_retile() can be used to generate a new collection of files that contains only first returns. This is not very useful in lidR because this can be achieved on-the-fly when reading the files using filter() (see section 2.1.2), though could be useful to process point clouds in other software. opt_chunk_buffer(ctg) &lt;- 0 opt_chunk_size(ctg) &lt;- 0 opt_output_files(ctg) &lt;- paste0(tempdir(), &quot;/{ORIGINALFILENAME}_first&quot;) opt_filter(ctg) &lt;- &quot;-keep_first&quot; first &lt;- catalog_retile(ctg) 14.13.4 Create a new collection of small and buffered ground returns in parallel We can combine all the options seen in this section to generate a buffered set of tiny files containing only ground returns. We present that here in parallel. library(future) plan(multisession) opt_output_files(ctg) &lt;- paste0(tempdir(), &quot;/{XLEFT}_{YBOTTOM}_first_buffered&quot;) opt_chunk_buffer(ctg) &lt;- 10 opt_chunk_size(ctg) &lt;- 250 opt_filter(ctg) &lt;- &quot;-keep_class 2&quot; newctg &lt;- catalog_retile(ctg) plot(newctg) 14.14 The case of ground inventories The case of ground inventories or more generally independent files is particular. We generated such collections in the first example of this section. rois is made of unrelated circular files. Loading a buffer is meaningless because there is no neighbouring files. Creating chunks is meaningless as well because we usually want one output per file without generating a wall-to-wall object. Worse even, with the wrong options the output might be incorrect. For example on the top right of the previous collection we can see two overlapping files. If processed by chunk and with a buffer this will load the same points twice in the overlapping area, plus some extra points from the other plot that are not related at all to the first plot. In short, in the case of ground plot inventories 99% of the cases consist in processing iteratively by file, without a buffer and without returning a single aggregated object. This case actually corresponds to a regular for loop, as shown in the introduction of this section. Thus there is no need for all the features provided by the LAScatalog engine and anyone with any background in programming can write a small script that does this job. The engine can however do it with appropriate options. In this case the LAScatalog processing engine becomes more or less a parallelizable for loop with a real time monitoring. opt_chunk_size(rois) &lt;- 0 # processing by files opt_chunk_buffer(rois) &lt;- 0 # no buffer opt_wall_to_wall(rois) &lt;- FALSE # disable internal checks to ensure a valid output. Free wheel mode opt_merge(rois) &lt;- FALSE opt_output_files(rois) &lt;- &quot;&quot; dtm &lt;- rasterize_terrain(rois, 1, tin()) plot(dtm[[1]], col = gray(1:50/50)) This can be set in one command: opt_independent_files(rois) &lt;- TRUE Everything seen so far remains true. But with these options we are sure to not make mistakes when processing independent files. 14.15 Summary of available options The engine processes the covered area by chunk. A chunk is an arbitrary square region queried from the collection of files. Each chunk is loaded with a buffer, and each chunk correspond to an independent results. All together the chunks tessellate the coverage unless files are not overlapping. Results are merged into a single object at the end of the processing when possible. opt_chunk_size(ctg) &lt;- 800 controls the size of the chunks. Set 0 use the tiling pattern (processing by file) opt_chunk_buffer(ctg) &lt;- 40 controls the size of the buffer around each chunks. opt_chunk_alignment(ctg) &lt;- c(100, 200) controls the alignment of the chunks opt_output_file(ctg) &lt;- \"{TEMPLATE} redirects the results in files named after the templated name. opt_select(ctg) &lt;- \"xyzi\" loads only the attributes of interest when querying a chunk, in this case the xyz and intensity values (see readLAS() - section 2.1) opt_filter(ctg) &lt;- \"-keep_first\" loads only the points of interest when querying a chunk (see readLAS() - section 2.1) opt_wall_to_wall(ctg) &lt;- FALSE disables some internal control that guarantee that output will be strictly wall-to-wall. It should not be used actually because opt_independent_files() should cover the vast majority of cases opt_independent_files() &lt;- TRUE to configure the processing for the special case of independent files (typically plot inventories). It turns the engine into a simple loop and the notion of chunks covering the area is lost. opt_merge(ctg) &lt;- FALSE disables the final merging. In this case a list is returned in all cases. opt_progress(ctg) &lt;- FALSE disables progress estimation ticker. opt_stop_early(ctg) &lt;- FALSE disable early exit. If a chunk throws an error the processing keeps going and this chunk will be missing in the output. Use future and register a parallelization plan to process several chunks at a time taking advantage of multiple cores or multiple machines architectures. One can use the function summary() to display the main current processing options of the catalog: summary(ctg) #&gt; class : LAScatalog (v1.0 format 1) #&gt; extent : 338000, 339500, 5238000, 5239500 (xmin, xmax, ymin, ymax) #&gt; coord. ref. : WGS 84 / UTM zone 19N #&gt; area : 2.25 km² #&gt; points : 9.45 million points #&gt; density : 4.2 points/m² #&gt; density : 3.4 pulses/m² #&gt; num. files : 9 #&gt; proc. opt. : buffer: 10 | chunk: 250 #&gt; input opt. : select: * | filter: -keep_class 2 #&gt; output opt. : on disk | w2w guaranteed | merging enabled #&gt; drivers : #&gt; - Raster : format = GTiff NAflag = -999999 #&gt; - stars : NA_value = -999999 #&gt; - SpatRaster : overwrite = FALSE NAflag = -999999 #&gt; - SpatVector : overwrite = FALSE #&gt; - LAS : no parameter #&gt; - Spatial : overwrite = FALSE #&gt; - sf : quiet = TRUE #&gt; - data.frame : no parameter #&gt; NULL "],["engine2.html", "15 LAScatalog processing engine (2/2) 15.1 catalog_apply() 15.2 Create user-defined function 15.3 Create an intermediate function for catalog_apply() 15.4 Make a user-friendly function for third party users 15.5 Make a safe function for third party users", " 15 LAScatalog processing engine (2/2) Section 14 showed how to use the LAScatalog processing engine to apply lidR functions on a collection of files. This included how to process acquisitions in user defined chunk sizes, loading buffers on-the-fly, saving the output to disk, and parallelizing tasks. These examples all used existing functions provided with the lidR package. The power of the engine goes beyond limited used cases, allowing developers to design their own applications. Let’s imagine we want to normalize a data set and colorize it using RGB aerial data. According to the previous sections we could do something like this: opt_output_files(ctg) &lt;- paste0(tempdir(), &quot;/{*}_normalized&quot;) # specify output normalized las disk save location ctg_norm &lt;- normalize_height(ctg, tin()) # normalize # loop through catalog files and apply rgb colour rgb &lt;- terra::rast(&quot;rgb.tif&quot;) for (file in ctg$filename) { newname &lt;- tools::file_path_sans_ext(basename(file)) newname &lt;- paste0(&quot;folder/&quot;, newname, &quot;_colored.laz&quot;) las &lt;- readLAS(file) las &lt;- merge_spatial(las, rgb) writeLAS(las, newname) } This method is however far from optimal. First, the whole point cloud is read two times: To normalize To colorize This takes a significant amount of time. It also implies the need to create copies of the acquisition after each processing step, taking up a lot of disk storage. Then, to finish, a custom for loop is used because merge_spatial() cannot be applied on a LAScatalog natively. What if we were able to do that in a single run? 15.1 catalog_apply() In this section we outline the catalog_apply() function, which is used subliminally in every function seen in the section 14. Here we show how to leverage its versatility to design tools that do not yet exist, like the normalization + colorization example from above, but also more innovative processes. Again, the LAScatalog class and the LAScatalog engine are deeply documented in a two dedicated vignettes available here and here. The purpose of these examples is to provide alternative, hands-on documentation with more applied examples and real use cases. In this section we will use the same 9 files used in section 14 and an RGB raster that encompasses the collection of files. ctg &lt;- readLAScatalog(&quot;data/ENGINE/catalog/&quot;) plot(ctg) The RGB image is available for download here. rgbmap &lt;- terra::rast(&quot;data/ENGINE/catalog/rgb.tif&quot;) terra::plotRGB(rgbmap) 15.2 Create user-defined function First we need to create a function that combines the normalization + colorization steps. Let’s call it norm_color(). It’s simple and uses only 2 functions. norm_color &lt;- function(las, rgbmap) # create user-defined function { nlas &lt;- normalize_height(las, tin()) # normalize colorized &lt;- merge_spatial(nlas, rgbmap) # colorize return(colorized) # output } Let’s try it on a sample plot. las &lt;- clip_circle(ctg, 338800, 5238500, 40) #nlasrgb &lt;- norm_color(las, rgbmap) # apply user defined function #plot(nlasrgb, color = &quot;RGB&quot;, bg = &quot;white&quot;, size = 6) # some plotting It works! We have a workable function that performs on a point cloud. Remember that norm_color() is just an example. You could instead choose to make an algorithm for individual tree detection or a method to sample point of interest, let your imagination run wild! So far it works only when using a LAS object. Now let’s make it working with a LAScatalog. 15.3 Create an intermediate function for catalog_apply() The simple way is: new_ctg &lt;- catalog_map(ctg, norm_color, rgbmap = rgbmap) However, here we will learn the hard way in order to understand better how it works internally and understand the full potential of catalog_apply() for which catalog_map() is only a simplified version. The core engine is the function catalog_apply() but this function does not work with any user-defined function. User-defined functions must respect a specific template (take a look at the documentation in R) and is expected to perform some specific tasks. First, the primary variable must be a chunk from the catalog (see chunk below), the function must read the chunk, check that it is not empty, then perform the computation. A valid function is the following: norm_color_chunk &lt;- function(chunk, rgbmap) # user defined function { las &lt;- readLAS(chunk) # read the chunk if (is.empty(las)) return(NULL) # check if it actually contain points nlasrgb &lt;- norm_color(las, rgbmap) # apply computation of interest return(nlasrgb) # output } This introduces an additional level of complexity that is crucial. First, catalog chunks will be sequentially fed into the user’s function. Each chunk is read inside the user-defined function with all processing options being automatically respected (filter, select, chunk_size, chunk_buffer etc.). The las variable in the code snippet above is a point cloud extracted from the catalog that contains the chunks + a buffer. It’s important to check that the loaded portion of the collection is not empty or subsequent code will likely fail. This may happen depending on the size of the chunk and the filter options chosen. This function is now catalog_apply() compatible and can be applied over the entire ALS acquisition. The output will be a point cloud, so we need to pay attention mitigate memory issues and save to disk. At this stage, we haven’t mitigated that problem yet so we add it below. #opt_output_files(ctg) &lt;- paste0(tempdir(), &quot;/{*}_norm_rgb&quot;) # write to disk #output &lt;- catalog_apply(ctg, norm_color_chunk, rgbmap = rgbmap) # implement user-defined function using catalog_apply #str(output) We can see that the output is a list with the name of each file written to disk. This is the default behavior of the engine. It returns a list with one element per chunk. This isn’t really convenient. We have seen in section 14 that in the case of LAS files we can return a LAScatalog, which is far more convenient. We can use the option automerge = TRUE so catalog_apply() will automatically merge the list into something more user-friendly. opt_output_files(ctg) &lt;- paste0(tempdir(), &quot;/{*}_norm_rgb&quot;) #options &lt;- list(automerge = TRUE) # merge all the outputs #output &lt;- catalog_apply(ctg, norm_color_chunk, rgbmap = rgbmap, .options = options) #output We now have a LAscatalog. It is much better but: #plot(output) We still have a problem here. The files are overlapping because we read each file with a buffer and then the outputs have been written with their buffer. This is bad practice. It’s important to always remove the buffer after the computation. In this specific case, the output is a LAS file. When readLAS() reads a chunk from the catalog the points in the buffer are flagged so they can be easily manipulated. Since they are flagged, we can easily remove these points by adding this step to our user-defined function. It is always the role of the user to remove the buffer of the output. norm_color_chunk &lt;- function(chunk, rgbmap) { las &lt;- readLAS(chunk) if (is.empty(las)) return(NULL) nlasrgb &lt;- norm_color(las, rgbmap) nlasrgb &lt;- filter_poi(nlasrgb, buffer == 0) # remove buffer return(nlasrgb) } #opt_output_files(ctg) &lt;- paste0(tempdir(), &quot;/{*}_norm_rgb&quot;) #options &lt;- list(automerge = TRUE) #output &lt;- catalog_apply(ctg, norm_color_chunk, rgbmap = rgbmap, .options = options) #plot(output) When the outputs are spatial vectors or spatial rasters they can be cropped using the bounding box of the chunk accessible via extent(chunk) or bbox(chunk) or st_bbox(chunk) or ext(chunk) that match respectively with raster, sp, sf/stars and terra. Nice! We created a custom function – norm_color() – and we upscale its definition to capably and efficiently process an entire ALS acquisition made up of hundreds of files. Using options described in section 14, we can also introduce parallelization, chunk size control, and buffer size control. library(future) plan(multisession) opt_filter(ctg) &lt;- &quot;-keep_class 2&quot; opt_chunk_size(ctg) &lt;- 300 opt_chunk_buffer(ctg) &lt;- 40 opt_output_files(ctg) &lt;- paste0(tempdir(), &quot;/{*}_norm_rgb&quot;) options &lt;- list(automerge = TRUE) output &lt;- catalog_apply(ctg, norm_color_chunk, rgbmap = rgbmap, .options = options) We can check that it worked by loading a sample from somewhere in our new catalog. According to the options put above, we are expecting to get a normalized (we normalized), colored (we merged RGB image), ground points (we used -keep_class 2). #opt_output_files(output) &lt;- &quot;&quot; #las &lt;- clip_circle(output, 338800, 5238500, 60) #plot(las, color = &quot;RGB&quot;, size = 6, bg = &quot;white&quot;) 15.4 Make a user-friendly function for third party users When designing tools that will be used by third parties, we would like to hide catalog_apply() and intermediate functions inside a more high level and user-friendly function similarly to lidR functions that work the same both with a LAS or a LAScatalog. Moreover we would like to make the function foolproof for users. To do this we can create wrap our codes if if-else statements to apply to correct code to the correct object. # Create a generic function norm_color &lt;- function(las, rgbmap) { if (is(las, &quot;LAScatalog&quot;)) { options &lt;- list(automerge = TRUE) output &lt;- catalog_apply(ctg, norm_color, rgbmap = rgbmap, .options = options) return(output) } else if (is(las, &quot;LAScluster&quot;)) { x &lt;- readLAS(las) if (is.empty(x)) return(NULL) nlasrgb &lt;- norm_color(x, rgbmap) nlasrgb &lt;- filter_poi(nlasrgb, buffer == 0) return(nlasrgb) } else if (is(las, &quot;LAS&quot;)) { nlas &lt;- normalize_height(las, tin()) colorized &lt;- merge_spatial(nlas, rgbmap) return(colorized) } else { stop(&quot;Not supported input&quot;) } } We now have a single function that can be used seamlessly on a LAS or a LAScatalog object. las_norm_colored &lt;- norm_color(las, rgbmap) opt_output_files(ctg) &lt;- paste0(tempdir(), &quot;/{*}_norm_rgb&quot;) ctg_norm_colored &lt;- norm_color(ctg, rgbmap) The different steps to apply to a LAScluster (i.e. readLAS + remove buffer of the output) are, in this case, very common. It is possible to use catalog_map() instead, that is capable of doing this tasks for you. But in some cases the output may be not supported, because the cropping step is more complex. In those cases, catalog_apply() must be used because it allows for a more accurate control. # Create a generic function norm_color &lt;- function(las, rgbmap) { if (is(las, &quot;LAScatalog&quot;)) { output &lt;- catalog_map(ctg, norm_color, rgbmap = rgbmap, .options = options) return(output) } else if (is(las, &quot;LAS&quot;)) { nlas &lt;- normalize_height(las, tin()) colorized &lt;- merge_spatial(nlas, rgbmap) return(colorized) } else { stop(&quot;Not supported input&quot;) } } 15.5 Make a safe function for third party users At this stage our function is almost finished. However it is not foolproof. What if the user of this new function does not provide any output path template? The point cloud in each chunk will be loaded in memory and will be retained in memory until it eventually becomes full and R crashes. We must prevent the use of the function if no path to the disk is given. Also, the computation of a DTM requires a buffer. If the user sets a 0 m buffer the output of this function will be incorrect. We must prevent the use of the function if a buffer of 0 is given. These two cases are covered by the engine with the options need_output_file = TRUE and need_buffer = TRUE. To finish we would like to disable the ability to tune the select option to ensure no attribute is lost. norm_color &lt;- function(las, rgbmap) { if (is(las, &quot;LAScatalog&quot;)) { opt_select(las) &lt;- &quot;*&quot; # disable select tuning options &lt;- list(automerge = TRUE, need_output_file = TRUE, need_buffer = TRUE) # require output path &amp; buffer size output &lt;- catalog_map(ctg, norm_color, rgbmap = rgbmap, .options = options) return(output) } else if (is(las, &quot;LAS&quot;)) { nlas &lt;- normalize_height(las, tin()) colorized &lt;- merge_spatial(nlas, rgbmap) return(colorized) } else { stop(&quot;Not supported input&quot;) } } We can test what happens if we use this function naively. #opt_output_files(ctg) &lt;- &quot;&quot; #ctg_norm_colored &lt;- norm_color(ctg, rgbmap) It failed with an informative message! We are done. This is exactly how the lidR package works and we have presented all the tools needed to extend it with new applications. More examples can be found further along in this book. For example, section 17.1.1 presents how to compute a raster of the average distance between first and last returns, and section 17.1.2 presents how to compute a rumple index of the canopy from the point cloud and, of course, the documentation of the package itself is more comprehensive than this book. "],["modeling-aba.html", "16 The Area-Based Approach (ABA) to forest modelling 16.1 Read in data 16.2 ABA database building 16.3 Statistical modeling 16.4 Wall to wall modelling 16.5 Calculate wall-to-wall predictions", " 16 The Area-Based Approach (ABA) to forest modelling This section presents a complete workflow about processing ALS point cloud data to create wall-to-wall predictions of selected forest stand attributes using an area-based approach (ABA) to forest inventories. The steps used in this vignette as well as further details about enhanced forest inventories (EFI) are described in depth in White et al. 2013 and White et al. 2017. This vignette assumes that the user has a directory of classified and normalized .las/.laz files. First we load the package sf to work with spatial data. library(sf) 16.1 Read in data The function readLAScatalog() seen in section 14 builds a LAScatalog object from a folder. Make sure the ALS files have associated index files - for details see: Speed-up the computations on a LAScatalog. ctg &lt;- readLAScatalog(&quot;folder/&quot;) print(ctg) #&gt; class : LAScatalog #&gt; extent : 297317.5 , 316001 , 5089000 , 5099000 (xmin, xmax, ymin, ymax) #&gt; coord. ref. : +proj=utm +zone=18 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs #&gt; area : 125.19 km² #&gt; points : 1.58 billion points #&gt; density : 12.6 points/m² #&gt; num. files : 138 plot(ctg) Now that our ALS data is prepared lets read in our inventory data using the st_read() function from the sf package. We read in a .shp file where each features is a point with a unique plot ID and corresponding plot level inventory summaries. plots &lt;- st_read(&quot;PRF_plots.shp&quot;) Our plots object contains 203 plots with coordinates of plot centers, plot name, and stand attributes: Lorey’s height (HL), Basal area (BA), and gross timber volume (GTV). plots #&gt; Simple feature collection with 203 features and 4 fields #&gt; geometry type: POINT #&gt; dimension: XY #&gt; bbox: xmin: 299051.3 ymin: 5089952 xmax: 314849.4 ymax: 5098426 #&gt; epsg (SRID): NA #&gt; proj4string: +proj=utm +zone=18 +ellps=GRS80 +units=m +no_defs #&gt; First 5 features: #&gt; PlotID HL BA GTV geometry #&gt; 1 PRF002 20.71166 38.21648 322.1351 POINT (313983.8 5094190) #&gt; 2 PRF003 19.46735 28.92692 251.1327 POINT (312218.6 5091995) #&gt; 3 PRF004 21.74877 45.16215 467.1033 POINT (311125.1 5092501) #&gt; 4 PRF005 27.76175 61.55561 783.9303 POINT (313425.2 5091836) #&gt; 5 PRF006 27.26387 39.78153 508.0337 POINT (313106.2 5091393) To visualize where the plots are in our study area we can overlay them on the LAScatalog. plot(ctg) plot(plots, add = TRUE, col = &quot;red&quot;) We have now prepared both our ALS and plot inventory data and can begin ABA processing. 16.2 ABA database building To build a database ready for modeling the steps are the following: Clip the regions of interest (stands) using a shapefile of stand locations. Calculate derived metrics for each stand. Associate stand attributes (ID,HL,BA,GTV) to the metrics. This can be done with a single function plot_metrics(). We also use the opt_filter() function on the LAScatalog to ignore noise below 0 m at read time so that they do not influence future processing. We clip discs with a radius of 14.1 meters because our plot radius is 14.1 m. opt_filter(ctg) &lt;- &quot;-drop_z_below 0&quot; # Ignore points with elevations below 0 D &lt;- plot_metrics(ctg, .stdmetrics_z, plots, radius = 14.1) We have now calculated a variety of ALS metrics for each plot. 16.3 Statistical modeling Here we provide a simple example of how to create an OLS model (lm) for Lorey’s Mean Height (HL). Using the summary and plot functions we found that the 85th percentile of height (zq85) for ALS metrics explain a large amount of variation in HL values. We are more than happy with this simple model. m &lt;- lm(HL ~ zq85, data = D) summary(m) #&gt; Call: #&gt; lm(formula = HL ~ zq85, data = D) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -4.640 -1.053 -0.031 1.030 4.258 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 2.29496 0.31116 7.376 4.2e-12 *** #&gt; zq85 0.93389 0.01525 61.237 &lt; 2e-16 *** #&gt; --- #&gt; Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #&gt; #&gt; Residual standard error: 1.5 on 201 degrees of freedom #&gt; Multiple R-squared: 0.9491, Adjusted R-squared: 0.9489 #&gt; F-statistic: 3750 on 1 and 201 DF, p-value: &lt; 2.2e-16 Here we visualize the relationship between the observed (measured) and predicted (ALS-based) HL values. plot(D$HL, predict(m)) abline(0,1) 16.4 Wall to wall modelling Now that we have created a model using plot data, we need to apply that model across our entire study area. To do so we first need to generate the same suite of ALS metrics (.stdmetrics_z) for our ALS data using the pixel_metrics() function on our original collection of files. Note that the res parameter is set to 25 because we want the resolution of our metrics to match the area of our sample plots (14.1 m radius = 625m2). metrics_w2w &lt;- pixel_metrics(ctg, .stdmetrics_z, res = 25, pkg = &quot;terra&quot;) To visualize any of the metrics you can use the plot function. plot(metrics_w2w$zq85) plot(metrics_w2w$zmean) plot(metrics_w2w$pzabovezmean) 16.5 Calculate wall-to-wall predictions We can use the predict() function to produce HL_pred which is a wall-to-wall raster of predicted Lorey’s mean height. HL_pred &lt;- predict(metrics_w2w, m) To visualize wall-to-wall predictions use plot(HL_pred) #&gt; NULL "],["outbox.html", "17 Thinking outside the box 17.1 New complex metrics in ABA 17.2 Multi-spectral coloring", " 17 Thinking outside the box We have presented many common processes through these tutorials. Most examples presented could have been done with other softwares, but what we have seen is only the tip of the lidR iceberg! In fact, lidR was never designed with common processes in mind, but instead to explore new ones and push innovation in this field. Some examples might be found here and there in the book. The following section presents some applications that are likely to be impossible to reproduce in other softwares, demonstrating how we can go from a novel idea to a fully functional tool using lidR. 17.1 New complex metrics in ABA 17.1.1 Distance between returns In forestry, simple metrics can be derived from first returns. But what about metrics that estimate the average distance between first and last returns? Is it a valuable metric for characterizing of forest stucture? Can we refine the prediction with such metric? Nobody knows, but we can try. First, we need to retrieve each emitted pulse and keep only the first and last point of each sequence excluding sequences with only one return. las &lt;- retrieve_pulses(las) las &lt;- filter_firstlast(las) las &lt;- filter_poi(las, NumberOfReturns &gt; 1) Using the data.table syntax we can compute the distance between first and last returns for each pulse. It could be done with dplyr as well using group_by() %&gt;% mutate() but we are proponent of using data.table(). las@data[, d := Z[1]-Z[2], by = pulseID] At this stage our data set contains distances twice. 1 associated with the first return and 2 with the last return. We only need a single value, so lets keep only first returns. Some distances are NA and correspond to pulses at the very edge of the file. We can discard those case for the moment. las &lt;- filter_first(las) las &lt;- filter_poi(las, !is.na(d)) We can now plot the first returns colored by their distance to the last return. plot(las, bg = &quot;white&quot;, color = &quot;d&quot;, colorPalette = viridis::magma(50), size = 4) #&gt; The argument &#39;coloPalette&#39; is deprecated. Use &#39;pal&#39; instead Now we can compute the average in an ABA (see section 10). d_first_last &lt;- pixel_metrics(las, ~mean(d), 15) plot(d_first_last, col = height.colors(50)) Now we have a working method, we can extend the idea to apply it over a broad coverage using the LAScatalog processing engine (section 14). First we create a function that does this job. This is not mandatory but factorizing the code is good practice. grid_distance_first_last &lt;- function(las, res) { las &lt;- retrieve_pulses(las) las &lt;- filter_firstlast(las) las &lt;- filter_poi(las, NumberOfReturns &gt; 1) las@data[, d := Z[1]-Z[2], by = pulseID] las &lt;- filter_first(las) las &lt;- filter_poi(las, !is.na(d)) return(pixel_metrics(las, ~mean(d), res)) } To finish we use the LAScatalog processing engine with catalog_map(). We need to process with a buffer to avoid the case where pulses are separated in two files, so we use opt_chunk_buffer(ctg) &lt;- 2. In addition, with the output being a raster, we ensure alignment of the chunk with the raster. ctg &lt;- readLAScatalog(&quot;folder/&quot;) opt_chunk_buffer(ctg) &lt;- 2 options &lt;- list(raster_alignment = 15) metrics &lt;- catalog_map(ctg, grid_distance_first_last, res = 15, .options = options) And we are done. Our innovative metric is computing with real-time monitoring. It can run in parallel on a single machine or in parallel on several machines. At the end, the output is a wall-to-wall raster map of the average distance between first and last returns for beams that return multiple returns. In 19 lines of code! plot(metrics, col = height.colors(50)) 17.1.2 Rumple index Another interesting metric that could be computed is the rumple index (see Kane et al. (2008)). To compute a rumple index we can triangulate points, measure the surface created, and divide by the projected surface. Rumple index is however expected to measure surface roughness of the canopy and it doesn’t makes sense to triangulate every point. An option is to triangulate each first return but yet first returns are not all representative of the canopy. In the following example we are using the surface points every 1 m. grid_rumple_index &lt;- function(las, res) { # user-defined function las &lt;- filter_surfacepoints(las, 1) return(pixel_metrics(las, ~rumple_index(X, Y, Z), res)) } Then the code is pretty much the same than in previous section ctg &lt;- readLAScatalog(&quot;folder/&quot;) opt_chunk_buffer(ctg) &lt;- 1 options &lt;- list(raster_alignment = 20) metrics &lt;- catalog_map(ctg, grid_rumple_index, res = 20, .options = options) plot(metrics, col = height.colors(50)) 17.2 Multi-spectral coloring We have already seen a multi-spectral coloring example in section 13.2.3. This is truly a good example of how to think outside the box with lidR, so we show several variants here. The goal is to use multi-spectral data to generate false coloring and see how false coloring may be used for further analyses. Multi-spectral ALS data are sampled with 3 devices each emitting a different wavelength. The point cloud is usually provided in the form of 3 LAS files, each file corresponding to a spectral wavelength. No matter the actual wavelength, we can consider the first band as blue, the second as red, and the third as green, and thus consider that each point has a pure color. f1 &lt;- &quot;data/chap11/PR1107_c1_ar_c.laz&quot; f2 &lt;- &quot;data/chap11/PR1107_c2_ar_c.laz&quot; f3 &lt;- &quot;data/chap11/PR1107_c3_ar_c.laz&quot; las &lt;- readMSLAS(f1, f2, f3, filter = &quot;-keep_z_below 300&quot;) plot(las, color = &quot;ScannerChannel&quot;, size = 6) Each channel’s returns vary in intensities as seen in the figure below because of the wavelength reflectance properties of the targets. A first step could be to normalize the intensities. For this example we won’t do that to keep things simple. library(ggplot2) ggplot(las@data) + aes(x = Intensity, fill = as.factor(ScannerChannel)) + geom_density(alpha = 0.5) + theme_minimal() + theme(legend.position = c(.9, .90), legend.title = element_blank()) In this specific example, notice that only few intensities are above 255, So intensity ranges from 0 to almost 255. This is very convenient because we will be able to use the raw values as color without transformation. We will simply force values above 255 to be 255. This is a dirty solution but fair enough for the example which is not about intensity normalization. las@data[Intensity &gt; 255L, Intensity := 255L] To finish, we define a function that takes the intensities and the associated channel as inputs. This function decomposes the 3 channels, computes the average intensity of each channel, and returns these 3 values that will later be interpreted as RGB. Also if one color is missing we force RGB to be NA. colorize &lt;- function(intensity, channel) { # Split the intensities of each channel i1 &lt;- intensity[channel == 1L] i2 &lt;- intensity[channel == 2L] i3 &lt;- intensity[channel == 3L] # If one channel is missing return RGB = NA if (length(i1) == 0 | length(i2) == 0 | length(i3) == 0) return(list(R = NA_integer_, G = NA_integer_, B = NA_integer_)) i1 &lt;- as.integer(mean(i1)) i2 &lt;- as.integer(mean(i2)) i3 &lt;- as.integer(mean(i3)) return(list(R = i1, G = i2, B = i3)) } Now we can use this function with different levels of regularization. We can use colorize() with grid_metrics() in an ABA. It returns a multi-layer raster that can be interpreted as RGB rgb &lt;- pixel_metrics(las, ~colorize(Intensity, ScannerChannel), 2) terra::plotRGB(rgb) We can also attribute an RGB color per voxel, discarding the voxels where RGB = NA by using colorize() with voxel_metrics() (see section 12). rgb &lt;- voxel_metrics(las, ~colorize(Intensity, ScannerChannel), 2) rgb &lt;- rgb[!is.na(R)] # Remove NAs rgb &lt;- LAS(rgb, las@header) # Convert to LAS for display plot(rgb, color = &quot;RGB&quot;, size = 5) To finish we can colorize() with point_metrics() (see section 13). This has the advantage of maintaining much more points compared to the voxel-based version and to preserve coordinates. rgb &lt;- point_metrics(las, ~colorize(Intensity, ScannerChannel), r = 0.5) las &lt;- add_lasrgb(las, rgb$R, rgb$G, rgb$B) colored &lt;- filter_poi(las, !is.na(R)) # Remove NAs plot(colored, color = &quot;RGB&quot;, size = 4) #&gt; NULL "],["spatial-indexing.html", "18 Spatial indexing 18.1 Introduction to spatial indexes 18.2 Spatial indexes for LAS objects 18.3 Spatial index for LAS files", " 18 Spatial indexing Spatial indexing is a key feature for performing spatial queries over a large point cloud. Any search for points of interest in the absence of indexing would require a “sequential scan” of every point - this could take a lot of time. In brief, spatial indexing organizes data into a search structure that can be quickly traversed to find specific records. Some algorithms would take unreasonable amounts of time to complete without spatial indexing. 18.1 Introduction to spatial indexes This section presents a layman overview of how spatial indexing works. If the reader is already knowledgeable about spatial indexing, they can skip this section. Imagine we have a lidar point cloud with 1 million points (sounds like alot… but it isn’t!). We want to query all points that fall within the extent of a circle centered on the coordinates p = c(300, 350) with a radius (R) of 25 meters. This is a typical query made thousands of times per second by many algorithms including the local maximum filter (section 7.1.1) to locate individual trees. Without spatial indexing, the method consists of computing the distance to p for every single point. This is called a ‘sequential scan’, and it means that the computation for distance and comparisons must be conducted 1,000,000 times each. In R we can write: p = c(300, 350) R = 25 X = runif(1e4, 0, 1000) Y = runif(1e4, 0, 1000) query = sqrt((X - p[1])^2 + (Y - p[2])^2) &lt; R Xq = X[query] Yq = Y[query] This is what the filter_poi() function does. It a non-specialized function that enables querying points of interest (POI) based on their attribute values (including non-spatial queries such as Intensity &gt; x). filter_poi(las, sqrt((X - p[1])^2 + (Y - p[2])^2) &lt; R) Now imagine we want to perform 1,000,000 queries like that for 1,000,000 different points. That translates to 1,000,000 x 1,000,000 = 1 billion operations. This does not scale-up and quickly becomes unrealistic (or at least dramatically slow). With a spatial index, the points are organized in such a way that the computer does not need to perform all the comparisons. In a quadtree, for example, the point cloud can be subdivided in 4 quadrants that are themselves subdivided in four quadrants and so on hierarchically (see figure below). In this example we can immediately exclude 75% of the points (750,000 points) in 4 operations at the top level (in red). The bounding box of our query being [275,325]x[325,375] we know that the POIs do not belong in top-left quadrant ([0,500] x [500,1000]) nor in top-right quadrant ([500,1000] x [500,1000]) nor in bottom-right quadrant. At the second level (in blue) in 4 more operations we can exclude another 75% of the remaining points to search only in one quadrant. At this stage we can perform a sequential scan on only 1/16th of the points (i.e. 62,500 points) meaning that we discarded 937,500 points in 8 operations! Consequently our query is (roughly) 16 times faster and could be even faster yet with more subdivision levels. In lidR a typical quadtree has 8 levels i.e. the space is subdivided in (28)2 = 65,536 quadrants. As we can see, spatial indexing provides a way to dramatically speed-up many common spatial queries using discs, rectangles, polygons, 2D, 3D and so on. Different types of spatial indexes exist for different purposes but in all cases the use of a spatial index is not free and comes at the cost of greater memory usage. 18.2 Spatial indexes for LAS objects 18.2.1 Overview lidR makes use of spatial indexes in many functions and can choose different types of spatial indexes on-the-fly. So far, the book only presented the function readLAS() (see section 2) but the package has some variations of readLAS() named readALSLAS(), readTLSLAS(), readUAVLAS() and so on that enable the registration of a point cloud type allowing lidR to adequately choose the most appropriated spatial indexing method to perform a given computation as fast as possible. As an example we can use the TLS point cloud pine_plot.laz from the TreeLS package. First we read the file with readLAS(), which considers the point cloud to be ALS because lidR was originally designed for ALS and by legacy readLAS() from version &lt;= 3.1 behaves optimally for ALS. In the second case we use readTLSLAS() to inform lidR that this point cloud was sampled with a terrestrial device. In the following test we can see that the computation time was reduced from ~2.5 sec to ~1.3 sec by registering the proper point type. Improvements may range from 2 to 10 times faster depending on the point cloud and the method used. file &lt;- system.file(&quot;extdata&quot;, &quot;pine_plot.laz&quot;, package=&quot;TreeLS&quot;) las &lt;- readLAS(file, select=&#39;xyz&#39;) tls &lt;- readTLSLAS(file, select=&#39;xyz&#39;) system.time(segment_shapes(las, shp_plane(k = 15), &quot;Coplanar&quot;)) #&gt; user system elapsed #&gt; 3.45 0.01 0.28 system.time(segment_shapes(tls, shp_plane(k = 15), &quot;Coplanar&quot;)) #&gt; user system elapsed #&gt; 2.25 0.00 0.17 This works for each method that implies many sequential spatial queries. In the following example we can observe a ~8 fold processing time reduction. system.time(point_metrics(las, r = 1, ~length(Z))) #&gt; user system elapsed #&gt; 7.48 0.00 7.48 system.time(point_metrics(tls, r = 1, ~length(Z))) #&gt; user system elapsed #&gt; 0.87 0.00 0.87 Now lets try with an ALS point cloud. We can see that it’s better to read an ALS point cloud as ALS rather than as TLS (~2 fold difference). This is because registering the correct point cloud type enables the selection of the optimal spatial indexing algorithm internally. als = readALSLAS(&quot;data/ENGINE/catalog/tiles_338000_5238500_1.laz&quot;) tls = readTLSLAS(&quot;data/ENGINE/catalog/tiles_338000_5238500_1.laz&quot;) system.time(classify_noise(als, sor())) #&gt; user system elapsed #&gt; 7.25 0.50 0.58 system.time(classify_noise(tls, sor())) #&gt; user system elapsed #&gt; 11.76 0.09 0.86 Take away: It is always a good idea to use the functions readALSLAS(), readTLSLAS(), readDAPLAS(), and so on introduced in lidR v3.1.0. There are however caveats resulting from using the optimal read*LAS() function that may not guarantee optimal processing performance. All functions do not use spatial indexing or do not use the spatial index framework of lidR. For example, the kriging() function is based on the gstat package. The choice of spatial index relies on some assumptions that may not be met in specific point clouds. The internal dispatch is designed to work with ‘typical’ point clouds under some assumptions. An ALS point-cloud is typically spatially large (1 km² or more) with little Z dispersion (0 to 40 meters) relative to the XY dispersion (0 to 1000 meters). On the contrary, a TLS point cloud is typically spatially narrow (~3000 m²) with larger variations in Z relative to XY. read*LAS() should be sufficient for most use cases but for some specific cases users can manually choose which spatial index is best suited. We cover this in the next section. 18.2.2 Spatial indexes and selection strategies lidR currently has 4 spatial indexes: a grid partition, a voxel partition, a quadtree and an octree. Each has its own pros and cons. Grid partition and quadtree are 2D indexes while voxel partition and octree are 3D indexes. They are all able to perform any kind of spatial query similarly. This is why it doesn’t matter if the point cloud is read with readALSLAS() or readTLSLAS(). The result will be the same. However their efficiency depends on the point cloud type and the query type. This is why using the proper read*LAS() function can matter. 18.2.2.1 ALS strategies For ALS we use a 2D index even for 3D queries. Indeed, an ALS point cloud is ‘mostly 2D’ because more than 99% of the dispersion is in XY. When querying the knn of a given point (3D query) from a 2D index the vast majority of the points are discarded on a 2D basis. The remaining sequential scan occurs only on a very tiny fraction of the data set. This is also true for a 3D index but querying a 3D spatial index is slower and thus in lidR our 2D indexes perform best for ALS. A grid partition is used by default because it is often faster than a quadtree because ALS points are uniformly distributed on XY. The following example demonstrates how to manually register a spatial index and compare the computation times for a quadtree and an octree. las = readLAS(&quot;data/ENGINE/catalog/tiles_338000_5238500_1.laz&quot;, select = &quot;xyz&quot;) index(las) &lt;- &quot;quadtree&quot; system.time(classify_noise(las, sor())) #&gt; user system elapsed #&gt; 8.34 0.04 0.67 index(las) &lt;- &quot;octree&quot; system.time(classify_noise(las, sor())) #&gt; user system elapsed #&gt; 11.94 0.01 0.85 18.2.2.2 TLS strategies For TLS we use a 3D index because the points are almost evenly distributed in XYZ and thus a 2D query does not allow for discarding a large fraction of the points - the sequential scan remains important. An octree is used because points are expected to be not uniformly distributed on XYZ. file &lt;- system.file(&quot;extdata&quot;, &quot;pine_plot.laz&quot;, package=&quot;TreeLS&quot;) las &lt;- readLAS(file, select=&#39;xyz&#39;) index(las) &lt;- &quot;quadtree&quot; system.time(classify_noise(las, sor())) #&gt; user system elapsed #&gt; 2.47 0.00 0.24 index(las) &lt;- &quot;octree&quot; system.time(classify_noise(las, sor())) #&gt; user system elapsed #&gt; 0.96 0.02 0.08 18.2.2.3 DAP and UAV strategies For digital photogrammetry and UAV data we apply the same rules as TLS. When encountering a data set that does not follow these rules, it may be optimal to manually select a spatial index. This is the case of the data set seen in chapter 13.2.1, which is an ALS data set but in practice it’s a small subset in which we can no longer say that more than 99% of the point dispersion is in XY only. In that sense it’s more of a TLS ish point cloud. But in the meantime the points are uniformly spread on XY because it’s actually an ALS data set. Thus making 3D queries using a 3D index most viable. Let’s try it: las &lt;- readLAS(&quot;data/chap11/building_WilliamsAZ_Urban_normalized.laz&quot;) index(las) &lt;- &quot;gridpartition&quot; system.time(segment_shapes(las, shp_plane(k = 20), &quot;planar&quot;, filter = ~Classification != LASGROUND)) #&gt; user system elapsed #&gt; 3.87 0.03 0.31 index(las) &lt;- &quot;voxelpartition&quot; system.time(segment_shapes(las, shp_plane(k = 20), &quot;planar&quot;, filter = ~Classification != LASGROUND)) #&gt; user system elapsed #&gt; 4.27 0.02 0.35 We see that both tests are almost equal, and that octree is slower. But one may find limit cases where its worth it to perform manual selection and thus lidR allows for overwriting the default rules. More details in help(\"lidR-spatial-index\"). 18.2.3 C++ API For more advanced users and developers, the lidR spatial index framework is provided as header-only C++ classes meaning that users can link to lidR to develop R/C++ applications using lidR spatial indexes. If the reader is not comfortable with the terms C++, Rcpp, header-only, external pointer and other C++ related concepts, we understand! You can skip this section, which is dedicated to advanced users and package developers who want to develop complex and efficient tools. For the purpose of this example we will create a function clip_disc() similar to clip_circle() available in lidR. clip_circle() performs a sequential scan and is thus not suitable to perform many queries in a loop. The function clip_disc() on the contrary will take advantage of spatial indexing. There is only one C++ class to know named SpatialIndex. It has one constructor that accepts an S4 class and has two public members knn and lookup. First we can write a C++ function that returns a pointer on a SpatialIndex. Here we are using an external pointer because it’s simple to write, and implies fewer lines of code. We can however also imagine taking advantage of Rcpp modules. // [[Rcpp::depends(lidR)]] #include &lt;SpatialIndex.h&gt; using namespace Rcpp; using namespace lidR; // [[Rcpp::export]] XPtr&lt;SpatialIndex&gt; spatial_index(S4 las) { SpatialIndex* idx = new SpatialIndex(las); XPtr&lt;SpatialIndex&gt; p(idx, true); return p; Now we can instantiate a SpatialIndex at the R level. las = readLAS(&quot;data/ENGINE/catalog/tiles_338000_5238500_1.laz&quot;) index = spatial_index(las) index #&gt; &lt;pointer: 0x000001983ecf5110&gt; What has been created here is either a grid partition, a voxel partition, a quadtree or an octree depending on which readLAS() function was used to read the files or depending on the spatial index that was manually registered. Then we can write the C++ side of the query. // [[Rcpp::export]] IntegerVector filter_disc_with_index(SEXP xptr, double xc, double yc, double r) { XPtr&lt;SpatialIndex&gt; tree(xptr); Circle circ(xc, yc, r); std::vector&lt;PointXYZ&gt; pts; tree-&gt;lookup(circ, pts); IntegerVector ids(pts.size()); for(int i = 0 ; i &lt; pts.size(); i++) { ids[i] = pts[i].id; } return ids + 1; // C++ is 0-indexed } And the R side of the query clip_disc = function(las, index, xcenter, ycenter, radius) { ii &lt;- filter_disc_with_index(index, xcenter, ycenter, radius) return(las[ii]) } Now we can make a query and verify that both functions return the same points. sub1 = clip_disc(las, index, 338200, 5238585, 10) sub2 = clip_circle(las, 338200, 5238585, 10) sub1 #&gt; class : LAS (v1.0 format 1) #&gt; memory : 78.6 Kb #&gt; extent : 338190, 338209.9, 5238575, 5238595 (xmin, xmax, ymin, ymax) #&gt; coord. ref. : WGS 84 / UTM zone 19N #&gt; area : 305.5 m² #&gt; points : 1.1 thousand points #&gt; density : 3.75 points/m² #&gt; density : 2.92 pulses/m² sub2 #&gt; class : LAS (v1.0 format 1) #&gt; memory : 78.6 Kb #&gt; extent : 338190, 338209.9, 5238575, 5238595 (xmin, xmax, ymin, ymax) #&gt; coord. ref. : WGS 84 / UTM zone 19N #&gt; area : 305.5 m² #&gt; points : 1.1 thousand points #&gt; density : 3.75 points/m² #&gt; density : 2.92 pulses/m² While there is no gain with a single query because of the overhead of creating an index, it is indispensable to perform many successive queries. In the following we perform 50 queries in a loop. n = 50 x = runif(n, 338000, 338500) y = runif(n, 5238500, 5239000) system.time(for (i in 1:n) u = clip_circle(las, x[i], y[i], 10)) #&gt; user system elapsed #&gt; 2.61 0.44 3.08 system.time(for (i in 1:n) u = clip_disc(las, index, x[i], y[i], 10)) #&gt; user system elapsed #&gt; 0.25 0.01 0.27 For more functionalities one can look at the source code of SpatialIndex where we can see there are actually 2 constructors and 5 members including 2D and 3D knn, 2D and 3D knn with maximum radius and lookup that is templated to allow queries within any kind of user-defined shapes. The source code of many lidR functions such as lmf() or detect_shape() might be useful resources as well. SpatialIndex(const Rcpp::S4 las); SpatialIndex(const Rcpp::S4 las, const std::vector&lt;bool&gt;&amp; filter); template&lt;typename T&gt; void lookup(T&amp; shape, std::vector&lt;PointXYZ&gt;&amp; res); void knn(const PointXY&amp; p, const unsigned int k, std::vector&lt;PointXYZ&gt;&amp; res); void knn(const PointXYZ&amp; p, const unsigned int k, std::vector&lt;PointXYZ&gt;&amp; res); void knn(const PointXY&amp; p, const unsigned int k, const double r, std::vector&lt;PointXYZ&gt;&amp; res); void knn(const PointXYZ&amp; p, const unsigned int k, const double r, std::vector&lt;PointXYZ&gt;&amp; res); 18.2.4 Benchmark lidR’s spatial index framework is very fast, especially when large point clouds are used. In the following we compare how fast lidR searches for the 10-nearest neighbours of every point in a 2.3 million point ALS point cloud relative to the RANN, FANN and nabor packages. We see that it is competitive with the very fast libnabo library but does more than libnabo since it also performs range queries such as point in discs, rectangles, cylinders, triangles, polygons. We don’t know any R library providing such capability to produce benchmark comparisons. Moreover, lidR leverages the C++ classes to allow the creation of efficient third party applications. This functionality is heavily used in the lidRplugins package. 18.3 Spatial index for LAS files Previous sections were dedicated to explaining spatial indexing for LAS objects i.e. point clouds read with readLAS() and loaded in memory. This section focuses on spatial indexing for LAS files i.e. point clouds stored in las/laz files and not (yet) loaded in memory. The problem of spatial queries at read time is the same but the solution is different because it was developed in an independent context. Fast spatial queries are made possible by indexing the .las or .laz files with .lax files. A .lax file is a tiny file associated with a .las or .laz file that spatially indexes the points. This file type was created by Martin Isenburg in LAStools. For a better understanding of how it works one can refer to a talk given by Martin Isenburg about lasindex. In short it uses quadtree. By adding .lax files along with your .las/.laz files it is possible to make fast 2D queries without reading the whole file. The best way to create a .lax file is to use laxindex from LAStools. It is a free and open-source part of LAStools. If you cannot or do not want to use LAStools the rlas package has a function to creates lax files but lasindex should be preferred. rlas::writelax(&quot;file.las&quot;) The gain is really significant and transparent for users. If you have a .lax file it will be used. Here we test with 150 queries from the same indexed and a non-indexed LAScatalog with 400 files: indexed = readLAScatalog(&quot;LiDAR with lax/&quot;) noindex = readLAScatalog(&quot;LiDAR no lax/&quot;) clip_circle(indexed, xc, yc, radius = 12) #&gt; 45 sec clip_circle(noindex, xc, yc, radius = 12) #&gt; 4 sec If the reader did not skip section 18.2.3 they might have noticed that clip_circle() can use a spatial index with a LAScatalog but not with a LAS. This is because they behave very differently internally and rely on two independent mechanisms. With a LAScatalog it inherits the capabilities of the library used to read the files while with a LAS object nothing has been implemented (yet) for taking advantage of spatial indexing at the R level (but the section above provide the solution). It’s easy to guess that every clip_something() function can take advantage of spatial indexing with .lax files but the LAScatalog processing engine also makes heavy usage of such features. Users can significantly reduce the processing time by loading a buffer faster. Indeed loading a buffer implies spatial queries. This topic is covered by the vignette: Speed-up the computations on a LAScatalog. #&gt; NULL "],["plugins.html", "19 lidR plugin system 19.1 Understanding lidR algorithms 19.2 Creation of the mba algorithm 19.3 What about other algorithms?", " 19 lidR plugin system We have seen that lidR has many functions capable of processing a collection of files. Most of these functions support more than one algorithm to achieve a given task. rasterize_terrain() supports tin(), knnidw(), kriging() rasterize_canopy() supports p2r(), dsmtin(), pitfree() classify_ground() support csf(), pmf() locate_trees() supports lmf() segment_trees() supports dalponte2016(), li2012(), watershed(), silva2016() track_sensor() supports roussel2020(), Gatziolis2019() and so on What if a user wanted to create a new algorithm integration? For example, what about a new algorithm for ground segmentation or tree segmentation or another interpolation method to create a digital terrain model using cubic spline interpolation? Sounds interesting. With catalog_apply() one can more or less replicate the original functions and apply a new method over a catalog. After all, lidR functions actually use catalog_apply() internally. However this implies a lot of code and is error prone especially for users who are not fully comfortable with the engine. There is another way to create new algorithms that are fully compatible with lidR functions. This is not documented in the package because the underlying mechanism is not yet fully consistent and is still subject to improvements. Let’s continue with the bicubic spline interpolation method for creating a digital terrain model. There is a package called MBA that implements bicubic spline interpolation. We will create a function mba() that can be used like any other algorithm: dtm &lt;- rasterize_terrain(las, algorithm = mba(n = 1, h = 8)) 19.1 Understanding lidR algorithms In lidR, an algorithm such as tin(), p2r() or lmf() is a function factory. The output is functions with extra classes so regular users wont immediately recognize that they are functions. algo &lt;- knnidw(k = 10) algo #&gt; Object of class lidR algorithm #&gt; Algorithm for: spatial interpolation #&gt; Designed to be used with: normalize_height or rasterize_terrain or p2r or spatial_interpolation #&gt; Native C++ parallelization: yes #&gt; Parameters: #&gt; - k = 10 &lt;numeric&gt; #&gt; - p = 2 &lt;numeric&gt; #&gt; - rmax = 50 &lt;numeric&gt; class(algo) # algo is a function #&gt; [1] &quot;lidRAlgorithm&quot; &quot;SpatialInterpolation&quot; &quot;function&quot; &quot;omp&quot; Removing the extra classes we can see its a function and we can see the source code. class(algo) &lt;- &quot;function&quot; algo #&gt; function (las, where) #&gt; { #&gt; assert_is_valid_context(LIDRCONTEXTSPI, &quot;knnidw&quot;) #&gt; return(interpolate_knnidw(las, where, k, p, rmax)) #&gt; } #&gt; &lt;bytecode: 0x0000019724c88620&gt; #&gt; &lt;environment: 0x0000019724ead118&gt; #&gt; attr(,&quot;class&quot;) #&gt; [1] &quot;function&quot; We can see how a function designed to be used in rasterize_terrain() is designed. The signature is function(las, where) When creating a new algorithm for spatial interpolation, the function factory must return a function similar to what you see above. In the case of spatial interpolation las is a LAS with X Y and Z coordinates of ground points (cleaned of duplicates). where is a data.frame with the X Y coordinates of the location where we want to interpolate Z. 19.2 Creation of the mba algorithm Now let’s create our mba algorithm. # mba is our function factory mba &lt;- function(n = 1, m = 1, h = 8, extend = TRUE) { # f is created inside mba and receive the ground points in a LAS (gnd) # and the location where to compute the interpolation (where) f &lt;- function(gnd, where) { # computation of the interpolation (see the documentation of MBA package) res &lt;- MBA::mba.points(gnd@data, where, n, m , h, extend) return(res$xyz.est[,3]) } # f is a function but we can set compatible classes. Here it is an # algorithm for DTM f &lt;- plugin_dtm(f) return(f) } Now let see what happens if we instantiate the mba algorithm: algo &lt;- mba(h = 6) algo #&gt; Object of class lidR algorithm #&gt; Algorithm for: spatial interpolation #&gt; Designed to be used with: normalize_height or rasterize_terrain or p2r or spatial_interpolation #&gt; Native C++ parallelization: no #&gt; Parameters: #&gt; - extend = TRUE &lt;logical&gt; #&gt; - h = 6 &lt;numeric&gt; #&gt; - m = 1 &lt;numeric&gt; #&gt; - n = 1 &lt;numeric&gt; We can now use it like any other lidR algorithm: LASfile &lt;- system.file(&quot;extdata&quot;, &quot;Topography.laz&quot;, package=&quot;lidR&quot;) las &lt;- readLAS(LASfile) dtm &lt;- rasterize_terrain(las, algorithm = mba()) plot(dtm, col = gray(1:50/50)) plot_dtm3d(dtm, bg = &quot;white&quot;) It will even fail nicely if used inappropriately! rasterize_canopy(las, 1, mba()) #&gt; Error: The algorithm used is not an algorithm for digital surface model. 19.3 What about other algorithms? lidR has algorithms for canopy height models, individual tree segmentation, individual tree detection, sensor tracking, snag segmentation and so on. They all have different behaviours and this is why it’s difficult to document. If you want to create a new algorithm the best first step is to communicate directly with lidR developers :). The lidRplugins package makes heavy use of the plugins system to provide extra methods for diverse tasks. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
